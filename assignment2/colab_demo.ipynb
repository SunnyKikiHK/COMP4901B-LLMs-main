{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d3b43e0",
      "metadata": {
        "id": "8d3b43e0"
      },
      "source": [
        "###  Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45c19e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c19e77",
        "outputId": "063da4f8-646c-48ec-e9a4-14d4c2a13268",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420dc8b7",
      "metadata": {
        "id": "420dc8b7"
      },
      "source": [
        "### Clone Codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e20bdbde",
      "metadata": {
        "id": "e20bdbde",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4426a2-aed8-45b9-bdb0-8ed495c2cdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMP4901B-LLMs'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 155 (delta 24), reused 19 (delta 19), pack-reused 118 (from 1)\u001b[K\n",
            "Receiving objects: 100% (155/155), 716.72 KiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework2\n",
        "! cd /content/drive/MyDrive/COMP4901B-Homework2 && git clone https://github.com/hkust-nlp/COMP4901B-LLMs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c6cfe0",
      "metadata": {
        "id": "29c6cfe0"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5510016",
      "metadata": {
        "id": "b5510016",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c79b290-6368-4dc3-9455-218761d274bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (1.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]==4.57.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]==4.57.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]==4.57.1) (3.0.3)\n",
            "Collecting vllm==0.10.2\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.10.2)\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.120.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.13.1)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm==0.10.2)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm==0.10.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.10.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm==0.10.2)\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.10.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.10.2)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm==0.10.2)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm==0.10.2)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm==0.10.2)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.1.1)\n",
            "Collecting watchfiles (from vllm==0.10.2)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.16.3)\n",
            "Collecting ninja (from vllm==0.10.2)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm==0.10.2)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm==0.10.2)\n",
            "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm==0.10.2)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2)\n",
            "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm==0.10.2)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm==0.10.2)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm==0.10.2) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm==0.10.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm==0.10.2) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm==0.10.2) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.49.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.3)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cli-0.0.14-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.4.2)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm==0.10.2) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm==0.10.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm==0.10.2) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.22.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.20.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.2) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm==0.10.2) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.28.0)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm==0.10.2) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rignore-0.7.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.42.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.1.2)\n",
            "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl (71.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.14-py3-none-any.whl (11 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, click, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.8 cbor2-5.7.1 click-8.2.1 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.14 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.4 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.51.1 rich-toolkit-0.15.1 rignore-0.7.4 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.1.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.1-py3-none-any.whl size=1764323 sha256=0e2bbe871fad6eeac0ace401705247613f466c520758edd2c0f2c5eaf27c0e6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/41/59/a9d46caf09e118b9276f33e0f6d502a45b1e455296e98a2a11\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: deepspeed\n",
            "Successfully installed deepspeed-0.18.1\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hjson\n",
            "Successfully installed hjson-3.1.0\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (4.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=55e6ce56e97ae0dd39af8ec81324120a4b9d1160aa5a36b2eeabc22a56a29a38\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "--2025-11-03 13:38:15--  https://huggingface.co/datasets/PeterV09/smol-smoltalk-6k/resolve/main/smol-smoltalk-6k.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/68f5a0e304d834b9230fe31e/ee26a26f39a0bad5f07c70cb78dd7c0cf86dd6a9b52e1012d2616e7f7b7ef244?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251103T133816Z&X-Amz-Expires=3600&X-Amz-Signature=d100bcfc517b30c1b5eee11e355536d489ee3983110d4e2ce4ed05a7e74d1029&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27smol-smoltalk-6k.json%3B+filename%3D%22smol-smoltalk-6k.json%22%3B&response-content-type=application%2Fjson&x-id=GetObject&Expires=1762180696&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjE4MDY5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OGY1YTBlMzA0ZDgzNGI5MjMwZmUzMWUvZWUyNmEyNmYzOWEwYmFkNWYwN2M3MGNiNzhkZDdjMGNmODZkZDZhOWI1MmUxMDEyZDI2MTZlN2Y3YjdlZjI0NCoifV19&Signature=NtOCezPoCqrEqDdmcg9BWtTUh-wxBSGb6YNu7-V8W-5mdZ-VrAWh2W0j8GwC7P4JYX0-2NELcQvgv%7EhjcmwMCP3qPf2Hd0GGN%7EO8jAbzuQ1xKk0wTzGJ6dD30%7EANO8uPddhBGqTET3tHDjazYY2lOIZht1cBNr4rTkhfKjJKvQl9lD5VHJuSnqGyddk44%7EVmqkVYJj%7EHg24EMKEeZBSMrehb64ETCB-j4vbgF6HMsjrDMIG0DfUMWujHt6gyK5FhWt1gd%7EIiGaE1J4iAhQnJXrZ2YExSmrC1CO9hYT%7Ewa2KEnJ5UNBkn47TiGFmsOIr0YfkObJQ%7EUbbJHBcmArc2Xg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-03 13:38:16--  https://cas-bridge.xethub.hf.co/xet-bridge-us/68f5a0e304d834b9230fe31e/ee26a26f39a0bad5f07c70cb78dd7c0cf86dd6a9b52e1012d2616e7f7b7ef244?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251103T133816Z&X-Amz-Expires=3600&X-Amz-Signature=d100bcfc517b30c1b5eee11e355536d489ee3983110d4e2ce4ed05a7e74d1029&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27smol-smoltalk-6k.json%3B+filename%3D%22smol-smoltalk-6k.json%22%3B&response-content-type=application%2Fjson&x-id=GetObject&Expires=1762180696&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjE4MDY5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OGY1YTBlMzA0ZDgzNGI5MjMwZmUzMWUvZWUyNmEyNmYzOWEwYmFkNWYwN2M3MGNiNzhkZDdjMGNmODZkZDZhOWI1MmUxMDEyZDI2MTZlN2Y3YjdlZjI0NCoifV19&Signature=NtOCezPoCqrEqDdmcg9BWtTUh-wxBSGb6YNu7-V8W-5mdZ-VrAWh2W0j8GwC7P4JYX0-2NELcQvgv%7EhjcmwMCP3qPf2Hd0GGN%7EO8jAbzuQ1xKk0wTzGJ6dD30%7EANO8uPddhBGqTET3tHDjazYY2lOIZht1cBNr4rTkhfKjJKvQl9lD5VHJuSnqGyddk44%7EVmqkVYJj%7EHg24EMKEeZBSMrehb64ETCB-j4vbgF6HMsjrDMIG0DfUMWujHt6gyK5FhWt1gd%7EIiGaE1J4iAhQnJXrZ2YExSmrC1CO9hYT%7Ewa2KEnJ5UNBkn47TiGFmsOIr0YfkObJQ%7EUbbJHBcmArc2Xg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.125, 18.155.68.69, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24144050 (23M) [application/json]\n",
            "Saving to: ‘smol-smoltalk-6k.json.3’\n",
            "\n",
            "smol-smoltalk-6k.js 100%[===================>]  23.03M  90.1MB/s    in 0.3s    \n",
            "\n",
            "2025-11-03 13:38:16 (90.1 MB/s) - ‘smol-smoltalk-6k.json.3’ saved [24144050/24144050]\n",
            "\n",
            "Fetching 10 files: 100% 10/10 [00:03<00:00,  2.51it/s]\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2 && bash setup.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe18aa7",
      "metadata": {
        "id": "dfe18aa7"
      },
      "source": [
        "### Q6 Start The Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76f6abed",
      "metadata": {
        "id": "76f6abed",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ae3727-8bb0-4348-fb6c-94863570c60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps\n",
            "2025-11-03 07:25:01.595043: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 07:25:01.612700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762154701.633810    2214 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762154701.640328    2214 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762154701.656704    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656732    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656735    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656737    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 07:25:01.661599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251103_072516-39d60x8o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHW2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/39d60x8o\u001b[0m\n",
            "{'loss': 1.7031, 'grad_norm': 1.328125, 'learning_rate': 0.0, 'epoch': 0.02}\n",
            "{'loss': 1.5794, 'grad_norm': 1.2734375, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.04}\n",
            "{'loss': 1.6427, 'grad_norm': 1.3671875, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.07}\n",
            "{'loss': 1.6225, 'grad_norm': 1.2578125, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.09}\n",
            "{'loss': 1.6573, 'grad_norm': 1.25, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.11}\n",
            "{'loss': 1.6054, 'grad_norm': 1.3125, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.13}\n",
            "{'loss': 1.6017, 'grad_norm': 1.25, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.15}\n",
            "{'loss': 1.6064, 'grad_norm': 1.390625, 'learning_rate': 1e-05, 'epoch': 0.17}\n",
            "{'loss': 1.6865, 'grad_norm': 1.375, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.2}\n",
            "{'loss': 1.5592, 'grad_norm': 1.1953125, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.22}\n",
            "{'loss': 1.639, 'grad_norm': 1.1796875, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.24}\n",
            "{'loss': 1.6229, 'grad_norm': 1.28125, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.26}\n",
            "{'loss': 1.6696, 'grad_norm': 1.2578125, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.28}\n",
            "{'loss': 1.5967, 'grad_norm': 1.234375, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.3}\n",
            "{'loss': 1.66, 'grad_norm': 1.171875, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
            "{'loss': 1.6548, 'grad_norm': 1.1640625, 'learning_rate': 1.9996790752964305e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6493, 'grad_norm': 1.109375, 'learning_rate': 1.998716507171053e-05, 'epoch': 0.37}\n",
            "{'loss': 1.6086, 'grad_norm': 1.1484375, 'learning_rate': 1.9971129134476474e-05, 'epoch': 0.39}\n",
            "{'loss': 1.587, 'grad_norm': 1.09375, 'learning_rate': 1.994869323391895e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6133, 'grad_norm': 1.0859375, 'learning_rate': 1.991987177050743e-05, 'epoch': 0.44}\n",
            "{'loss': 1.5807, 'grad_norm': 1.0390625, 'learning_rate': 1.9884683243281117e-05, 'epoch': 0.46}\n",
            "{'loss': 1.5863, 'grad_norm': 0.9609375, 'learning_rate': 1.9843150237975343e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5755, 'grad_norm': 0.96875, 'learning_rate': 1.9795299412524948e-05, 'epoch': 0.5}\n",
            "{'loss': 1.563, 'grad_norm': 0.94921875, 'learning_rate': 1.9741161479953872e-05, 'epoch': 0.52}\n",
            "{'loss': 1.5954, 'grad_norm': 0.9375, 'learning_rate': 1.9680771188662044e-05, 'epoch': 0.54}\n",
            "{'loss': 1.6019, 'grad_norm': 0.9765625, 'learning_rate': 1.9614167300122126e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5502, 'grad_norm': 0.96875, 'learning_rate': 1.954139256400049e-05, 'epoch': 0.59}\n",
            "{'loss': 1.6226, 'grad_norm': 0.921875, 'learning_rate': 1.9462493690718373e-05, 'epoch': 0.61}\n",
            "{'loss': 1.5818, 'grad_norm': 0.90234375, 'learning_rate': 1.9377521321470806e-05, 'epoch': 0.63}\n",
            "{'loss': 1.5806, 'grad_norm': 0.89453125, 'learning_rate': 1.9286529995722624e-05, 'epoch': 0.65}\n",
            "{'loss': 1.6317, 'grad_norm': 0.89453125, 'learning_rate': 1.918957811620231e-05, 'epoch': 0.67}\n",
            "{'loss': 1.6333, 'grad_norm': 0.9296875, 'learning_rate': 1.908672791141625e-05, 'epoch': 0.7}\n",
            "{'loss': 1.6068, 'grad_norm': 0.89453125, 'learning_rate': 1.897804539570742e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5765, 'grad_norm': 0.8359375, 'learning_rate': 1.8863600326884085e-05, 'epoch': 0.74}\n",
            "{'loss': 1.5844, 'grad_norm': 0.8515625, 'learning_rate': 1.8743466161445823e-05, 'epoch': 0.76}\n",
            "{'loss': 1.5969, 'grad_norm': 0.78515625, 'learning_rate': 1.8617720007435497e-05, 'epoch': 0.78}\n",
            "{'loss': 1.7293, 'grad_norm': 0.85546875, 'learning_rate': 1.848644257494751e-05, 'epoch': 0.81}\n",
            "{'loss': 1.5751, 'grad_norm': 0.828125, 'learning_rate': 1.8349718124324075e-05, 'epoch': 0.83}\n",
            "{'loss': 1.603, 'grad_norm': 0.84765625, 'learning_rate': 1.8207634412072765e-05, 'epoch': 0.85}\n",
            "{'loss': 1.627, 'grad_norm': 0.80078125, 'learning_rate': 1.8060282634540053e-05, 'epoch': 0.87}\n",
            "{'loss': 1.6209, 'grad_norm': 0.79296875, 'learning_rate': 1.7907757369376984e-05, 'epoch': 0.89}\n",
            "{'loss': 1.5895, 'grad_norm': 0.83984375, 'learning_rate': 1.775015651483459e-05, 'epoch': 0.91}\n",
            "{'loss': 1.6359, 'grad_norm': 0.84765625, 'learning_rate': 1.758758122692791e-05, 'epoch': 0.94}\n",
            "{'loss': 1.5604, 'grad_norm': 0.75390625, 'learning_rate': 1.742013585450911e-05, 'epoch': 0.96}\n",
            "{'loss': 1.6512, 'grad_norm': 0.75, 'learning_rate': 1.72479278722912e-05, 'epoch': 0.98}\n",
            "{'loss': 1.5754, 'grad_norm': 0.78515625, 'learning_rate': 1.7071067811865477e-05, 'epoch': 1.0}\n",
            "{'loss': 1.6125, 'grad_norm': 0.75390625, 'learning_rate': 1.688966919075687e-05, 'epoch': 1.02}\n",
            "{'loss': 1.604, 'grad_norm': 0.77734375, 'learning_rate': 1.6703848439562787e-05, 'epoch': 1.04}\n",
            "{'loss': 1.5861, 'grad_norm': 0.73046875, 'learning_rate': 1.6513724827222225e-05, 'epoch': 1.07}\n",
            "{'loss': 1.6738, 'grad_norm': 0.80078125, 'learning_rate': 1.631942038446304e-05, 'epoch': 1.09}\n",
            "{'loss': 1.5962, 'grad_norm': 0.765625, 'learning_rate': 1.612105982547663e-05, 'epoch': 1.11}\n",
            "{'loss': 1.5809, 'grad_norm': 0.7421875, 'learning_rate': 1.5918770467870174e-05, 'epoch': 1.13}\n",
            "{'loss': 1.586, 'grad_norm': 0.7578125, 'learning_rate': 1.5712682150947926e-05, 'epoch': 1.15}\n",
            "{'loss': 1.5209, 'grad_norm': 0.7578125, 'learning_rate': 1.5502927152373913e-05, 'epoch': 1.17}\n",
            "{'loss': 1.5982, 'grad_norm': 0.76953125, 'learning_rate': 1.5289640103269626e-05, 'epoch': 1.2}\n",
            "{'loss': 1.6945, 'grad_norm': 0.7421875, 'learning_rate': 1.5072957901801075e-05, 'epoch': 1.22}\n",
            "{'loss': 1.5653, 'grad_norm': 0.72265625, 'learning_rate': 1.4853019625310813e-05, 'epoch': 1.24}\n",
            "{'loss': 1.5477, 'grad_norm': 0.7421875, 'learning_rate': 1.4629966441051208e-05, 'epoch': 1.26}\n",
            "{'loss': 1.525, 'grad_norm': 0.72265625, 'learning_rate': 1.4403941515576344e-05, 'epoch': 1.28}\n",
            "{'loss': 1.5715, 'grad_norm': 0.73828125, 'learning_rate': 1.4175089922850633e-05, 'epoch': 1.3}\n",
            "{'loss': 1.5547, 'grad_norm': 0.70703125, 'learning_rate': 1.3943558551133186e-05, 'epoch': 1.33}\n",
            "{'loss': 1.5454, 'grad_norm': 0.73828125, 'learning_rate': 1.370949600869768e-05, 'epoch': 1.35}\n",
            "{'loss': 1.5369, 'grad_norm': 0.72265625, 'learning_rate': 1.3473052528448203e-05, 'epoch': 1.37}\n",
            "{'loss': 1.5832, 'grad_norm': 0.734375, 'learning_rate': 1.3234379871492381e-05, 'epoch': 1.39}\n",
            "{'loss': 1.5609, 'grad_norm': 0.75, 'learning_rate': 1.2993631229733584e-05, 'epoch': 1.41}\n",
            "{'loss': 1.5281, 'grad_norm': 0.73828125, 'learning_rate': 1.2750961127544782e-05, 'epoch': 1.44}\n",
            "{'loss': 1.5247, 'grad_norm': 0.703125, 'learning_rate': 1.2506525322587207e-05, 'epoch': 1.46}\n",
            "{'loss': 1.633, 'grad_norm': 0.76171875, 'learning_rate': 1.226048070583735e-05, 'epoch': 1.48}\n",
            "{'loss': 1.5374, 'grad_norm': 0.66796875, 'learning_rate': 1.2012985200886602e-05, 'epoch': 1.5}\n",
            "{'loss': 1.546, 'grad_norm': 0.69921875, 'learning_rate': 1.1764197662578087e-05, 'epoch': 1.52}\n",
            "{'loss': 1.5387, 'grad_norm': 0.69921875, 'learning_rate': 1.1514277775045768e-05, 'epoch': 1.54}\n",
            "{'loss': 1.6854, 'grad_norm': 0.8046875, 'learning_rate': 1.1263385949221294e-05, 'epoch': 1.57}\n",
            "{'loss': 1.581, 'grad_norm': 0.69140625, 'learning_rate': 1.1011683219874324e-05, 'epoch': 1.59}\n",
            "{'loss': 1.5865, 'grad_norm': 0.71875, 'learning_rate': 1.0759331142252463e-05, 'epoch': 1.61}\n",
            "{'loss': 1.5812, 'grad_norm': 0.70703125, 'learning_rate': 1.0506491688387128e-05, 'epoch': 1.63}\n",
            "{'loss': 1.5746, 'grad_norm': 0.70703125, 'learning_rate': 1.025332714313188e-05, 'epoch': 1.65}\n",
            "{'loss': 1.5684, 'grad_norm': 0.73046875, 'learning_rate': 1e-05, 'epoch': 1.67}\n",
            "{'loss': 1.555, 'grad_norm': 0.66796875, 'learning_rate': 9.746672856868124e-06, 'epoch': 1.7}\n",
            "{'loss': 1.5294, 'grad_norm': 0.68359375, 'learning_rate': 9.493508311612874e-06, 'epoch': 1.72}\n",
            "{'loss': 1.5932, 'grad_norm': 0.69140625, 'learning_rate': 9.24066885774754e-06, 'epoch': 1.74}\n",
            "{'loss': 1.546, 'grad_norm': 0.7265625, 'learning_rate': 8.98831678012568e-06, 'epoch': 1.76}\n",
            "{'loss': 1.5882, 'grad_norm': 0.69921875, 'learning_rate': 8.73661405077871e-06, 'epoch': 1.78}\n",
            "{'loss': 1.5645, 'grad_norm': 0.69921875, 'learning_rate': 8.485722224954237e-06, 'epoch': 1.81}\n",
            "{'loss': 1.5975, 'grad_norm': 0.734375, 'learning_rate': 8.23580233742192e-06, 'epoch': 1.83}\n",
            "{'loss': 1.5939, 'grad_norm': 0.69140625, 'learning_rate': 7.987014799113398e-06, 'epoch': 1.85}\n",
            "{'loss': 1.5486, 'grad_norm': 0.64453125, 'learning_rate': 7.739519294162652e-06, 'epoch': 1.87}\n",
            "{'loss': 1.5327, 'grad_norm': 0.70703125, 'learning_rate': 7.493474677412795e-06, 'epoch': 1.89}\n",
            "{'loss': 1.5216, 'grad_norm': 0.71875, 'learning_rate': 7.24903887245522e-06, 'epoch': 1.91}\n",
            "{'loss': 1.527, 'grad_norm': 0.671875, 'learning_rate': 7.006368770266421e-06, 'epoch': 1.94}\n",
            "{'loss': 1.5839, 'grad_norm': 0.69921875, 'learning_rate': 6.7656201285076195e-06, 'epoch': 1.96}\n",
            "{'loss': 1.6167, 'grad_norm': 0.703125, 'learning_rate': 6.526947471551799e-06, 'epoch': 1.98}\n",
            "{'loss': 1.5528, 'grad_norm': 0.6953125, 'learning_rate': 6.290503991302324e-06, 'epoch': 2.0}\n",
            "{'loss': 1.6211, 'grad_norm': 0.69921875, 'learning_rate': 6.056441448866817e-06, 'epoch': 2.02}\n",
            "{'loss': 1.5138, 'grad_norm': 0.703125, 'learning_rate': 5.824910077149372e-06, 'epoch': 2.04}\n",
            "{'loss': 1.5518, 'grad_norm': 0.6796875, 'learning_rate': 5.5960584844236565e-06, 'epoch': 2.07}\n",
            "{'loss': 1.5187, 'grad_norm': 0.66015625, 'learning_rate': 5.370033558948793e-06, 'epoch': 2.09}\n",
            "{'loss': 1.5643, 'grad_norm': 0.69140625, 'learning_rate': 5.146980374689192e-06, 'epoch': 2.11}\n",
            "{'loss': 1.6129, 'grad_norm': 0.703125, 'learning_rate': 4.9270420981989295e-06, 'epoch': 2.13}\n",
            "{'loss': 1.5449, 'grad_norm': 0.7265625, 'learning_rate': 4.710359896730379e-06, 'epoch': 2.15}\n",
            "{'loss': 1.5684, 'grad_norm': 0.6953125, 'learning_rate': 4.497072847626087e-06, 'epoch': 2.17}\n",
            "{'loss': 1.5152, 'grad_norm': 0.66796875, 'learning_rate': 4.287317849052075e-06, 'epoch': 2.2}\n",
            "{'loss': 1.5863, 'grad_norm': 0.66796875, 'learning_rate': 4.081229532129826e-06, 'epoch': 2.22}\n",
            "{'loss': 1.588, 'grad_norm': 0.69140625, 'learning_rate': 3.878940174523371e-06, 'epoch': 2.24}\n",
            "{'loss': 1.6067, 'grad_norm': 0.7265625, 'learning_rate': 3.680579615536961e-06, 'epoch': 2.26}\n",
            "{'loss': 1.5982, 'grad_norm': 0.7109375, 'learning_rate': 3.48627517277778e-06, 'epoch': 2.28}\n",
            "{'loss': 1.5615, 'grad_norm': 0.73828125, 'learning_rate': 3.296151560437214e-06, 'epoch': 2.3}\n",
            "{'loss': 1.5684, 'grad_norm': 0.703125, 'learning_rate': 3.110330809243134e-06, 'epoch': 2.33}\n",
            "{'loss': 1.5677, 'grad_norm': 0.70703125, 'learning_rate': 2.9289321881345257e-06, 'epoch': 2.35}\n",
            "{'loss': 1.5773, 'grad_norm': 0.71875, 'learning_rate': 2.7520721277088023e-06, 'epoch': 2.37}\n",
            "{'loss': 1.5303, 'grad_norm': 0.74609375, 'learning_rate': 2.5798641454908945e-06, 'epoch': 2.39}\n",
            "{'loss': 1.568, 'grad_norm': 0.69140625, 'learning_rate': 2.4124187730720916e-06, 'epoch': 2.41}\n",
            "{'loss': 1.5919, 'grad_norm': 0.66796875, 'learning_rate': 2.2498434851654125e-06, 'epoch': 2.44}\n",
            "{'loss': 1.582, 'grad_norm': 0.6953125, 'learning_rate': 2.092242630623016e-06, 'epoch': 2.46}\n",
            "{'loss': 1.5504, 'grad_norm': 0.68359375, 'learning_rate': 1.939717365459952e-06, 'epoch': 2.48}\n",
            "{'loss': 1.6357, 'grad_norm': 0.7265625, 'learning_rate': 1.7923655879272395e-06, 'epoch': 2.5}\n",
            "{'loss': 1.6043, 'grad_norm': 0.6953125, 'learning_rate': 1.6502818756759275e-06, 'epoch': 2.52}\n",
            "{'loss': 1.5852, 'grad_norm': 0.6953125, 'learning_rate': 1.5135574250524898e-06, 'epoch': 2.54}\n",
            "{'loss': 1.4942, 'grad_norm': 0.72265625, 'learning_rate': 1.3822799925645036e-06, 'epoch': 2.57}\n",
            "{'loss': 1.5318, 'grad_norm': 0.6875, 'learning_rate': 1.2565338385541792e-06, 'epoch': 2.59}\n",
            "{'loss': 1.5348, 'grad_norm': 0.72265625, 'learning_rate': 1.1363996731159188e-06, 'epoch': 2.61}\n",
            "{'loss': 1.5637, 'grad_norm': 0.69140625, 'learning_rate': 1.0219546042925842e-06, 'epoch': 2.63}\n",
            "{'loss': 1.563, 'grad_norm': 0.703125, 'learning_rate': 9.132720885837509e-07, 'epoch': 2.65}\n",
            "{'loss': 1.6325, 'grad_norm': 0.74609375, 'learning_rate': 8.10421883797694e-07, 'epoch': 2.67}\n",
            "{'loss': 1.5447, 'grad_norm': 0.69921875, 'learning_rate': 7.13470004277379e-07, 'epoch': 2.7}\n",
            "{'loss': 1.6078, 'grad_norm': 0.73046875, 'learning_rate': 6.22478678529197e-07, 'epoch': 2.72}\n",
            "{'loss': 1.6189, 'grad_norm': 0.71875, 'learning_rate': 5.375063092816313e-07, 'epoch': 2.74}\n",
            "{'loss': 1.5905, 'grad_norm': 0.68359375, 'learning_rate': 4.5860743599951186e-07, 'epoch': 2.76}\n",
            "{'loss': 1.5195, 'grad_norm': 0.6953125, 'learning_rate': 3.8583269987787607e-07, 'epoch': 2.78}\n",
            "{'loss': 1.5692, 'grad_norm': 0.70703125, 'learning_rate': 3.1922881133795827e-07, 'epoch': 2.81}\n",
            "{'loss': 1.5509, 'grad_norm': 0.7265625, 'learning_rate': 2.588385200461307e-07, 'epoch': 2.83}\n",
            "{'loss': 1.5252, 'grad_norm': 0.69921875, 'learning_rate': 2.0470058747505516e-07, 'epoch': 2.85}\n",
            "{'loss': 1.5468, 'grad_norm': 0.71484375, 'learning_rate': 1.5684976202465786e-07, 'epoch': 2.87}\n",
            "{'loss': 1.5174, 'grad_norm': 0.70703125, 'learning_rate': 1.1531675671888621e-07, 'epoch': 2.89}\n",
            "{'loss': 1.5503, 'grad_norm': 0.703125, 'learning_rate': 8.012822949256981e-08, 'epoch': 2.91}\n",
            "{'loss': 1.5394, 'grad_norm': 0.7578125, 'learning_rate': 5.1306766081048456e-08, 'epoch': 2.94}\n",
            "{'loss': 1.5757, 'grad_norm': 0.67578125, 'learning_rate': 2.8870865523525916e-08, 'epoch': 2.96}\n",
            "{'loss': 1.5586, 'grad_norm': 0.66796875, 'learning_rate': 1.2834928289472415e-08, 'epoch': 2.98}\n",
            "{'loss': 1.6142, 'grad_norm': 0.7109375, 'learning_rate': 3.209247035694807e-09, 'epoch': 3.0}\n",
            "{'train_runtime': 3041.347, 'train_samples_per_second': 5.8, 'train_steps_per_second': 0.045, 'train_loss': 1.5843606375265813, 'epoch': 3.0}\n",
            "100% 138/138 [50:36<00:00, 22.00s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mHW2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_072516-39d60x8o/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JkLh9zWv9jLz",
      "metadata": {
        "id": "JkLh9zWv9jLz"
      },
      "source": [
        "### Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate base model"
      ],
      "metadata": {
        "id": "moQp_TDwcmSE"
      },
      "id": "moQp_TDwcmSE"
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../SmolLM2-135M ./results/SmolLM2-135M"
      ],
      "metadata": {
        "id": "Fx0k4X64cp1d",
        "outputId": "be05b2bc-f48b-4cde-acfa-857959f487b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Fx0k4X64cp1d",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-03 09:26:58.782060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 09:26:58.799149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762162018.820343    2775 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762162018.826755    2775 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762162018.843022    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843053    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843056    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843059    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 09:26:58.847862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-03 09:27:07 [__init__.py:216] Automatically detected platform cuda.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "INFO 11-03 09:27:13 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../SmolLM2-135M'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-03 09:27:30 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-03 09:27:30 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-03 09:27:33 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-03 09:27:33 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-03 09:27:37.860910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762162057.881507    3034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762162057.887969    3034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762162057.903635    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903659    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903662    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903664    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-03 09:27:45 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:46 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:46 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../SmolLM2-135M', speculative_config=None, tokenizer='../SmolLM2-135M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../SmolLM2-135M, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1103 09:27:49.993825269 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m WARNING 11-03 09:27:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [gpu_model_runner.py:2338] Starting to load model ../SmolLM2-135M...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:07<00:00,  7.05s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:56 [default_loader.py:268] Loading weights took 7.06 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:57 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 7.396194 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:28:05 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/34b05a8e0e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:28:06 [backends.py:550] Dynamo bytecode transform time: 8.03 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:34:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 409.423 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:08 [monitor.py:34] torch.compile takes 8.03 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:09 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:10 [kv_cache_utils.py:864] GPU KV cache size: 688,672 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:10 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 26.93it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15240215347` to fit into requested memory, or `--kv-cache-memory=22148614144` to fully utilize gpu memory. Current kv cache memory in use is 15867263795 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [core.py:218] init engine (profile, create kv cache, warmup model) took 436.17 seconds\n",
            "INFO 11-03 09:35:14 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-03 09:35:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 2084.41it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.99it/s, est. speed input: 404.58 toks/s, output: 9173.99 toks/s]\n",
            "[rank0]:[W1103 09:35:26.203615151 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arvWLhUL9tDy",
      "metadata": {
        "id": "arvWLhUL9tDy"
      },
      "source": [
        "#### My Hyperparameter Tuning 1 in Q6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/HW2 ./results/SmolLM2-135M-SFT-1"
      ],
      "metadata": {
        "id": "BzGaO2Y66fkJ",
        "outputId": "9cdf6bfe-9143-4391-f666-be510b1379f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BzGaO2Y66fkJ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-03 13:40:42.336827: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 13:40:42.353764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762177242.375098    2706 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762177242.381551    2706 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762177242.397832    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397864    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397867    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397869    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 13:40:42.402750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-03 13:40:50 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-03 13:41:00 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../ckpt/HW2'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-03 13:41:18 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-03 13:41:18 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-03 13:41:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-03 13:41:21 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-03 13:41:25.824521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762177285.845840    2988 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762177285.852368    2988 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762177285.868907    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868935    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868938    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868940    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-03 13:41:33 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:34 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:34 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../ckpt/HW2', speculative_config=None, tokenizer='../ckpt/HW2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../ckpt/HW2, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1103 13:41:37.506504543 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m WARNING 11-03 13:41:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [gpu_model_runner.py:2338] Starting to load model ../ckpt/HW2...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:08<00:00,  8.37s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:46 [default_loader.py:268] Loading weights took 8.37 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:47 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 8.739695 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:55 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/36f5027955/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:55 [backends.py:550] Dynamo bytecode transform time: 7.72 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m [rank0]:W1103 13:41:56.526000 2988 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:59 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:23 [backends.py:215] Compiling a graph for dynamic shape takes 27.44 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:25 [monitor.py:34] torch.compile takes 35.16 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:26 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:27 [kv_cache_utils.py:864] GPU KV cache size: 688,720 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:27 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 25.62it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15241394995` to fit into requested memory, or `--kv-cache-memory=22149793792` to fully utilize gpu memory. Current kv cache memory in use is 15868443443 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [core.py:218] init engine (profile, create kv cache, warmup model) took 43.57 seconds\n",
            "INFO 11-03 13:42:31 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-03 13:42:31 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 1833.30it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.82it/s, est. speed input: 485.08 toks/s, output: 8946.33 toks/s]\n",
            "[rank0]:[W1103 13:42:43.363768373 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "ERROR:absl:Unable to detect language for text ## 1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1. due to No features in text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### My Hyperparameter Tuning 2"
      ],
      "metadata": {
        "id": "hu4Vf1Df6a61"
      },
      "id": "hu4Vf1Df6a61"
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh --output_dir ckpt/FT2 --learning_rate 5e-6"
      ],
      "metadata": {
        "id": "GwocvS_6dBr1",
        "outputId": "1f727106-a549-4677-f201-9091d4149c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GwocvS_6dBr1",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps\n",
            "2025-11-03 13:53:08.523911: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 13:53:08.541665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762177988.563390    6506 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762177988.569960    6506 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762177988.586368    6506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177988.586398    6506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177988.586401    6506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177988.586409    6506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 13:53:08.591308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run n3dm0r47 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run n3dm0r47 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251103_135327-n3dm0r47\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHW2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/n3dm0r47\u001b[0m\n",
            "{'train_runtime': 1.9968, 'train_samples_per_second': 8834.17, 'train_steps_per_second': 69.111, 'train_loss': 0.0, 'epoch': 3.0}\n",
            "  0% 0/138 [00:00<?, ?it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mHW2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_135327-n3dm0r47/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd ifeval && bash run.sh SmolLM2-135M-SFT-1 ./results/SmolLM2-135M-SFT-1"
      ],
      "metadata": {
        "id": "EtfCHfr6c1IC"
      },
      "id": "EtfCHfr6c1IC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}