{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d3b43e0",
      "metadata": {
        "id": "8d3b43e0"
      },
      "source": [
        "###  Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45c19e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c19e77",
        "outputId": "07230d26-339d-4a4f-ee9d-6308019cf811",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420dc8b7",
      "metadata": {
        "id": "420dc8b7"
      },
      "source": [
        "### Clone Codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20bdbde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20bdbde",
        "outputId": "7a4426a2-aed8-45b9-bdb0-8ed495c2cdf7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'COMP4901B-LLMs'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 155 (delta 24), reused 19 (delta 19), pack-reused 118 (from 1)\u001b[K\n",
            "Receiving objects: 100% (155/155), 716.72 KiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework3\n",
        "! cd /content/drive/MyDrive/COMP4901B-Homework3 && git clone https://github.com/hkust-nlp/COMP4901B-LLMs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c6cfe0",
      "metadata": {
        "id": "29c6cfe0"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5510016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5510016",
        "outputId": "0da50778-f72c-41e5-bcaa-c1b975a40106",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (1.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]==4.57.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]==4.57.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]==4.57.1) (3.0.3)\n",
            "Collecting vllm==0.10.2\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.10.2)\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.120.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.13.1)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm==0.10.2)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm==0.10.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.10.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm==0.10.2)\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.10.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.10.2)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm==0.10.2)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm==0.10.2)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm==0.10.2)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.1.1)\n",
            "Collecting watchfiles (from vllm==0.10.2)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.16.3)\n",
            "Collecting ninja (from vllm==0.10.2)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm==0.10.2)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm==0.10.2)\n",
            "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm==0.10.2)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2)\n",
            "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm==0.10.2)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm==0.10.2)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm==0.10.2) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm==0.10.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm==0.10.2) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm==0.10.2) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.49.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.3)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cli-0.0.14-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.4.2)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm==0.10.2) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm==0.10.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm==0.10.2) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.22.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.20.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.2) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm==0.10.2) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.28.0)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm==0.10.2) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rignore-0.7.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.42.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.1.2)\n",
            "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl (71.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.14-py3-none-any.whl (11 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, click, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.8 cbor2-5.7.1 click-8.2.1 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.14 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.4 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.51.1 rich-toolkit-0.15.1 rignore-0.7.4 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.1.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.1-py3-none-any.whl size=1764315 sha256=616934211fa685b39f302aa4b48a046a22de818cbed4b9b57624e80706944565\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/41/59/a9d46caf09e118b9276f33e0f6d502a45b1e455296e98a2a11\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: deepspeed\n",
            "Successfully installed deepspeed-0.18.1\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hjson\n",
            "Successfully installed hjson-3.1.0\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (4.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=d5e7f40f2e1cbdc05ce15050b45afad94c3d440a8ddd05d1d9620c2f9e776124\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "--2025-11-05 07:26:37--  https://huggingface.co/datasets/PeterV09/smol-smoltalk-6k/resolve/main/smol-smoltalk-6k.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/68f5a0e304d834b9230fe31e/ee26a26f39a0bad5f07c70cb78dd7c0cf86dd6a9b52e1012d2616e7f7b7ef244?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251105T072638Z&X-Amz-Expires=3600&X-Amz-Signature=47aebb7830dc9435e824981037d5e9a122daa4fb4cfa9be1797944252e372a75&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27smol-smoltalk-6k.json%3B+filename%3D%22smol-smoltalk-6k.json%22%3B&response-content-type=application%2Fjson&x-id=GetObject&Expires=1762331198&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjMzMTE5OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OGY1YTBlMzA0ZDgzNGI5MjMwZmUzMWUvZWUyNmEyNmYzOWEwYmFkNWYwN2M3MGNiNzhkZDdjMGNmODZkZDZhOWI1MmUxMDEyZDI2MTZlN2Y3YjdlZjI0NCoifV19&Signature=cEfYEmVyBQJ0ANp62Ynd7thOkjzvFc%7EXjDob7s%7E4lqftq5JYwN4IfOWzyZwmoQZyi0zKG6Jml20cU2B139Is3weRFbrHCCZ4ZgIH80AxMtz4ApID1a1fuyv9SBPq1jwEdlw6wWBGALHM5lx9bXog-35dc%7EjUW1jsCmNXIwpYz-0A8Nm9zeROXDFNaMwHfWL1Wr6nIuRM5mouu28qFQMLoHGyAxty-DLX8nvrxfs7LkeRvr%7EztKoFFovs-18FEGFZx1hghnGKihp7NDGOOsEeKJau87T4EQ0o7l0kI-9TMB95zkK5L6TUeMtJ3e8gR9wbJsyBSxx5dtB97h-rIrvaow__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-05 07:26:38--  https://cas-bridge.xethub.hf.co/xet-bridge-us/68f5a0e304d834b9230fe31e/ee26a26f39a0bad5f07c70cb78dd7c0cf86dd6a9b52e1012d2616e7f7b7ef244?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251105T072638Z&X-Amz-Expires=3600&X-Amz-Signature=47aebb7830dc9435e824981037d5e9a122daa4fb4cfa9be1797944252e372a75&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27smol-smoltalk-6k.json%3B+filename%3D%22smol-smoltalk-6k.json%22%3B&response-content-type=application%2Fjson&x-id=GetObject&Expires=1762331198&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjMzMTE5OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OGY1YTBlMzA0ZDgzNGI5MjMwZmUzMWUvZWUyNmEyNmYzOWEwYmFkNWYwN2M3MGNiNzhkZDdjMGNmODZkZDZhOWI1MmUxMDEyZDI2MTZlN2Y3YjdlZjI0NCoifV19&Signature=cEfYEmVyBQJ0ANp62Ynd7thOkjzvFc%7EXjDob7s%7E4lqftq5JYwN4IfOWzyZwmoQZyi0zKG6Jml20cU2B139Is3weRFbrHCCZ4ZgIH80AxMtz4ApID1a1fuyv9SBPq1jwEdlw6wWBGALHM5lx9bXog-35dc%7EjUW1jsCmNXIwpYz-0A8Nm9zeROXDFNaMwHfWL1Wr6nIuRM5mouu28qFQMLoHGyAxty-DLX8nvrxfs7LkeRvr%7EztKoFFovs-18FEGFZx1hghnGKihp7NDGOOsEeKJau87T4EQ0o7l0kI-9TMB95zkK5L6TUeMtJ3e8gR9wbJsyBSxx5dtB97h-rIrvaow__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.165.102.90, 3.165.102.48, 3.165.102.125, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.165.102.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24144050 (23M) [application/json]\n",
            "Saving to: ‘smol-smoltalk-6k.json.6’\n",
            "\n",
            "smol-smoltalk-6k.js 100%[===================>]  23.03M  56.2MB/s    in 0.4s    \n",
            "\n",
            "2025-11-05 07:26:38 (56.2 MB/s) - ‘smol-smoltalk-6k.json.6’ saved [24144050/24144050]\n",
            "\n",
            "Fetching 10 files: 100% 10/10 [00:03<00:00,  2.85it/s]\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment3 && bash scripts/setup.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe18aa7",
      "metadata": {
        "id": "dfe18aa7"
      },
      "source": [
        "### Q3 Start The Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f6abed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76f6abed",
        "outputId": "02ae3727-8bb0-4348-fb6c-94863570c60a",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps\n",
            "2025-11-03 07:25:01.595043: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 07:25:01.612700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762154701.633810    2214 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762154701.640328    2214 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762154701.656704    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656732    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656735    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762154701.656737    2214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 07:25:01.661599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251103_072516-39d60x8o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHW2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/39d60x8o\u001b[0m\n",
            "{'loss': 1.7031, 'grad_norm': 1.328125, 'learning_rate': 0.0, 'epoch': 0.02}\n",
            "{'loss': 1.5794, 'grad_norm': 1.2734375, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.04}\n",
            "{'loss': 1.6427, 'grad_norm': 1.3671875, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.07}\n",
            "{'loss': 1.6225, 'grad_norm': 1.2578125, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.09}\n",
            "{'loss': 1.6573, 'grad_norm': 1.25, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.11}\n",
            "{'loss': 1.6054, 'grad_norm': 1.3125, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.13}\n",
            "{'loss': 1.6017, 'grad_norm': 1.25, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.15}\n",
            "{'loss': 1.6064, 'grad_norm': 1.390625, 'learning_rate': 1e-05, 'epoch': 0.17}\n",
            "{'loss': 1.6865, 'grad_norm': 1.375, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.2}\n",
            "{'loss': 1.5592, 'grad_norm': 1.1953125, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.22}\n",
            "{'loss': 1.639, 'grad_norm': 1.1796875, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.24}\n",
            "{'loss': 1.6229, 'grad_norm': 1.28125, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.26}\n",
            "{'loss': 1.6696, 'grad_norm': 1.2578125, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.28}\n",
            "{'loss': 1.5967, 'grad_norm': 1.234375, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.3}\n",
            "{'loss': 1.66, 'grad_norm': 1.171875, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
            "{'loss': 1.6548, 'grad_norm': 1.1640625, 'learning_rate': 1.9996790752964305e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6493, 'grad_norm': 1.109375, 'learning_rate': 1.998716507171053e-05, 'epoch': 0.37}\n",
            "{'loss': 1.6086, 'grad_norm': 1.1484375, 'learning_rate': 1.9971129134476474e-05, 'epoch': 0.39}\n",
            "{'loss': 1.587, 'grad_norm': 1.09375, 'learning_rate': 1.994869323391895e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6133, 'grad_norm': 1.0859375, 'learning_rate': 1.991987177050743e-05, 'epoch': 0.44}\n",
            "{'loss': 1.5807, 'grad_norm': 1.0390625, 'learning_rate': 1.9884683243281117e-05, 'epoch': 0.46}\n",
            "{'loss': 1.5863, 'grad_norm': 0.9609375, 'learning_rate': 1.9843150237975343e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5755, 'grad_norm': 0.96875, 'learning_rate': 1.9795299412524948e-05, 'epoch': 0.5}\n",
            "{'loss': 1.563, 'grad_norm': 0.94921875, 'learning_rate': 1.9741161479953872e-05, 'epoch': 0.52}\n",
            "{'loss': 1.5954, 'grad_norm': 0.9375, 'learning_rate': 1.9680771188662044e-05, 'epoch': 0.54}\n",
            "{'loss': 1.6019, 'grad_norm': 0.9765625, 'learning_rate': 1.9614167300122126e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5502, 'grad_norm': 0.96875, 'learning_rate': 1.954139256400049e-05, 'epoch': 0.59}\n",
            "{'loss': 1.6226, 'grad_norm': 0.921875, 'learning_rate': 1.9462493690718373e-05, 'epoch': 0.61}\n",
            "{'loss': 1.5818, 'grad_norm': 0.90234375, 'learning_rate': 1.9377521321470806e-05, 'epoch': 0.63}\n",
            "{'loss': 1.5806, 'grad_norm': 0.89453125, 'learning_rate': 1.9286529995722624e-05, 'epoch': 0.65}\n",
            "{'loss': 1.6317, 'grad_norm': 0.89453125, 'learning_rate': 1.918957811620231e-05, 'epoch': 0.67}\n",
            "{'loss': 1.6333, 'grad_norm': 0.9296875, 'learning_rate': 1.908672791141625e-05, 'epoch': 0.7}\n",
            "{'loss': 1.6068, 'grad_norm': 0.89453125, 'learning_rate': 1.897804539570742e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5765, 'grad_norm': 0.8359375, 'learning_rate': 1.8863600326884085e-05, 'epoch': 0.74}\n",
            "{'loss': 1.5844, 'grad_norm': 0.8515625, 'learning_rate': 1.8743466161445823e-05, 'epoch': 0.76}\n",
            "{'loss': 1.5969, 'grad_norm': 0.78515625, 'learning_rate': 1.8617720007435497e-05, 'epoch': 0.78}\n",
            "{'loss': 1.7293, 'grad_norm': 0.85546875, 'learning_rate': 1.848644257494751e-05, 'epoch': 0.81}\n",
            "{'loss': 1.5751, 'grad_norm': 0.828125, 'learning_rate': 1.8349718124324075e-05, 'epoch': 0.83}\n",
            "{'loss': 1.603, 'grad_norm': 0.84765625, 'learning_rate': 1.8207634412072765e-05, 'epoch': 0.85}\n",
            "{'loss': 1.627, 'grad_norm': 0.80078125, 'learning_rate': 1.8060282634540053e-05, 'epoch': 0.87}\n",
            "{'loss': 1.6209, 'grad_norm': 0.79296875, 'learning_rate': 1.7907757369376984e-05, 'epoch': 0.89}\n",
            "{'loss': 1.5895, 'grad_norm': 0.83984375, 'learning_rate': 1.775015651483459e-05, 'epoch': 0.91}\n",
            "{'loss': 1.6359, 'grad_norm': 0.84765625, 'learning_rate': 1.758758122692791e-05, 'epoch': 0.94}\n",
            "{'loss': 1.5604, 'grad_norm': 0.75390625, 'learning_rate': 1.742013585450911e-05, 'epoch': 0.96}\n",
            "{'loss': 1.6512, 'grad_norm': 0.75, 'learning_rate': 1.72479278722912e-05, 'epoch': 0.98}\n",
            "{'loss': 1.5754, 'grad_norm': 0.78515625, 'learning_rate': 1.7071067811865477e-05, 'epoch': 1.0}\n",
            "{'loss': 1.6125, 'grad_norm': 0.75390625, 'learning_rate': 1.688966919075687e-05, 'epoch': 1.02}\n",
            "{'loss': 1.604, 'grad_norm': 0.77734375, 'learning_rate': 1.6703848439562787e-05, 'epoch': 1.04}\n",
            "{'loss': 1.5861, 'grad_norm': 0.73046875, 'learning_rate': 1.6513724827222225e-05, 'epoch': 1.07}\n",
            "{'loss': 1.6738, 'grad_norm': 0.80078125, 'learning_rate': 1.631942038446304e-05, 'epoch': 1.09}\n",
            "{'loss': 1.5962, 'grad_norm': 0.765625, 'learning_rate': 1.612105982547663e-05, 'epoch': 1.11}\n",
            "{'loss': 1.5809, 'grad_norm': 0.7421875, 'learning_rate': 1.5918770467870174e-05, 'epoch': 1.13}\n",
            "{'loss': 1.586, 'grad_norm': 0.7578125, 'learning_rate': 1.5712682150947926e-05, 'epoch': 1.15}\n",
            "{'loss': 1.5209, 'grad_norm': 0.7578125, 'learning_rate': 1.5502927152373913e-05, 'epoch': 1.17}\n",
            "{'loss': 1.5982, 'grad_norm': 0.76953125, 'learning_rate': 1.5289640103269626e-05, 'epoch': 1.2}\n",
            "{'loss': 1.6945, 'grad_norm': 0.7421875, 'learning_rate': 1.5072957901801075e-05, 'epoch': 1.22}\n",
            "{'loss': 1.5653, 'grad_norm': 0.72265625, 'learning_rate': 1.4853019625310813e-05, 'epoch': 1.24}\n",
            "{'loss': 1.5477, 'grad_norm': 0.7421875, 'learning_rate': 1.4629966441051208e-05, 'epoch': 1.26}\n",
            "{'loss': 1.525, 'grad_norm': 0.72265625, 'learning_rate': 1.4403941515576344e-05, 'epoch': 1.28}\n",
            "{'loss': 1.5715, 'grad_norm': 0.73828125, 'learning_rate': 1.4175089922850633e-05, 'epoch': 1.3}\n",
            "{'loss': 1.5547, 'grad_norm': 0.70703125, 'learning_rate': 1.3943558551133186e-05, 'epoch': 1.33}\n",
            "{'loss': 1.5454, 'grad_norm': 0.73828125, 'learning_rate': 1.370949600869768e-05, 'epoch': 1.35}\n",
            "{'loss': 1.5369, 'grad_norm': 0.72265625, 'learning_rate': 1.3473052528448203e-05, 'epoch': 1.37}\n",
            "{'loss': 1.5832, 'grad_norm': 0.734375, 'learning_rate': 1.3234379871492381e-05, 'epoch': 1.39}\n",
            "{'loss': 1.5609, 'grad_norm': 0.75, 'learning_rate': 1.2993631229733584e-05, 'epoch': 1.41}\n",
            "{'loss': 1.5281, 'grad_norm': 0.73828125, 'learning_rate': 1.2750961127544782e-05, 'epoch': 1.44}\n",
            "{'loss': 1.5247, 'grad_norm': 0.703125, 'learning_rate': 1.2506525322587207e-05, 'epoch': 1.46}\n",
            "{'loss': 1.633, 'grad_norm': 0.76171875, 'learning_rate': 1.226048070583735e-05, 'epoch': 1.48}\n",
            "{'loss': 1.5374, 'grad_norm': 0.66796875, 'learning_rate': 1.2012985200886602e-05, 'epoch': 1.5}\n",
            "{'loss': 1.546, 'grad_norm': 0.69921875, 'learning_rate': 1.1764197662578087e-05, 'epoch': 1.52}\n",
            "{'loss': 1.5387, 'grad_norm': 0.69921875, 'learning_rate': 1.1514277775045768e-05, 'epoch': 1.54}\n",
            "{'loss': 1.6854, 'grad_norm': 0.8046875, 'learning_rate': 1.1263385949221294e-05, 'epoch': 1.57}\n",
            "{'loss': 1.581, 'grad_norm': 0.69140625, 'learning_rate': 1.1011683219874324e-05, 'epoch': 1.59}\n",
            "{'loss': 1.5865, 'grad_norm': 0.71875, 'learning_rate': 1.0759331142252463e-05, 'epoch': 1.61}\n",
            "{'loss': 1.5812, 'grad_norm': 0.70703125, 'learning_rate': 1.0506491688387128e-05, 'epoch': 1.63}\n",
            "{'loss': 1.5746, 'grad_norm': 0.70703125, 'learning_rate': 1.025332714313188e-05, 'epoch': 1.65}\n",
            "{'loss': 1.5684, 'grad_norm': 0.73046875, 'learning_rate': 1e-05, 'epoch': 1.67}\n",
            "{'loss': 1.555, 'grad_norm': 0.66796875, 'learning_rate': 9.746672856868124e-06, 'epoch': 1.7}\n",
            "{'loss': 1.5294, 'grad_norm': 0.68359375, 'learning_rate': 9.493508311612874e-06, 'epoch': 1.72}\n",
            "{'loss': 1.5932, 'grad_norm': 0.69140625, 'learning_rate': 9.24066885774754e-06, 'epoch': 1.74}\n",
            "{'loss': 1.546, 'grad_norm': 0.7265625, 'learning_rate': 8.98831678012568e-06, 'epoch': 1.76}\n",
            "{'loss': 1.5882, 'grad_norm': 0.69921875, 'learning_rate': 8.73661405077871e-06, 'epoch': 1.78}\n",
            "{'loss': 1.5645, 'grad_norm': 0.69921875, 'learning_rate': 8.485722224954237e-06, 'epoch': 1.81}\n",
            "{'loss': 1.5975, 'grad_norm': 0.734375, 'learning_rate': 8.23580233742192e-06, 'epoch': 1.83}\n",
            "{'loss': 1.5939, 'grad_norm': 0.69140625, 'learning_rate': 7.987014799113398e-06, 'epoch': 1.85}\n",
            "{'loss': 1.5486, 'grad_norm': 0.64453125, 'learning_rate': 7.739519294162652e-06, 'epoch': 1.87}\n",
            "{'loss': 1.5327, 'grad_norm': 0.70703125, 'learning_rate': 7.493474677412795e-06, 'epoch': 1.89}\n",
            "{'loss': 1.5216, 'grad_norm': 0.71875, 'learning_rate': 7.24903887245522e-06, 'epoch': 1.91}\n",
            "{'loss': 1.527, 'grad_norm': 0.671875, 'learning_rate': 7.006368770266421e-06, 'epoch': 1.94}\n",
            "{'loss': 1.5839, 'grad_norm': 0.69921875, 'learning_rate': 6.7656201285076195e-06, 'epoch': 1.96}\n",
            "{'loss': 1.6167, 'grad_norm': 0.703125, 'learning_rate': 6.526947471551799e-06, 'epoch': 1.98}\n",
            "{'loss': 1.5528, 'grad_norm': 0.6953125, 'learning_rate': 6.290503991302324e-06, 'epoch': 2.0}\n",
            "{'loss': 1.6211, 'grad_norm': 0.69921875, 'learning_rate': 6.056441448866817e-06, 'epoch': 2.02}\n",
            "{'loss': 1.5138, 'grad_norm': 0.703125, 'learning_rate': 5.824910077149372e-06, 'epoch': 2.04}\n",
            "{'loss': 1.5518, 'grad_norm': 0.6796875, 'learning_rate': 5.5960584844236565e-06, 'epoch': 2.07}\n",
            "{'loss': 1.5187, 'grad_norm': 0.66015625, 'learning_rate': 5.370033558948793e-06, 'epoch': 2.09}\n",
            "{'loss': 1.5643, 'grad_norm': 0.69140625, 'learning_rate': 5.146980374689192e-06, 'epoch': 2.11}\n",
            "{'loss': 1.6129, 'grad_norm': 0.703125, 'learning_rate': 4.9270420981989295e-06, 'epoch': 2.13}\n",
            "{'loss': 1.5449, 'grad_norm': 0.7265625, 'learning_rate': 4.710359896730379e-06, 'epoch': 2.15}\n",
            "{'loss': 1.5684, 'grad_norm': 0.6953125, 'learning_rate': 4.497072847626087e-06, 'epoch': 2.17}\n",
            "{'loss': 1.5152, 'grad_norm': 0.66796875, 'learning_rate': 4.287317849052075e-06, 'epoch': 2.2}\n",
            "{'loss': 1.5863, 'grad_norm': 0.66796875, 'learning_rate': 4.081229532129826e-06, 'epoch': 2.22}\n",
            "{'loss': 1.588, 'grad_norm': 0.69140625, 'learning_rate': 3.878940174523371e-06, 'epoch': 2.24}\n",
            "{'loss': 1.6067, 'grad_norm': 0.7265625, 'learning_rate': 3.680579615536961e-06, 'epoch': 2.26}\n",
            "{'loss': 1.5982, 'grad_norm': 0.7109375, 'learning_rate': 3.48627517277778e-06, 'epoch': 2.28}\n",
            "{'loss': 1.5615, 'grad_norm': 0.73828125, 'learning_rate': 3.296151560437214e-06, 'epoch': 2.3}\n",
            "{'loss': 1.5684, 'grad_norm': 0.703125, 'learning_rate': 3.110330809243134e-06, 'epoch': 2.33}\n",
            "{'loss': 1.5677, 'grad_norm': 0.70703125, 'learning_rate': 2.9289321881345257e-06, 'epoch': 2.35}\n",
            "{'loss': 1.5773, 'grad_norm': 0.71875, 'learning_rate': 2.7520721277088023e-06, 'epoch': 2.37}\n",
            "{'loss': 1.5303, 'grad_norm': 0.74609375, 'learning_rate': 2.5798641454908945e-06, 'epoch': 2.39}\n",
            "{'loss': 1.568, 'grad_norm': 0.69140625, 'learning_rate': 2.4124187730720916e-06, 'epoch': 2.41}\n",
            "{'loss': 1.5919, 'grad_norm': 0.66796875, 'learning_rate': 2.2498434851654125e-06, 'epoch': 2.44}\n",
            "{'loss': 1.582, 'grad_norm': 0.6953125, 'learning_rate': 2.092242630623016e-06, 'epoch': 2.46}\n",
            "{'loss': 1.5504, 'grad_norm': 0.68359375, 'learning_rate': 1.939717365459952e-06, 'epoch': 2.48}\n",
            "{'loss': 1.6357, 'grad_norm': 0.7265625, 'learning_rate': 1.7923655879272395e-06, 'epoch': 2.5}\n",
            "{'loss': 1.6043, 'grad_norm': 0.6953125, 'learning_rate': 1.6502818756759275e-06, 'epoch': 2.52}\n",
            "{'loss': 1.5852, 'grad_norm': 0.6953125, 'learning_rate': 1.5135574250524898e-06, 'epoch': 2.54}\n",
            "{'loss': 1.4942, 'grad_norm': 0.72265625, 'learning_rate': 1.3822799925645036e-06, 'epoch': 2.57}\n",
            "{'loss': 1.5318, 'grad_norm': 0.6875, 'learning_rate': 1.2565338385541792e-06, 'epoch': 2.59}\n",
            "{'loss': 1.5348, 'grad_norm': 0.72265625, 'learning_rate': 1.1363996731159188e-06, 'epoch': 2.61}\n",
            "{'loss': 1.5637, 'grad_norm': 0.69140625, 'learning_rate': 1.0219546042925842e-06, 'epoch': 2.63}\n",
            "{'loss': 1.563, 'grad_norm': 0.703125, 'learning_rate': 9.132720885837509e-07, 'epoch': 2.65}\n",
            "{'loss': 1.6325, 'grad_norm': 0.74609375, 'learning_rate': 8.10421883797694e-07, 'epoch': 2.67}\n",
            "{'loss': 1.5447, 'grad_norm': 0.69921875, 'learning_rate': 7.13470004277379e-07, 'epoch': 2.7}\n",
            "{'loss': 1.6078, 'grad_norm': 0.73046875, 'learning_rate': 6.22478678529197e-07, 'epoch': 2.72}\n",
            "{'loss': 1.6189, 'grad_norm': 0.71875, 'learning_rate': 5.375063092816313e-07, 'epoch': 2.74}\n",
            "{'loss': 1.5905, 'grad_norm': 0.68359375, 'learning_rate': 4.5860743599951186e-07, 'epoch': 2.76}\n",
            "{'loss': 1.5195, 'grad_norm': 0.6953125, 'learning_rate': 3.8583269987787607e-07, 'epoch': 2.78}\n",
            "{'loss': 1.5692, 'grad_norm': 0.70703125, 'learning_rate': 3.1922881133795827e-07, 'epoch': 2.81}\n",
            "{'loss': 1.5509, 'grad_norm': 0.7265625, 'learning_rate': 2.588385200461307e-07, 'epoch': 2.83}\n",
            "{'loss': 1.5252, 'grad_norm': 0.69921875, 'learning_rate': 2.0470058747505516e-07, 'epoch': 2.85}\n",
            "{'loss': 1.5468, 'grad_norm': 0.71484375, 'learning_rate': 1.5684976202465786e-07, 'epoch': 2.87}\n",
            "{'loss': 1.5174, 'grad_norm': 0.70703125, 'learning_rate': 1.1531675671888621e-07, 'epoch': 2.89}\n",
            "{'loss': 1.5503, 'grad_norm': 0.703125, 'learning_rate': 8.012822949256981e-08, 'epoch': 2.91}\n",
            "{'loss': 1.5394, 'grad_norm': 0.7578125, 'learning_rate': 5.1306766081048456e-08, 'epoch': 2.94}\n",
            "{'loss': 1.5757, 'grad_norm': 0.67578125, 'learning_rate': 2.8870865523525916e-08, 'epoch': 2.96}\n",
            "{'loss': 1.5586, 'grad_norm': 0.66796875, 'learning_rate': 1.2834928289472415e-08, 'epoch': 2.98}\n",
            "{'loss': 1.6142, 'grad_norm': 0.7109375, 'learning_rate': 3.209247035694807e-09, 'epoch': 3.0}\n",
            "{'train_runtime': 3041.347, 'train_samples_per_second': 5.8, 'train_steps_per_second': 0.045, 'train_loss': 1.5843606375265813, 'epoch': 3.0}\n",
            "100% 138/138 [50:36<00:00, 22.00s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mHW2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_072516-39d60x8o/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment3/ && bash scripts/self_train_gsm8k.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JkLh9zWv9jLz",
      "metadata": {
        "id": "JkLh9zWv9jLz"
      },
      "source": [
        "### Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moQp_TDwcmSE",
      "metadata": {
        "id": "moQp_TDwcmSE"
      },
      "source": [
        "### Evaluate base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fx0k4X64cp1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx0k4X64cp1d",
        "outputId": "be05b2bc-f48b-4cde-acfa-857959f487b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-03 09:26:58.782060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 09:26:58.799149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762162018.820343    2775 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762162018.826755    2775 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762162018.843022    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843053    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843056    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162018.843059    2775 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 09:26:58.847862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-03 09:27:07 [__init__.py:216] Automatically detected platform cuda.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "WARNING:__main__:Failed to apply chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating. Using raw prompt.\n",
            "INFO 11-03 09:27:13 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../SmolLM2-135M'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-03 09:27:30 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-03 09:27:30 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-03 09:27:33 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-03 09:27:33 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-03 09:27:37.860910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762162057.881507    3034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762162057.887969    3034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762162057.903635    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903659    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903662    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762162057.903664    3034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-03 09:27:45 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:46 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:46 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../SmolLM2-135M', speculative_config=None, tokenizer='../SmolLM2-135M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../SmolLM2-135M, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1103 09:27:49.993825269 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m WARNING 11-03 09:27:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [gpu_model_runner.py:2338] Starting to load model ../SmolLM2-135M...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:49 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:07<00:00,  7.05s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:56 [default_loader.py:268] Loading weights took 7.06 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:27:57 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 7.396194 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:28:05 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/34b05a8e0e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:28:06 [backends.py:550] Dynamo bytecode transform time: 8.03 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:34:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 409.423 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:08 [monitor.py:34] torch.compile takes 8.03 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:09 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:10 [kv_cache_utils.py:864] GPU KV cache size: 688,672 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:10 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 26.93it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15240215347` to fit into requested memory, or `--kv-cache-memory=22148614144` to fully utilize gpu memory. Current kv cache memory in use is 15867263795 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=3034)\u001b[0;0m INFO 11-03 09:35:13 [core.py:218] init engine (profile, create kv cache, warmup model) took 436.17 seconds\n",
            "INFO 11-03 09:35:14 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-03 09:35:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 2084.41it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.99it/s, est. speed input: 404.58 toks/s, output: 9173.99 toks/s]\n",
            "[rank0]:[W1103 09:35:26.203615151 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../SmolLM2-135M ./results/SmolLM2-135M"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arvWLhUL9tDy",
      "metadata": {
        "id": "arvWLhUL9tDy"
      },
      "source": [
        "#### My Hyperparameter Tuning 1 in Q6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BzGaO2Y66fkJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzGaO2Y66fkJ",
        "outputId": "9cdf6bfe-9143-4391-f666-be510b1379f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-03 13:40:42.336827: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 13:40:42.353764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762177242.375098    2706 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762177242.381551    2706 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762177242.397832    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397864    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397867    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177242.397869    2706 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 13:40:42.402750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-03 13:40:50 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-03 13:41:00 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../ckpt/HW2'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-03 13:41:18 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-03 13:41:18 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-03 13:41:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-03 13:41:21 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-03 13:41:25.824521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762177285.845840    2988 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762177285.852368    2988 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762177285.868907    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868935    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868938    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762177285.868940    2988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-03 13:41:33 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:34 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:34 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../ckpt/HW2', speculative_config=None, tokenizer='../ckpt/HW2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../ckpt/HW2, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1103 13:41:37.506504543 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m WARNING 11-03 13:41:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [gpu_model_runner.py:2338] Starting to load model ../ckpt/HW2...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:37 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:08<00:00,  8.37s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:46 [default_loader.py:268] Loading weights took 8.37 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:47 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 8.739695 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:55 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/36f5027955/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:55 [backends.py:550] Dynamo bytecode transform time: 7.72 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m [rank0]:W1103 13:41:56.526000 2988 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:41:59 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:23 [backends.py:215] Compiling a graph for dynamic shape takes 27.44 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:25 [monitor.py:34] torch.compile takes 35.16 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:26 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:27 [kv_cache_utils.py:864] GPU KV cache size: 688,720 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:27 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 25.62it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15241394995` to fit into requested memory, or `--kv-cache-memory=22149793792` to fully utilize gpu memory. Current kv cache memory in use is 15868443443 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2988)\u001b[0;0m INFO 11-03 13:42:30 [core.py:218] init engine (profile, create kv cache, warmup model) took 43.57 seconds\n",
            "INFO 11-03 13:42:31 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-03 13:42:31 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 1833.30it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.82it/s, est. speed input: 485.08 toks/s, output: 8946.33 toks/s]\n",
            "[rank0]:[W1103 13:42:43.363768373 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "ERROR:absl:Unable to detect language for text ## 1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1. due to No features in text.\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/FT1 ./results/SmolLM2-135M-SFT-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hu4Vf1Df6a61",
      "metadata": {
        "id": "hu4Vf1Df6a61"
      },
      "source": [
        "#### My Hyperparameter Tuning 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GwocvS_6dBr1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwocvS_6dBr1",
        "outputId": "9a65c6fd-4a53-4a55-8a15-7e3291994ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 128 gradient accumulation steps\n",
            "2025-11-03 14:04:18.452124: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 14:04:18.469796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762178658.491773    9823 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762178658.498452    9823 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762178658.514921    9823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762178658.514955    9823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762178658.514958    9823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762178658.514960    9823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 14:04:18.520021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run 4yflaqhr (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 4yflaqhr (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251103_140436-4yflaqhr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFT2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/4yflaqhr\u001b[0m\n",
            "{'loss': 1.7031, 'grad_norm': 1.328125, 'learning_rate': 0.0, 'epoch': 0.02}\n",
            "{'loss': 1.5794, 'grad_norm': 1.2734375, 'learning_rate': 3.5714285714285716e-07, 'epoch': 0.04}\n",
            "{'loss': 1.6428, 'grad_norm': 1.3671875, 'learning_rate': 7.142857142857143e-07, 'epoch': 0.07}\n",
            "{'loss': 1.6226, 'grad_norm': 1.265625, 'learning_rate': 1.0714285714285714e-06, 'epoch': 0.09}\n",
            "{'loss': 1.6571, 'grad_norm': 1.25, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.11}\n",
            "{'loss': 1.6053, 'grad_norm': 1.3125, 'learning_rate': 1.7857142857142859e-06, 'epoch': 0.13}\n",
            "{'loss': 1.6021, 'grad_norm': 1.25, 'learning_rate': 2.1428571428571427e-06, 'epoch': 0.15}\n",
            "{'loss': 1.6072, 'grad_norm': 1.4140625, 'learning_rate': 2.5e-06, 'epoch': 0.17}\n",
            "{'loss': 1.6885, 'grad_norm': 1.390625, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.2}\n",
            "{'loss': 1.5617, 'grad_norm': 1.21875, 'learning_rate': 3.2142857142857147e-06, 'epoch': 0.22}\n",
            "{'loss': 1.6412, 'grad_norm': 1.21875, 'learning_rate': 3.5714285714285718e-06, 'epoch': 0.24}\n",
            "{'loss': 1.6263, 'grad_norm': 1.3359375, 'learning_rate': 3.928571428571429e-06, 'epoch': 0.26}\n",
            "{'loss': 1.6739, 'grad_norm': 1.3203125, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.28}\n",
            "{'loss': 1.6033, 'grad_norm': 1.3359375, 'learning_rate': 4.642857142857144e-06, 'epoch': 0.3}\n",
            "{'loss': 1.6684, 'grad_norm': 1.328125, 'learning_rate': 5e-06, 'epoch': 0.33}\n",
            "{'loss': 1.6648, 'grad_norm': 1.2890625, 'learning_rate': 4.999197688241076e-06, 'epoch': 0.35}\n",
            "{'loss': 1.6608, 'grad_norm': 1.265625, 'learning_rate': 4.996791267927632e-06, 'epoch': 0.37}\n",
            "{'loss': 1.6218, 'grad_norm': 1.3125, 'learning_rate': 4.9927822836191185e-06, 'epoch': 0.39}\n",
            "{'loss': 1.6019, 'grad_norm': 1.3359375, 'learning_rate': 4.987173308479738e-06, 'epoch': 0.41}\n",
            "{'loss': 1.6299, 'grad_norm': 1.3203125, 'learning_rate': 4.9799679426268575e-06, 'epoch': 0.44}\n",
            "{'loss': 1.5985, 'grad_norm': 1.28125, 'learning_rate': 4.971170810820279e-06, 'epoch': 0.46}\n",
            "{'loss': 1.6052, 'grad_norm': 1.2578125, 'learning_rate': 4.960787559493836e-06, 'epoch': 0.48}\n",
            "{'loss': 1.5954, 'grad_norm': 1.3046875, 'learning_rate': 4.948824853131237e-06, 'epoch': 0.5}\n",
            "{'loss': 1.5845, 'grad_norm': 1.2578125, 'learning_rate': 4.935290369988468e-06, 'epoch': 0.52}\n",
            "{'loss': 1.6186, 'grad_norm': 1.25, 'learning_rate': 4.920192797165511e-06, 'epoch': 0.54}\n",
            "{'loss': 1.6277, 'grad_norm': 1.3125, 'learning_rate': 4.9035418250305314e-06, 'epoch': 0.57}\n",
            "{'loss': 1.5776, 'grad_norm': 1.3046875, 'learning_rate': 4.8853481410001225e-06, 'epoch': 0.59}\n",
            "{'loss': 1.6507, 'grad_norm': 1.2890625, 'learning_rate': 4.865623422679593e-06, 'epoch': 0.61}\n",
            "{'loss': 1.6104, 'grad_norm': 1.2578125, 'learning_rate': 4.844380330367701e-06, 'epoch': 0.63}\n",
            "{'loss': 1.6111, 'grad_norm': 1.3125, 'learning_rate': 4.821632498930656e-06, 'epoch': 0.65}\n",
            "{'loss': 1.6643, 'grad_norm': 1.296875, 'learning_rate': 4.797394529050577e-06, 'epoch': 0.67}\n",
            "{'loss': 1.6674, 'grad_norm': 1.265625, 'learning_rate': 4.771681977854062e-06, 'epoch': 0.7}\n",
            "{'loss': 1.6419, 'grad_norm': 1.3984375, 'learning_rate': 4.744511348926855e-06, 'epoch': 0.72}\n",
            "{'loss': 1.6095, 'grad_norm': 1.234375, 'learning_rate': 4.715900081721021e-06, 'epoch': 0.74}\n",
            "{'loss': 1.6193, 'grad_norm': 1.2734375, 'learning_rate': 4.685866540361456e-06, 'epoch': 0.76}\n",
            "{'loss': 1.6318, 'grad_norm': 1.2421875, 'learning_rate': 4.654430001858874e-06, 'epoch': 0.78}\n",
            "{'loss': 1.7677, 'grad_norm': 1.3203125, 'learning_rate': 4.621610643736878e-06, 'epoch': 0.81}\n",
            "{'loss': 1.6144, 'grad_norm': 1.3359375, 'learning_rate': 4.587429531081019e-06, 'epoch': 0.83}\n",
            "{'loss': 1.6423, 'grad_norm': 1.2578125, 'learning_rate': 4.551908603018191e-06, 'epoch': 0.85}\n",
            "{'loss': 1.6668, 'grad_norm': 1.296875, 'learning_rate': 4.515070658635013e-06, 'epoch': 0.87}\n",
            "{'loss': 1.6626, 'grad_norm': 1.3046875, 'learning_rate': 4.476939342344246e-06, 'epoch': 0.89}\n",
            "{'loss': 1.6353, 'grad_norm': 1.3359375, 'learning_rate': 4.437539128708647e-06, 'epoch': 0.91}\n",
            "{'loss': 1.681, 'grad_norm': 1.2734375, 'learning_rate': 4.396895306731978e-06, 'epoch': 0.94}\n",
            "{'loss': 1.6038, 'grad_norm': 1.296875, 'learning_rate': 4.355033963627277e-06, 'epoch': 0.96}\n",
            "{'loss': 1.6948, 'grad_norm': 1.2578125, 'learning_rate': 4.3119819680728e-06, 'epoch': 0.98}\n",
            "{'loss': 1.6203, 'grad_norm': 1.2890625, 'learning_rate': 4.267766952966369e-06, 'epoch': 1.0}\n",
            "{'loss': 1.6582, 'grad_norm': 1.265625, 'learning_rate': 4.222417297689217e-06, 'epoch': 1.02}\n",
            "{'loss': 1.6521, 'grad_norm': 1.3046875, 'learning_rate': 4.175962109890697e-06, 'epoch': 1.04}\n",
            "{'loss': 1.6325, 'grad_norm': 1.28125, 'learning_rate': 4.128431206805556e-06, 'epoch': 1.07}\n",
            "{'loss': 1.7224, 'grad_norm': 1.28125, 'learning_rate': 4.07985509611576e-06, 'epoch': 1.09}\n",
            "{'loss': 1.6459, 'grad_norm': 1.34375, 'learning_rate': 4.030264956369158e-06, 'epoch': 1.11}\n",
            "{'loss': 1.6319, 'grad_norm': 1.234375, 'learning_rate': 3.979692616967543e-06, 'epoch': 1.13}\n",
            "{'loss': 1.6369, 'grad_norm': 1.265625, 'learning_rate': 3.9281705377369814e-06, 'epoch': 1.15}\n",
            "{'loss': 1.5717, 'grad_norm': 1.2734375, 'learning_rate': 3.875731788093478e-06, 'epoch': 1.17}\n",
            "{'loss': 1.6497, 'grad_norm': 1.234375, 'learning_rate': 3.8224100258174066e-06, 'epoch': 1.2}\n",
            "{'loss': 1.7446, 'grad_norm': 1.203125, 'learning_rate': 3.7682394754502687e-06, 'epoch': 1.22}\n",
            "{'loss': 1.617, 'grad_norm': 1.25, 'learning_rate': 3.7132549063277033e-06, 'epoch': 1.24}\n",
            "{'loss': 1.6027, 'grad_norm': 1.2890625, 'learning_rate': 3.657491610262802e-06, 'epoch': 1.26}\n",
            "{'loss': 1.5786, 'grad_norm': 1.2421875, 'learning_rate': 3.600985378894086e-06, 'epoch': 1.28}\n",
            "{'loss': 1.6245, 'grad_norm': 1.2109375, 'learning_rate': 3.5437724807126583e-06, 'epoch': 1.3}\n",
            "{'loss': 1.6083, 'grad_norm': 1.21875, 'learning_rate': 3.4858896377832966e-06, 'epoch': 1.33}\n",
            "{'loss': 1.6028, 'grad_norm': 1.3125, 'learning_rate': 3.42737400217442e-06, 'epoch': 1.35}\n",
            "{'loss': 1.5915, 'grad_norm': 1.2109375, 'learning_rate': 3.3682631321120507e-06, 'epoch': 1.37}\n",
            "{'loss': 1.6398, 'grad_norm': 1.28125, 'learning_rate': 3.3085949678730953e-06, 'epoch': 1.39}\n",
            "{'loss': 1.6183, 'grad_norm': 1.2890625, 'learning_rate': 3.248407807433396e-06, 'epoch': 1.41}\n",
            "{'loss': 1.5836, 'grad_norm': 1.171875, 'learning_rate': 3.1877402818861954e-06, 'epoch': 1.44}\n",
            "{'loss': 1.5797, 'grad_norm': 1.1484375, 'learning_rate': 3.1266313306468018e-06, 'epoch': 1.46}\n",
            "{'loss': 1.6917, 'grad_norm': 1.3515625, 'learning_rate': 3.0651201764593375e-06, 'epoch': 1.48}\n",
            "{'loss': 1.5915, 'grad_norm': 1.2109375, 'learning_rate': 3.0032463002216504e-06, 'epoch': 1.5}\n",
            "{'loss': 1.6028, 'grad_norm': 1.15625, 'learning_rate': 2.941049415644522e-06, 'epoch': 1.52}\n",
            "{'loss': 1.5971, 'grad_norm': 1.2265625, 'learning_rate': 2.878569443761442e-06, 'epoch': 1.54}\n",
            "{'loss': 1.7493, 'grad_norm': 1.3515625, 'learning_rate': 2.8158464873053236e-06, 'epoch': 1.57}\n",
            "{'loss': 1.6384, 'grad_norm': 1.21875, 'learning_rate': 2.752920804968581e-06, 'epoch': 1.59}\n",
            "{'loss': 1.646, 'grad_norm': 1.25, 'learning_rate': 2.689832785563116e-06, 'epoch': 1.61}\n",
            "{'loss': 1.6407, 'grad_norm': 1.2734375, 'learning_rate': 2.626622922096782e-06, 'epoch': 1.63}\n",
            "{'loss': 1.6332, 'grad_norm': 1.265625, 'learning_rate': 2.56333178578297e-06, 'epoch': 1.65}\n",
            "{'loss': 1.6295, 'grad_norm': 1.265625, 'learning_rate': 2.5e-06, 'epoch': 1.67}\n",
            "{'loss': 1.6123, 'grad_norm': 1.15625, 'learning_rate': 2.436668214217031e-06, 'epoch': 1.7}\n",
            "{'loss': 1.5885, 'grad_norm': 1.2109375, 'learning_rate': 2.3733770779032185e-06, 'epoch': 1.72}\n",
            "{'loss': 1.6533, 'grad_norm': 1.1875, 'learning_rate': 2.310167214436885e-06, 'epoch': 1.74}\n",
            "{'loss': 1.6082, 'grad_norm': 1.1953125, 'learning_rate': 2.24707919503142e-06, 'epoch': 1.76}\n",
            "{'loss': 1.6475, 'grad_norm': 1.171875, 'learning_rate': 2.1841535126946777e-06, 'epoch': 1.78}\n",
            "{'loss': 1.6237, 'grad_norm': 1.21875, 'learning_rate': 2.1214305562385592e-06, 'epoch': 1.81}\n",
            "{'loss': 1.6599, 'grad_norm': 1.2890625, 'learning_rate': 2.05895058435548e-06, 'epoch': 1.83}\n",
            "{'loss': 1.6539, 'grad_norm': 1.25, 'learning_rate': 1.9967536997783495e-06, 'epoch': 1.85}\n",
            "{'loss': 1.6047, 'grad_norm': 1.109375, 'learning_rate': 1.934879823540663e-06, 'epoch': 1.87}\n",
            "{'loss': 1.5949, 'grad_norm': 1.234375, 'learning_rate': 1.8733686693531986e-06, 'epoch': 1.89}\n",
            "{'loss': 1.5831, 'grad_norm': 1.1484375, 'learning_rate': 1.812259718113805e-06, 'epoch': 1.91}\n",
            "{'loss': 1.5859, 'grad_norm': 1.2578125, 'learning_rate': 1.7515921925666053e-06, 'epoch': 1.94}\n",
            "{'loss': 1.6454, 'grad_norm': 1.234375, 'learning_rate': 1.6914050321269049e-06, 'epoch': 1.96}\n",
            "{'loss': 1.6753, 'grad_norm': 1.171875, 'learning_rate': 1.6317368678879497e-06, 'epoch': 1.98}\n",
            "{'loss': 1.6126, 'grad_norm': 1.1796875, 'learning_rate': 1.572625997825581e-06, 'epoch': 2.0}\n",
            "{'loss': 1.6824, 'grad_norm': 1.1953125, 'learning_rate': 1.5141103622167042e-06, 'epoch': 2.02}\n",
            "{'loss': 1.5734, 'grad_norm': 1.1484375, 'learning_rate': 1.456227519287343e-06, 'epoch': 2.04}\n",
            "{'loss': 1.6115, 'grad_norm': 1.1953125, 'learning_rate': 1.3990146211059141e-06, 'epoch': 2.07}\n",
            "{'loss': 1.5795, 'grad_norm': 1.1953125, 'learning_rate': 1.3425083897371983e-06, 'epoch': 2.09}\n",
            "{'loss': 1.6247, 'grad_norm': 1.2734375, 'learning_rate': 1.286745093672298e-06, 'epoch': 2.11}\n",
            "{'loss': 1.6764, 'grad_norm': 1.3359375, 'learning_rate': 1.2317605245497324e-06, 'epoch': 2.13}\n",
            "{'loss': 1.6095, 'grad_norm': 1.2265625, 'learning_rate': 1.1775899741825947e-06, 'epoch': 2.15}\n",
            "{'loss': 1.6322, 'grad_norm': 1.2890625, 'learning_rate': 1.1242682119065217e-06, 'epoch': 2.17}\n",
            "{'loss': 1.5739, 'grad_norm': 1.171875, 'learning_rate': 1.0718294622630188e-06, 'epoch': 2.2}\n",
            "{'loss': 1.647, 'grad_norm': 1.2265625, 'learning_rate': 1.0203073830324566e-06, 'epoch': 2.22}\n",
            "{'loss': 1.6493, 'grad_norm': 1.2109375, 'learning_rate': 9.697350436308428e-07, 'epoch': 2.24}\n",
            "{'loss': 1.6691, 'grad_norm': 1.21875, 'learning_rate': 9.201449038842403e-07, 'epoch': 2.26}\n",
            "{'loss': 1.6616, 'grad_norm': 1.21875, 'learning_rate': 8.71568793194445e-07, 'epoch': 2.28}\n",
            "{'loss': 1.6281, 'grad_norm': 1.3046875, 'learning_rate': 8.240378901093035e-07, 'epoch': 2.3}\n",
            "{'loss': 1.6305, 'grad_norm': 1.234375, 'learning_rate': 7.775827023107835e-07, 'epoch': 2.33}\n",
            "{'loss': 1.631, 'grad_norm': 1.234375, 'learning_rate': 7.322330470336314e-07, 'epoch': 2.35}\n",
            "{'loss': 1.6426, 'grad_norm': 1.328125, 'learning_rate': 6.880180319272006e-07, 'epoch': 2.37}\n",
            "{'loss': 1.5953, 'grad_norm': 1.2109375, 'learning_rate': 6.449660363727236e-07, 'epoch': 2.39}\n",
            "{'loss': 1.6303, 'grad_norm': 1.25, 'learning_rate': 6.031046932680229e-07, 'epoch': 2.41}\n",
            "{'loss': 1.6529, 'grad_norm': 1.234375, 'learning_rate': 5.624608712913531e-07, 'epoch': 2.44}\n",
            "{'loss': 1.6448, 'grad_norm': 1.1796875, 'learning_rate': 5.23060657655754e-07, 'epoch': 2.46}\n",
            "{'loss': 1.6102, 'grad_norm': 1.1953125, 'learning_rate': 4.84929341364988e-07, 'epoch': 2.48}\n",
            "{'loss': 1.7016, 'grad_norm': 1.28125, 'learning_rate': 4.480913969818099e-07, 'epoch': 2.5}\n",
            "{'loss': 1.6663, 'grad_norm': 1.1875, 'learning_rate': 4.125704689189819e-07, 'epoch': 2.52}\n",
            "{'loss': 1.6462, 'grad_norm': 1.25, 'learning_rate': 3.7838935626312246e-07, 'epoch': 2.54}\n",
            "{'loss': 1.5565, 'grad_norm': 1.1796875, 'learning_rate': 3.455699981411259e-07, 'epoch': 2.57}\n",
            "{'loss': 1.5908, 'grad_norm': 1.1640625, 'learning_rate': 3.141334596385448e-07, 'epoch': 2.59}\n",
            "{'loss': 1.6002, 'grad_norm': 1.296875, 'learning_rate': 2.840999182789797e-07, 'epoch': 2.61}\n",
            "{'loss': 1.6235, 'grad_norm': 1.2109375, 'learning_rate': 2.5548865107314606e-07, 'epoch': 2.63}\n",
            "{'loss': 1.6266, 'grad_norm': 1.28125, 'learning_rate': 2.2831802214593774e-07, 'epoch': 2.65}\n",
            "{'loss': 1.6952, 'grad_norm': 1.2734375, 'learning_rate': 2.026054709494235e-07, 'epoch': 2.67}\n",
            "{'loss': 1.6072, 'grad_norm': 1.1875, 'learning_rate': 1.7836750106934475e-07, 'epoch': 2.7}\n",
            "{'loss': 1.67, 'grad_norm': 1.1953125, 'learning_rate': 1.5561966963229925e-07, 'epoch': 2.72}\n",
            "{'loss': 1.6838, 'grad_norm': 1.328125, 'learning_rate': 1.3437657732040783e-07, 'epoch': 2.74}\n",
            "{'loss': 1.6532, 'grad_norm': 1.3203125, 'learning_rate': 1.1465185899987797e-07, 'epoch': 2.76}\n",
            "{'loss': 1.5824, 'grad_norm': 1.2421875, 'learning_rate': 9.645817496946902e-08, 'epoch': 2.78}\n",
            "{'loss': 1.633, 'grad_norm': 1.1953125, 'learning_rate': 7.980720283448957e-08, 'epoch': 2.81}\n",
            "{'loss': 1.6153, 'grad_norm': 1.1484375, 'learning_rate': 6.470963001153268e-08, 'epoch': 2.83}\n",
            "{'loss': 1.5888, 'grad_norm': 1.2421875, 'learning_rate': 5.117514686876379e-08, 'epoch': 2.85}\n",
            "{'loss': 1.6093, 'grad_norm': 1.1328125, 'learning_rate': 3.9212440506164465e-08, 'epoch': 2.87}\n",
            "{'loss': 1.5793, 'grad_norm': 1.203125, 'learning_rate': 2.8829189179721552e-08, 'epoch': 2.89}\n",
            "{'loss': 1.6127, 'grad_norm': 1.2109375, 'learning_rate': 2.0032057373142453e-08, 'epoch': 2.91}\n",
            "{'loss': 1.6036, 'grad_norm': 1.265625, 'learning_rate': 1.2826691520262114e-08, 'epoch': 2.94}\n",
            "{'loss': 1.6349, 'grad_norm': 1.140625, 'learning_rate': 7.217716380881479e-09, 'epoch': 2.96}\n",
            "{'loss': 1.6191, 'grad_norm': 1.1953125, 'learning_rate': 3.208732072368104e-09, 'epoch': 2.98}\n",
            "{'loss': 1.6774, 'grad_norm': 1.2421875, 'learning_rate': 8.023117589237017e-10, 'epoch': 3.0}\n",
            "{'train_runtime': 3059.1951, 'train_samples_per_second': 5.766, 'train_steps_per_second': 0.045, 'train_loss': 1.6308705564858257, 'epoch': 3.0}\n",
            "100% 138/138 [50:57<00:00, 22.15s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mFT2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_140436-4yflaqhr/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EtfCHfr6c1IC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtfCHfr6c1IC",
        "outputId": "81fc9707-e0ce-4403-e184-514bf581ebe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-03 15:05:32.151677: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-03 15:05:32.169608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762182332.191577   25411 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762182332.198250   25411 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762182332.215542   25411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182332.215576   25411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182332.215579   25411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182332.215581   25411 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-03 15:05:32.220621: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-03 15:05:39 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-03 15:05:41 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../ckpt/FT2'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-03 15:05:57 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-03 15:05:57 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-03 15:05:59 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-03 15:05:59 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-03 15:06:04.056083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762182364.077421   25627 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762182364.083880   25627 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762182364.100137   25627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182364.100170   25627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182364.100173   25627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762182364.100176   25627 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-03 15:06:11 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:13 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:13 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../ckpt/FT2', speculative_config=None, tokenizer='../ckpt/FT2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../ckpt/FT2, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1103 15:06:15.004976838 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:15 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m WARNING 11-03 15:06:15 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:15 [gpu_model_runner.py:2338] Starting to load model ../ckpt/FT2...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:16 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:16 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  4.27it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:16 [default_loader.py:268] Loading weights took 0.24 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:17 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 0.443895 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:24 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/a3abe33083/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:24 [backends.py:550] Dynamo bytecode transform time: 6.83 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 17.449 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:43 [monitor.py:34] torch.compile takes 6.83 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:44 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:45 [kv_cache_utils.py:864] GPU KV cache size: 688,768 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:45 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.08x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 24.73it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:48 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:48 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15242312499` to fit into requested memory, or `--kv-cache-memory=22150711296` to fully utilize gpu memory. Current kv cache memory in use is 15869360947 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=25627)\u001b[0;0m INFO 11-03 15:06:48 [core.py:218] init engine (profile, create kv cache, warmup model) took 31.37 seconds\n",
            "INFO 11-03 15:06:49 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-03 15:06:49 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 2134.54it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.99it/s, est. speed input: 494.29 toks/s, output: 8588.70 toks/s]\n",
            "[rank0]:[W1103 15:07:00.182278917 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/FT2 ./results/SmolLM2-135M-SFT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EzTRFUaVQwqE",
      "metadata": {
        "id": "EzTRFUaVQwqE"
      },
      "source": [
        "#### My Hyperparameter Tuning 3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ciKG6EzOQ1gf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciKG6EzOQ1gf",
        "outputId": "0f71fce3-6818-48ac-f6f0-00b829633fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 248 gradient accumulation steps\n",
            "2025-11-04 05:10:36.841936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-04 05:10:36.858984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762233036.880364    2137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762233036.886896    2137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762233036.903387    2137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762233036.903417    2137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762233036.903420    2137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762233036.903422    2137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-04 05:10:36.908408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run s85omany (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251104_051121-s85omany\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFT3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/s85omany\u001b[0m\n",
            "{'loss': 1.6438, 'grad_norm': 1.2890625, 'learning_rate': 0.0, 'epoch': 0.04}\n",
            "{'loss': 1.6355, 'grad_norm': 1.296875, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.08}\n",
            "{'loss': 1.6344, 'grad_norm': 1.265625, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.13}\n",
            "{'loss': 1.6031, 'grad_norm': 1.3125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.17}\n",
            "{'loss': 1.623, 'grad_norm': 1.328125, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.21}\n",
            "{'loss': 1.6301, 'grad_norm': 1.234375, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.25}\n",
            "{'loss': 1.6492, 'grad_norm': 1.3359375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.3}\n",
            "{'loss': 1.645, 'grad_norm': 1.2890625, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.34}\n",
            "{'loss': 1.6732, 'grad_norm': 1.265625, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.38}\n",
            "{'loss': 1.5834, 'grad_norm': 1.3046875, 'learning_rate': 3e-06, 'epoch': 0.42}\n",
            "{'loss': 1.6036, 'grad_norm': 1.2890625, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.46}\n",
            "{'loss': 1.6115, 'grad_norm': 1.3203125, 'learning_rate': 3.6666666666666666e-06, 'epoch': 0.51}\n",
            "{'loss': 1.6077, 'grad_norm': 1.234375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.55}\n",
            "{'loss': 1.5995, 'grad_norm': 1.3203125, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.59}\n",
            "{'loss': 1.6508, 'grad_norm': 1.3046875, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.63}\n",
            "{'loss': 1.6339, 'grad_norm': 1.328125, 'learning_rate': 5e-06, 'epoch': 0.67}\n",
            "{'loss': 1.6549, 'grad_norm': 1.3515625, 'learning_rate': 4.998119881260576e-06, 'epoch': 0.72}\n",
            "{'loss': 1.6109, 'grad_norm': 1.265625, 'learning_rate': 4.99248235291948e-06, 'epoch': 0.76}\n",
            "{'loss': 1.6899, 'grad_norm': 1.328125, 'learning_rate': 4.983095894354858e-06, 'epoch': 0.8}\n",
            "{'loss': 1.6415, 'grad_norm': 1.3203125, 'learning_rate': 4.969974623692023e-06, 'epoch': 0.84}\n",
            "{'loss': 1.6862, 'grad_norm': 1.3203125, 'learning_rate': 4.953138276568462e-06, 'epoch': 0.89}\n",
            "{'loss': 1.6593, 'grad_norm': 1.359375, 'learning_rate': 4.93261217644956e-06, 'epoch': 0.93}\n",
            "{'loss': 1.6153, 'grad_norm': 1.2265625, 'learning_rate': 4.908427196539701e-06, 'epoch': 0.97}\n",
            "{'loss': 1.673, 'grad_norm': 1.375, 'learning_rate': 4.8806197133460385e-06, 'epoch': 1.0}\n",
            "{'loss': 1.6622, 'grad_norm': 1.3125, 'learning_rate': 4.849231551964771e-06, 'epoch': 1.04}\n",
            "{'loss': 1.6675, 'grad_norm': 1.296875, 'learning_rate': 4.814309923172227e-06, 'epoch': 1.08}\n",
            "{'loss': 1.6487, 'grad_norm': 1.3125, 'learning_rate': 4.775907352415367e-06, 'epoch': 1.13}\n",
            "{'loss': 1.6151, 'grad_norm': 1.3125, 'learning_rate': 4.734081600808531e-06, 'epoch': 1.17}\n",
            "{'loss': 1.6717, 'grad_norm': 1.2265625, 'learning_rate': 4.688895578255228e-06, 'epoch': 1.21}\n",
            "{'loss': 1.6492, 'grad_norm': 1.2734375, 'learning_rate': 4.640417248825667e-06, 'epoch': 1.25}\n",
            "{'loss': 1.5846, 'grad_norm': 1.265625, 'learning_rate': 4.588719528532342e-06, 'epoch': 1.3}\n",
            "{'loss': 1.6362, 'grad_norm': 1.28125, 'learning_rate': 4.533880175657419e-06, 'epoch': 1.34}\n",
            "{'loss': 1.6077, 'grad_norm': 1.2890625, 'learning_rate': 4.475981673796899e-06, 'epoch': 1.38}\n",
            "{'loss': 1.5941, 'grad_norm': 1.25, 'learning_rate': 4.415111107797445e-06, 'epoch': 1.42}\n",
            "{'loss': 1.612, 'grad_norm': 1.234375, 'learning_rate': 4.351360032772512e-06, 'epoch': 1.46}\n",
            "{'loss': 1.6462, 'grad_norm': 1.28125, 'learning_rate': 4.284824336394748e-06, 'epoch': 1.51}\n",
            "{'loss': 1.6154, 'grad_norm': 1.2109375, 'learning_rate': 4.215604094671835e-06, 'epoch': 1.55}\n",
            "{'loss': 1.6683, 'grad_norm': 1.296875, 'learning_rate': 4.14380342142266e-06, 'epoch': 1.59}\n",
            "{'loss': 1.6576, 'grad_norm': 1.3046875, 'learning_rate': 4.069530311680247e-06, 'epoch': 1.63}\n",
            "{'loss': 1.6266, 'grad_norm': 1.28125, 'learning_rate': 3.992896479256966e-06, 'epoch': 1.67}\n",
            "{'loss': 1.6082, 'grad_norm': 1.21875, 'learning_rate': 3.914017188716347e-06, 'epoch': 1.72}\n",
            "{'loss': 1.6269, 'grad_norm': 1.203125, 'learning_rate': 3.833011082004229e-06, 'epoch': 1.76}\n",
            "{'loss': 1.6318, 'grad_norm': 1.203125, 'learning_rate': 3.7500000000000005e-06, 'epoch': 1.8}\n",
            "{'loss': 1.6749, 'grad_norm': 1.296875, 'learning_rate': 3.665108799256348e-06, 'epoch': 1.84}\n",
            "{'loss': 1.5929, 'grad_norm': 1.1875, 'learning_rate': 3.578465164203134e-06, 'epoch': 1.89}\n",
            "{'loss': 1.5988, 'grad_norm': 1.21875, 'learning_rate': 3.4901994150978926e-06, 'epoch': 1.93}\n",
            "{'loss': 1.6183, 'grad_norm': 1.2109375, 'learning_rate': 3.400444312011776e-06, 'epoch': 1.97}\n",
            "{'loss': 1.6649, 'grad_norm': 1.234375, 'learning_rate': 3.3093348551458033e-06, 'epoch': 2.0}\n",
            "{'loss': 1.6358, 'grad_norm': 1.1875, 'learning_rate': 3.217008081777726e-06, 'epoch': 2.04}\n",
            "{'loss': 1.5874, 'grad_norm': 1.1875, 'learning_rate': 3.1236028601449534e-06, 'epoch': 2.08}\n",
            "{'loss': 1.6364, 'grad_norm': 1.3203125, 'learning_rate': 3.0292596805735275e-06, 'epoch': 2.13}\n",
            "{'loss': 1.6508, 'grad_norm': 1.3046875, 'learning_rate': 2.9341204441673267e-06, 'epoch': 2.17}\n",
            "{'loss': 1.6083, 'grad_norm': 1.1796875, 'learning_rate': 2.8383282493753282e-06, 'epoch': 2.21}\n",
            "{'loss': 1.6555, 'grad_norm': 1.234375, 'learning_rate': 2.742027176757948e-06, 'epoch': 2.25}\n",
            "{'loss': 1.6564, 'grad_norm': 1.2734375, 'learning_rate': 2.6453620722761897e-06, 'epoch': 2.3}\n",
            "{'loss': 1.6277, 'grad_norm': 1.265625, 'learning_rate': 2.548478329429561e-06, 'epoch': 2.34}\n",
            "{'loss': 1.6348, 'grad_norm': 1.28125, 'learning_rate': 2.4515216705704396e-06, 'epoch': 2.38}\n",
            "{'loss': 1.6115, 'grad_norm': 1.2421875, 'learning_rate': 2.3546379277238107e-06, 'epoch': 2.42}\n",
            "{'loss': 1.6588, 'grad_norm': 1.203125, 'learning_rate': 2.2579728232420524e-06, 'epoch': 2.46}\n",
            "{'loss': 1.6542, 'grad_norm': 1.25, 'learning_rate': 2.161671750624673e-06, 'epoch': 2.51}\n",
            "{'loss': 1.6578, 'grad_norm': 1.2265625, 'learning_rate': 2.0658795558326745e-06, 'epoch': 2.55}\n",
            "{'loss': 1.5797, 'grad_norm': 1.1953125, 'learning_rate': 1.970740319426474e-06, 'epoch': 2.59}\n",
            "{'loss': 1.6113, 'grad_norm': 1.265625, 'learning_rate': 1.876397139855047e-06, 'epoch': 2.63}\n",
            "{'loss': 1.6636, 'grad_norm': 1.28125, 'learning_rate': 1.7829919182222752e-06, 'epoch': 2.67}\n",
            "{'loss': 1.6478, 'grad_norm': 1.1953125, 'learning_rate': 1.6906651448541977e-06, 'epoch': 2.72}\n",
            "{'loss': 1.6624, 'grad_norm': 1.3203125, 'learning_rate': 1.5995556879882246e-06, 'epoch': 2.76}\n",
            "{'loss': 1.6131, 'grad_norm': 1.2421875, 'learning_rate': 1.509800584902108e-06, 'epoch': 2.8}\n",
            "{'loss': 1.6204, 'grad_norm': 1.2109375, 'learning_rate': 1.421534835796867e-06, 'epoch': 2.84}\n",
            "{'loss': 1.5888, 'grad_norm': 1.15625, 'learning_rate': 1.3348912007436538e-06, 'epoch': 2.89}\n",
            "{'loss': 1.5905, 'grad_norm': 1.2265625, 'learning_rate': 1.2500000000000007e-06, 'epoch': 2.93}\n",
            "{'loss': 1.621, 'grad_norm': 1.1640625, 'learning_rate': 1.1669889179957725e-06, 'epoch': 2.97}\n",
            "{'loss': 1.6777, 'grad_norm': 1.2734375, 'learning_rate': 1.085982811283654e-06, 'epoch': 3.0}\n",
            "{'loss': 1.6506, 'grad_norm': 1.265625, 'learning_rate': 1.0071035207430352e-06, 'epoch': 3.04}\n",
            "{'loss': 1.6723, 'grad_norm': 1.2890625, 'learning_rate': 9.304696883197542e-07, 'epoch': 3.08}\n",
            "{'loss': 1.6333, 'grad_norm': 1.2734375, 'learning_rate': 8.561965785773413e-07, 'epoch': 3.13}\n",
            "{'loss': 1.6103, 'grad_norm': 1.1875, 'learning_rate': 7.843959053281663e-07, 'epoch': 3.17}\n",
            "{'loss': 1.5963, 'grad_norm': 1.2578125, 'learning_rate': 7.151756636052529e-07, 'epoch': 3.21}\n",
            "{'loss': 1.6042, 'grad_norm': 1.1796875, 'learning_rate': 6.48639967227489e-07, 'epoch': 3.25}\n",
            "{'loss': 1.6345, 'grad_norm': 1.2421875, 'learning_rate': 5.848888922025553e-07, 'epoch': 3.3}\n",
            "{'loss': 1.616, 'grad_norm': 1.21875, 'learning_rate': 5.240183262031021e-07, 'epoch': 3.34}\n",
            "{'loss': 1.6198, 'grad_norm': 1.25, 'learning_rate': 4.661198243425813e-07, 'epoch': 3.38}\n",
            "{'loss': 1.602, 'grad_norm': 1.203125, 'learning_rate': 4.1128047146765936e-07, 'epoch': 3.42}\n",
            "{'loss': 1.6327, 'grad_norm': 1.2421875, 'learning_rate': 3.595827511743341e-07, 'epoch': 3.46}\n",
            "{'loss': 1.6249, 'grad_norm': 1.1796875, 'learning_rate': 3.111044217447731e-07, 'epoch': 3.51}\n",
            "{'loss': 1.6347, 'grad_norm': 1.203125, 'learning_rate': 2.6591839919146963e-07, 'epoch': 3.55}\n",
            "{'loss': 1.6356, 'grad_norm': 1.328125, 'learning_rate': 2.240926475846336e-07, 'epoch': 3.59}\n",
            "{'loss': 1.6424, 'grad_norm': 1.2578125, 'learning_rate': 1.8569007682777417e-07, 'epoch': 3.63}\n",
            "{'loss': 1.6657, 'grad_norm': 1.2734375, 'learning_rate': 1.507684480352292e-07, 'epoch': 3.67}\n",
            "{'loss': 1.6388, 'grad_norm': 1.171875, 'learning_rate': 1.1938028665396172e-07, 'epoch': 3.72}\n",
            "{'loss': 1.6468, 'grad_norm': 1.2890625, 'learning_rate': 9.157280346029918e-08, 'epoch': 3.76}\n",
            "{'loss': 1.6182, 'grad_norm': 1.25, 'learning_rate': 6.738782355044048e-08, 'epoch': 3.8}\n",
            "{'loss': 1.6288, 'grad_norm': 1.1953125, 'learning_rate': 4.6861723431538273e-08, 'epoch': 3.84}\n",
            "{'loss': 1.6537, 'grad_norm': 1.3203125, 'learning_rate': 3.0025376307977474e-08, 'epoch': 3.89}\n",
            "{'loss': 1.6498, 'grad_norm': 1.203125, 'learning_rate': 1.6904105645142443e-08, 'epoch': 3.93}\n",
            "{'loss': 1.6354, 'grad_norm': 1.1328125, 'learning_rate': 7.517647080519941e-09, 'epoch': 3.97}\n",
            "{'loss': 1.5774, 'grad_norm': 1.2578125, 'learning_rate': 1.8801187394248966e-09, 'epoch': 4.0}\n",
            "{'train_runtime': 4011.6079, 'train_samples_per_second': 5.863, 'train_steps_per_second': 0.024, 'train_loss': 1.6324398120244343, 'epoch': 4.0}\n",
            "100% 96/96 [1:06:45<00:00, 41.73s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mFT3\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251104_051121-s85omany/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BtXfoEI4TzPI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtXfoEI4TzPI",
        "outputId": "814d5edc-9974-4528-ad91-06058a75a380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-04 06:20:53.764540: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-04 06:20:53.782766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762237253.804533   19707 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762237253.811249   19707 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762237253.828059   19707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237253.828093   19707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237253.828096   19707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237253.828098   19707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-04 06:20:53.833098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-04 06:21:01 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-04 06:21:05 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../ckpt/FT3'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-04 06:21:21 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-04 06:21:21 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-04 06:21:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-04 06:21:24 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-04 06:21:29.294463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762237289.315934   19954 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762237289.322477   19954 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762237289.338675   19954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237289.338709   19954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237289.338712   19954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762237289.338714   19954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-04 06:21:37 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:39 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:39 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../ckpt/FT3', speculative_config=None, tokenizer='../ckpt/FT3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../ckpt/FT3, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1104 06:21:41.586476004 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:41 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m WARNING 11-04 06:21:41 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:41 [gpu_model_runner.py:2338] Starting to load model ../ckpt/FT3...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:42 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:42 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  4.29it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:42 [default_loader.py:268] Loading weights took 0.24 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:43 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 0.540480 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:51 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/bd12251ae5/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:21:52 [backends.py:550] Dynamo bytecode transform time: 8.47 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:29:54 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 481.166 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:07 [monitor.py:34] torch.compile takes 8.47 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:08 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:08 [kv_cache_utils.py:864] GPU KV cache size: 688,672 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:08 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 25.27it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:12 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:12 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15240215347` to fit into requested memory, or `--kv-cache-memory=22148614144` to fully utilize gpu memory. Current kv cache memory in use is 15867263795 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=19954)\u001b[0;0m INFO 11-04 06:30:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 508.69 seconds\n",
            "INFO 11-04 06:30:13 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-04 06:30:13 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 1540.58it/s]\n",
            "Processed prompts: 100% 100/100 [00:11<00:00,  8.90it/s, est. speed input: 489.52 toks/s, output: 8508.86 toks/s]\n",
            "[rank0]:[W1104 06:30:24.486775497 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/FT3 ./results/SmolLM2-135M-SFT-3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BiR0SsXXmJab",
      "metadata": {
        "id": "BiR0SsXXmJab"
      },
      "source": [
        "#### My Hyperparameter Tuning 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sy6I2S6umL6Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy6I2S6umL6Q",
        "outputId": "f414dd55-fc5b-43cc-ee40-aa11d660d5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 256 gradient accumulation steps\n",
            "2025-11-04 06:45:32.973158: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-04 06:45:32.990991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762238733.012951   26107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762238733.019682   26107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762238733.036272   26107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762238733.036300   26107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762238733.036304   26107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762238733.036306   26107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-04 06:45:33.041166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251104_064550-rj654dsv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFT4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/rj654dsv\u001b[0m\n",
            "{'loss': 1.6377, 'grad_norm': 1.28125, 'learning_rate': 0.0, 'epoch': 0.04}\n",
            "{'loss': 1.6331, 'grad_norm': 1.296875, 'learning_rate': 1.7857142857142858e-07, 'epoch': 0.09}\n",
            "{'loss': 1.6322, 'grad_norm': 1.2734375, 'learning_rate': 3.5714285714285716e-07, 'epoch': 0.13}\n",
            "{'loss': 1.6045, 'grad_norm': 1.3203125, 'learning_rate': 5.357142857142857e-07, 'epoch': 0.17}\n",
            "{'loss': 1.6261, 'grad_norm': 1.296875, 'learning_rate': 7.142857142857143e-07, 'epoch': 0.22}\n",
            "{'loss': 1.6339, 'grad_norm': 1.265625, 'learning_rate': 8.928571428571429e-07, 'epoch': 0.26}\n",
            "{'loss': 1.6391, 'grad_norm': 1.3125, 'learning_rate': 1.0714285714285714e-06, 'epoch': 0.3}\n",
            "{'loss': 1.6673, 'grad_norm': 1.3046875, 'learning_rate': 1.25e-06, 'epoch': 0.35}\n",
            "{'loss': 1.6424, 'grad_norm': 1.28125, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.39}\n",
            "{'loss': 1.6173, 'grad_norm': 1.3359375, 'learning_rate': 1.6071428571428574e-06, 'epoch': 0.44}\n",
            "{'loss': 1.603, 'grad_norm': 1.265625, 'learning_rate': 1.7857142857142859e-06, 'epoch': 0.48}\n",
            "{'loss': 1.5922, 'grad_norm': 1.296875, 'learning_rate': 1.9642857142857144e-06, 'epoch': 0.52}\n",
            "{'loss': 1.6255, 'grad_norm': 1.296875, 'learning_rate': 2.1428571428571427e-06, 'epoch': 0.57}\n",
            "{'loss': 1.6156, 'grad_norm': 1.3046875, 'learning_rate': 2.321428571428572e-06, 'epoch': 0.61}\n",
            "{'loss': 1.6134, 'grad_norm': 1.3046875, 'learning_rate': 2.5e-06, 'epoch': 0.65}\n",
            "{'loss': 1.6689, 'grad_norm': 1.3046875, 'learning_rate': 2.498986247714462e-06, 'epoch': 0.7}\n",
            "{'loss': 1.6288, 'grad_norm': 1.34375, 'learning_rate': 2.495946635167763e-06, 'epoch': 0.74}\n",
            "{'loss': 1.6284, 'grad_norm': 1.296875, 'learning_rate': 2.4908860926225676e-06, 'epoch': 0.78}\n",
            "{'loss': 1.6956, 'grad_norm': 1.3671875, 'learning_rate': 2.483812828297391e-06, 'epoch': 0.83}\n",
            "{'loss': 1.6587, 'grad_norm': 1.3125, 'learning_rate': 2.474738315052835e-06, 'epoch': 0.87}\n",
            "{'loss': 1.653, 'grad_norm': 1.3671875, 'learning_rate': 2.4636772717825653e-06, 'epoch': 0.91}\n",
            "{'loss': 1.6423, 'grad_norm': 1.3125, 'learning_rate': 2.4506476395392157e-06, 'epoch': 0.96}\n",
            "{'loss': 1.6633, 'grad_norm': 1.3203125, 'learning_rate': 2.4356705524339317e-06, 'epoch': 1.0}\n",
            "{'loss': 1.6593, 'grad_norm': 1.3359375, 'learning_rate': 2.418770303356769e-06, 'epoch': 1.04}\n",
            "{'loss': 1.6808, 'grad_norm': 1.3203125, 'learning_rate': 2.3999743045735305e-06, 'epoch': 1.09}\n",
            "{'loss': 1.6445, 'grad_norm': 1.3359375, 'learning_rate': 2.379313043262978e-06, 'epoch': 1.13}\n",
            "{'loss': 1.6081, 'grad_norm': 1.328125, 'learning_rate': 2.3568200320665123e-06, 'epoch': 1.17}\n",
            "{'loss': 1.7043, 'grad_norm': 1.2734375, 'learning_rate': 2.332531754730549e-06, 'epoch': 1.22}\n",
            "{'loss': 1.615, 'grad_norm': 1.328125, 'learning_rate': 2.3064876069297436e-06, 'epoch': 1.26}\n",
            "{'loss': 1.6047, 'grad_norm': 1.2734375, 'learning_rate': 2.2787298323670707e-06, 'epoch': 1.3}\n",
            "{'loss': 1.611, 'grad_norm': 1.328125, 'learning_rate': 2.249303454254377e-06, 'epoch': 1.35}\n",
            "{'loss': 1.6199, 'grad_norm': 1.296875, 'learning_rate': 2.218256202284568e-06, 'epoch': 1.39}\n",
            "{'loss': 1.6058, 'grad_norm': 1.265625, 'learning_rate': 2.1856384352138767e-06, 'epoch': 1.44}\n",
            "{'loss': 1.644, 'grad_norm': 1.3046875, 'learning_rate': 2.151503059179768e-06, 'epoch': 1.48}\n",
            "{'loss': 1.6019, 'grad_norm': 1.2578125, 'learning_rate': 2.1159054418869994e-06, 'epoch': 1.52}\n",
            "{'loss': 1.6744, 'grad_norm': 1.3203125, 'learning_rate': 2.0789033228009943e-06, 'epoch': 1.57}\n",
            "{'loss': 1.6473, 'grad_norm': 1.3125, 'learning_rate': 2.0405567194942215e-06, 'epoch': 1.61}\n",
            "{'loss': 1.6423, 'grad_norm': 1.3515625, 'learning_rate': 2.000927830297474e-06, 'epoch': 1.65}\n",
            "{'loss': 1.6253, 'grad_norm': 1.2578125, 'learning_rate': 1.960080933413945e-06, 'epoch': 1.7}\n",
            "{'loss': 1.6256, 'grad_norm': 1.28125, 'learning_rate': 1.9180822826597516e-06, 'epoch': 1.74}\n",
            "{'loss': 1.6336, 'grad_norm': 1.234375, 'learning_rate': 1.8750000000000003e-06, 'epoch': 1.78}\n",
            "{'loss': 1.6463, 'grad_norm': 1.3203125, 'learning_rate': 1.830903965054711e-06, 'epoch': 1.83}\n",
            "{'loss': 1.6357, 'grad_norm': 1.234375, 'learning_rate': 1.7858657017538177e-06, 'epoch': 1.87}\n",
            "{'loss': 1.5952, 'grad_norm': 1.25, 'learning_rate': 1.739958262325094e-06, 'epoch': 1.91}\n",
            "{'loss': 1.6213, 'grad_norm': 1.34375, 'learning_rate': 1.6932561088031695e-06, 'epoch': 1.96}\n",
            "{'loss': 1.6507, 'grad_norm': 1.234375, 'learning_rate': 1.645834992251841e-06, 'epoch': 2.0}\n",
            "{'loss': 1.6318, 'grad_norm': 1.234375, 'learning_rate': 1.597771829895566e-06, 'epoch': 2.04}\n",
            "{'loss': 1.6013, 'grad_norm': 1.2734375, 'learning_rate': 1.5491445803594474e-06, 'epoch': 2.09}\n",
            "{'loss': 1.6558, 'grad_norm': 1.3828125, 'learning_rate': 1.5000321172200557e-06, 'epoch': 2.13}\n",
            "{'loss': 1.6275, 'grad_norm': 1.3046875, 'learning_rate': 1.4505141010722004e-06, 'epoch': 2.17}\n",
            "{'loss': 1.6161, 'grad_norm': 1.265625, 'learning_rate': 1.4006708503191539e-06, 'epoch': 2.22}\n",
            "{'loss': 1.6648, 'grad_norm': 1.28125, 'learning_rate': 1.3505832108959077e-06, 'epoch': 2.26}\n",
            "{'loss': 1.6509, 'grad_norm': 1.34375, 'learning_rate': 1.3003324251367692e-06, 'epoch': 2.3}\n",
            "{'loss': 1.6361, 'grad_norm': 1.3046875, 'learning_rate': 1.25e-06, 'epoch': 2.35}\n",
            "{'loss': 1.6242, 'grad_norm': 1.3359375, 'learning_rate': 1.1996675748632314e-06, 'epoch': 2.39}\n",
            "{'loss': 1.6472, 'grad_norm': 1.296875, 'learning_rate': 1.1494167891040927e-06, 'epoch': 2.44}\n",
            "{'loss': 1.6329, 'grad_norm': 1.2421875, 'learning_rate': 1.0993291496808463e-06, 'epoch': 2.48}\n",
            "{'loss': 1.6884, 'grad_norm': 1.2734375, 'learning_rate': 1.0494858989277996e-06, 'epoch': 2.52}\n",
            "{'loss': 1.6087, 'grad_norm': 1.28125, 'learning_rate': 9.999678827799447e-07, 'epoch': 2.57}\n",
            "{'loss': 1.6011, 'grad_norm': 1.2890625, 'learning_rate': 9.508554196405532e-07, 'epoch': 2.61}\n",
            "{'loss': 1.6312, 'grad_norm': 1.3203125, 'learning_rate': 9.022281701044343e-07, 'epoch': 2.65}\n",
            "{'loss': 1.6598, 'grad_norm': 1.2890625, 'learning_rate': 8.541650077481598e-07, 'epoch': 2.7}\n",
            "{'loss': 1.6825, 'grad_norm': 1.3203125, 'learning_rate': 8.067438911968305e-07, 'epoch': 2.74}\n",
            "{'loss': 1.6256, 'grad_norm': 1.3515625, 'learning_rate': 7.600417376749065e-07, 'epoch': 2.78}\n",
            "{'loss': 1.6297, 'grad_norm': 1.21875, 'learning_rate': 7.141342982461822e-07, 'epoch': 2.83}\n",
            "{'loss': 1.6042, 'grad_norm': 1.2421875, 'learning_rate': 6.690960349452894e-07, 'epoch': 2.87}\n",
            "{'loss': 1.6026, 'grad_norm': 1.265625, 'learning_rate': 6.250000000000004e-07, 'epoch': 2.91}\n",
            "{'loss': 1.6263, 'grad_norm': 1.2578125, 'learning_rate': 5.819177173402487e-07, 'epoch': 2.96}\n",
            "{'loss': 1.6524, 'grad_norm': 1.2890625, 'learning_rate': 5.399190665860555e-07, 'epoch': 3.0}\n",
            "{'loss': 1.6641, 'grad_norm': 1.3359375, 'learning_rate': 4.990721697025262e-07, 'epoch': 3.04}\n",
            "{'loss': 1.6661, 'grad_norm': 1.3515625, 'learning_rate': 4.594432805057786e-07, 'epoch': 3.09}\n",
            "{'loss': 1.6253, 'grad_norm': 1.296875, 'learning_rate': 4.210966771990063e-07, 'epoch': 3.13}\n",
            "{'loss': 1.6164, 'grad_norm': 1.2265625, 'learning_rate': 3.840945581130008e-07, 'epoch': 3.17}\n",
            "{'loss': 1.6, 'grad_norm': 1.328125, 'learning_rate': 3.4849694082023197e-07, 'epoch': 3.22}\n",
            "{'loss': 1.6056, 'grad_norm': 1.234375, 'learning_rate': 3.143615647861235e-07, 'epoch': 3.26}\n",
            "{'loss': 1.6726, 'grad_norm': 1.3046875, 'learning_rate': 2.817437977154319e-07, 'epoch': 3.3}\n",
            "{'loss': 1.588, 'grad_norm': 1.2265625, 'learning_rate': 2.5069654574562384e-07, 'epoch': 3.35}\n",
            "{'loss': 1.6309, 'grad_norm': 1.2890625, 'learning_rate': 2.2127016763292959e-07, 'epoch': 3.39}\n",
            "{'loss': 1.618, 'grad_norm': 1.328125, 'learning_rate': 1.9351239307025677e-07, 'epoch': 3.44}\n",
            "{'loss': 1.6282, 'grad_norm': 1.2578125, 'learning_rate': 1.6746824526945163e-07, 'epoch': 3.48}\n",
            "{'loss': 1.6289, 'grad_norm': 1.1875, 'learning_rate': 1.4317996793348778e-07, 'epoch': 3.52}\n",
            "{'loss': 1.6316, 'grad_norm': 1.2578125, 'learning_rate': 1.2068695673702237e-07, 'epoch': 3.57}\n",
            "{'loss': 1.6392, 'grad_norm': 1.4296875, 'learning_rate': 1.0002569542646972e-07, 'epoch': 3.61}\n",
            "{'loss': 1.6568, 'grad_norm': 1.2890625, 'learning_rate': 8.122969664323161e-08, 'epoch': 3.65}\n",
            "{'loss': 1.6644, 'grad_norm': 1.265625, 'learning_rate': 6.432944756606815e-08, 'epoch': 3.7}\n",
            "{'loss': 1.6623, 'grad_norm': 1.3515625, 'learning_rate': 4.9352360460784705e-08, 'epoch': 3.74}\n",
            "{'loss': 1.6307, 'grad_norm': 1.3515625, 'learning_rate': 3.6322728217434985e-08, 'epoch': 3.78}\n",
            "{'loss': 1.6145, 'grad_norm': 1.234375, 'learning_rate': 2.526168494716541e-08, 'epoch': 3.83}\n",
            "{'loss': 1.6415, 'grad_norm': 1.34375, 'learning_rate': 1.6187171702608946e-08, 'epoch': 3.87}\n",
            "{'loss': 1.6576, 'grad_norm': 1.296875, 'learning_rate': 9.113907377432534e-09, 'epoch': 3.91}\n",
            "{'loss': 1.6778, 'grad_norm': 1.234375, 'learning_rate': 4.053364832237589e-09, 'epoch': 3.96}\n",
            "{'loss': 1.5691, 'grad_norm': 1.2578125, 'learning_rate': 1.0137522855380166e-09, 'epoch': 4.0}\n",
            "{'train_runtime': 4021.4275, 'train_samples_per_second': 5.849, 'train_steps_per_second': 0.023, 'train_loss': 1.6348926813706108, 'epoch': 4.0}\n",
            "100% 92/92 [1:07:00<00:00, 43.70s/it]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mFT4\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251104_064550-rj654dsv/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a9Sy8U50R9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00a9Sy8U50R9",
        "outputId": "45efde51-4fa6-4f76-c4c2-3534979746be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-04 08:06:45.038308: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-04 08:06:45.055794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762243605.077405   46369 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762243605.083977   46369 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762243605.100585   46369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243605.100616   46369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243605.100619   46369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243605.100622   46369 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-04 08:06:45.105506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-04 08:06:52 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-04 08:06:54 [utils.py:328] non-default args: {'trust_remote_code': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': '../ckpt/FT4'}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 11-04 08:07:09 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-04 08:07:09 [__init__.py:1815] Using max model len 8192\n",
            "INFO 11-04 08:07:12 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-04 08:07:12 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-04 08:07:17.125523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762243637.146826   46602 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762243637.153310   46602 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762243637.169122   46602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243637.169150   46602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243637.169153   46602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762243637.169155   46602 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-04 08:07:24 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:26 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:26 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='../ckpt/FT4', speculative_config=None, tokenizer='../ckpt/FT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../ckpt/FT4, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1104 08:07:29.126333140 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:29 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m WARNING 11-04 08:07:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:29 [gpu_model_runner.py:2338] Starting to load model ../ckpt/FT4...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:29 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:29 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards: 100% 1/1 [00:00<00:00,  4.56it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:30 [default_loader.py:268] Loading weights took 0.23 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:31 [gpu_model_runner.py:2392] Model loading took 0.2550 GiB and 0.431594 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:38 [backends.py:539] Using cache directory: .vllm_cache/torch_compile_cache/92ae3f8b22/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:38 [backends.py:550] Dynamo bytecode transform time: 6.71 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m [rank0]:W1104 08:07:39.447000 46602 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:07:40 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:03 [backends.py:215] Compiling a graph for dynamic shape takes 24.76 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:04 [monitor.py:34] torch.compile takes 31.47 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:05 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:06 [kv_cache_utils.py:864] GPU KV cache size: 688,720 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:06 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 84.07x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 67/67 [00:02<00:00, 24.94it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:10 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.44 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:10 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.7, 15.51 GiB). Actual usage is 0.25 GiB for weight, 0.46 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.44 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15241394995` to fit into requested memory, or `--kv-cache-memory=22149793792` to fully utilize gpu memory. Current kv cache memory in use is 15868443443 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=46602)\u001b[0;0m INFO 11-04 08:08:10 [core.py:218] init engine (profile, create kv cache, warmup model) took 39.09 seconds\n",
            "INFO 11-04 08:08:10 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-04 08:08:10 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100% 100/100 [00:00<00:00, 1736.95it/s]\n",
            "Processed prompts: 100% 100/100 [00:10<00:00,  9.21it/s, est. speed input: 506.23 toks/s, output: 8526.91 toks/s]\n",
            "[rank0]:[W1104 08:08:21.819462875 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/FT4 ./results/SmolLM2-135M-SFT-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "irzzudFF34QZ",
      "metadata": {
        "id": "irzzudFF34QZ"
      },
      "source": [
        "#### My Hyperparameter Tuning 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VpLZYP3g36-p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpLZYP3g36-p",
        "outputId": "1d783544-b541-42f0-c2ae-194bff53eda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 0.6B using 1 GPUs, 1 batch size per GPU, 256 gradient accumulation steps\n",
            "2025-11-05 07:26:51.841072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762327611.874599    1599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762327611.887007    1599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762327611.910057    1599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762327611.910090    1599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762327611.910099    1599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762327611.910106    1599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-05 07:26:51.916863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model in torch.bfloat16 precision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Smollm2 detected, using custom chat template\n",
            "lazy_preprocess=True but data is still preprocessed eagerly for clarity.\n",
            "Loaded dataset from smol-smoltalk-6k.json.\n",
            "Loaded preprocessed features from smol-smoltalk-6k_processed.pickle.\n",
            "#train 5880, #eval 120\n",
            "/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/train_hw_parallel.py:676: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run iq2spcie (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run iq2spcie (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run iq2spcie (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/wandb/run-20251105_072736-iq2spcie\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFT5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework2/runs/iq2spcie\u001b[0m\n",
            "  0% 0/92 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ && bash scripts/sft.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sCYM3uO739ug",
      "metadata": {
        "id": "sCYM3uO739ug"
      },
      "outputs": [],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework2/COMP4901B-LLMs/assignment2/ifeval && bash run.sh ../ckpt/FT4 ./results/SmolLM2-135M-SFT-5"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
