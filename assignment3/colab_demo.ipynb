{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d3b43e0",
      "metadata": {
        "id": "8d3b43e0"
      },
      "source": [
        "###  Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45c19e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c19e77",
        "outputId": "50620fc0-a8f5-4b6e-afcd-7d439099032f",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420dc8b7",
      "metadata": {
        "id": "420dc8b7"
      },
      "source": [
        "### Clone Codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20bdbde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20bdbde",
        "outputId": "d69184eb-b0e9-4087-8326-0ac8d98a1bae",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMP4901B-LLMs'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 232 (delta 40), reused 24 (delta 24), pack-reused 169 (from 1)\u001b[K\n",
            "Receiving objects: 100% (232/232), 798.55 KiB | 1.87 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ],
      "source": [
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework3\n",
        "! cd /content/drive/MyDrive/COMP4901B-Homework3 && git clone https://github.com/hkust-nlp/COMP4901B-LLMs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c6cfe0",
      "metadata": {
        "id": "29c6cfe0"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5510016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5510016",
        "outputId": "c203baf9-5faa-4213-beb0-e0dda72aa99f",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (1.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]==4.57.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]==4.57.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]==4.57.1) (3.0.3)\n",
            "Collecting vllm==0.10.2\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.10.2)\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.121.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm==0.10.2)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm==0.10.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.10.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm==0.10.2)\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.10.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.10.2)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm==0.10.2)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm==0.10.2)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm==0.10.2)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.1.2)\n",
            "Collecting watchfiles (from vllm==0.10.2)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.16.3)\n",
            "Collecting ninja (from vllm==0.10.2)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm==0.10.2)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm==0.10.2)\n",
            "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm==0.10.2)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2)\n",
            "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm==0.10.2)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm==0.10.2)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm==0.10.2) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm==0.10.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm==0.10.2) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm==0.10.2) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.49.3)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.4)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cli-0.0.16-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.4.2)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm==0.10.2) (13.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm==0.10.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm==0.10.2) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.22.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.20.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.2) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm==0.10.2) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.28.0)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm==0.10.2) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.44.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.1.2)\n",
            "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl (71.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.16-py3-none-any.whl (12 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, click, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.8 cbor2-5.7.1 click-8.2.1 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.16 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.51.1 rich-toolkit-0.15.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763447 sha256=c10a0ab467f7dc1a924743a54930189337f927d076b22c7f957c3c3c6ff04975\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/ad/2e/e03d4739ddc0417efd8a120c2b9e784005aa226037e558c163\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: deepspeed\n",
            "Successfully installed deepspeed-0.18.2\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hjson\n",
            "Successfully installed hjson-3.1.0\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (4.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=824f1a7c968b72a67fba23184eec12f139adf73130d5e1e3e3f2ab5ae217daf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (79.0.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
            "Fetching 10 files: 100% 10/10 [00:02<00:00,  3.68it/s]\n",
            "Fetching 6 files: 100% 6/6 [00:02<00:00,  2.54it/s]\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/setup.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1 code evaulation for q1"
      ],
      "metadata": {
        "id": "1g5V_geSlmBY"
      },
      "id": "1g5V_geSlmBY"
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/ && python tests/test_format_prompts.py"
      ],
      "metadata": {
        "id": "MmryTBAWlrtb",
        "outputId": "ef7f48ea-131f-4f00-bc11-ccbd8f8d76a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "MmryTBAWlrtb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SANITY CHECK: format_prompts()\n",
            "======================================================================\n",
            "\n",
            "This script tests your implementation of the format_prompts function.\n",
            "It compares your outputs with expected reference outputs.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TEST 2: Few-shot without chat template\n",
            "======================================================================\n",
            "2025-11-13 09:43:54.067356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763027034.487824    6130 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763027034.594252    6130 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763027035.283583    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283628    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283633    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283637    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-13 09:43:55.353037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-13 09:44:17 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
            "WARNING 11-13 09:44:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
            "✅ PASSED: Few-shot prompt contains example problems\n",
            "\n",
            "======================================================================\n",
            "TEST 3: With chat template (using Qwen tokenizer)\n",
            "======================================================================\n",
            "Loading tokenizer: Qwen/Qwen2.5-0.5B-Instruct\n",
            "tokenizer_config.json: 7.30kB [00:00, 9.93MB/s]\n",
            "vocab.json: 2.78MB [00:00, 27.6MB/s]\n",
            "merges.txt: 1.67MB [00:00, 30.8MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 55.6MB/s]\n",
            "✅ PASSED: Chat template markers found in prompt\n",
            "\n",
            "Formatted prompt preview:\n",
            "<|im_start|>system\n",
            "You are a helpful math tutor.<|im_end|>\n",
            "<|im_start|>user\n",
            "If John has 5 apples and gives 2 to Mary, how many apples does he have left?\n",
            "Please reason step by step, and put your final ...\n",
            "\n",
            "======================================================================\n",
            "TEST 4: System message inclusion\n",
            "======================================================================\n",
            "✅ PASSED: System message appears to be included\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "✅ PASSED: Few-shot without chat\n",
            "✅ PASSED: With chat template\n",
            "✅ PASSED: System message\n",
            "\n",
            "Total: 3/3 passed\n",
            "\n",
            "🎉 All tests passed! Your format_prompts implementation looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe18aa7",
      "metadata": {
        "id": "dfe18aa7"
      },
      "source": [
        "### Q4 Start The Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Default"
      ],
      "metadata": {
        "id": "cwGpX4aNIjXF"
      },
      "id": "cwGpX4aNIjXF"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76f6abed",
      "metadata": {
        "id": "76f6abed",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68305762-e469-4d37-a6fa-d8218f85deae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: first-run\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/first-run\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 1\n",
            "  Gradient Accumulation Steps: 128 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/first-run/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-14 11:42:24.446372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 11:42:24.463881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763120544.485531    3843 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763120544.492055    3843 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763120544.508552    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508578    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508580    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508582    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 11:42:24.513471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-14 11:42:33 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-14 11:42:46 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-14 11:43:04 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-14 11:43:04 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-14 11:43:06 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-14 11:43:07 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-14 11:43:12.073903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763120592.095164    4156 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763120592.101696    4156 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763120592.117735    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117761    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117764    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117766    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-14 11:43:19 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:20 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:20 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1114 11:43:23.497668660 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m WARNING 11-14 11:43:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:21<00:00, 21.86s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:21<00:00, 21.86s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:45 [default_loader.py:268] Loading weights took 21.88 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:46 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 22.256874 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:55 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/44c42810b9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:55 [backends.py:550] Dynamo bytecode transform time: 8.30 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m [rank0]:W1114 11:43:56.675000 4156 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:01 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:28 [backends.py:215] Compiling a graph for dynamic shape takes 32.38 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:31 [monitor.py:34] torch.compile takes 40.68 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 22.01it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15646582988` to fit into requested memory, or `--kv-cache-memory=20175461376` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [core.py:218] init engine (profile, create kv cache, warmup model) took 51.39 seconds\n",
            "INFO 11-14 11:44:39 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-14 11:44:39 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 893.40it/s] \n",
            "Processed prompts: 100%|██████████| 16000/16000 [13:16<00:00, 20.10it/s, est. speed input: 1739.72 toks/s, output: 4991.45 toks/s]\n",
            "[rank0]:[W1114 11:57:59.326135498 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/first-run/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9338\n",
            "Overall Accuracy: 58.36%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5805          0.5805         \n",
            "2     0.5755          0.7065         \n",
            "3     0.5782          0.7790         \n",
            "4     0.5787          0.8065         \n",
            "5     0.5809          0.8285         \n",
            "6     0.5820          0.8460         \n",
            "7     0.5822          0.8600         \n",
            "8     0.5836          0.8755         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/first-run/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "Filtered 9338 correct examples out of 16000 total examples\n",
            "Accuracy: 58.36%\n",
            "Saved to ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9338\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/first-run/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-14 11:58:16.249834: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 11:58:16.268720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763121496.290789    8657 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763121496.297503    8657 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763121496.314997    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315026    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315028    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315030    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 11:58:16.320411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['q_proj', 'down_proj', 'k_proj', 'up_proj', 'gate_proj', 'o_proj', 'v_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9338 examples from ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "Cache metadata mismatch for key 'data_mtime': expected 1763121490.0, found 1763102020.0. Regenerating.\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9338/9338 [00:19<00:00, 481.04it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251114_115913-q55p38u7\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run brisk-forest-2\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/q55p38u7\n",
            "Saved preprocessed features to ckpt/first-run/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9151, #eval 187\n",
            "100%|██████████| 72/72 [1:01:24<00:00, 51.17s/it]\n",
            "{'loss': 0.2842, 'grad_norm': 0.0412321574985981, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2738, 'grad_norm': 0.040876783430576324, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2756, 'grad_norm': 0.039690688252449036, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2788, 'grad_norm': 0.042680516839027405, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2809, 'grad_norm': 0.04258733242750168, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.07}\n",
            "{'loss': 0.3023, 'grad_norm': 0.040956296026706696, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2837, 'grad_norm': 0.046012185513973236, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04344983398914337, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.11}\n",
            "{'loss': 0.3043, 'grad_norm': 0.04215284064412117, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2864, 'grad_norm': 0.041301827877759933, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.14}\n",
            "{'loss': 0.3106, 'grad_norm': 0.0407261960208416, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2848, 'grad_norm': 0.040137290954589844, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3131, 'grad_norm': 0.0441724956035614, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.18}\n",
            "{'loss': 0.3004, 'grad_norm': 0.04114912077784538, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.2}\n",
            "{'loss': 0.3012, 'grad_norm': 0.041343312710523605, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3138, 'grad_norm': 0.045458462089300156, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2901, 'grad_norm': 0.04221639037132263, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.24}\n",
            "{'loss': 0.3004, 'grad_norm': 0.041205115616321564, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2917, 'grad_norm': 0.04758389666676521, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2836, 'grad_norm': 0.045194391161203384, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3034, 'grad_norm': 0.04089658707380295, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.279, 'grad_norm': 0.041532088071107864, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2823, 'grad_norm': 0.039261385798454285, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2984, 'grad_norm': 0.04346533119678497, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04092961177229881, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3003, 'grad_norm': 0.04204783961176872, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2885, 'grad_norm': 0.04121629521250725, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2815, 'grad_norm': 0.04102322831749916, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03983001038432121, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.41}\n",
            "{'loss': 0.2989, 'grad_norm': 0.03859668970108032, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2837, 'grad_norm': 0.0408283956348896, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2856, 'grad_norm': 0.040170829743146896, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2697, 'grad_norm': 0.03837982937693596, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.46}\n",
            "{'loss': 0.3013, 'grad_norm': 0.04542221501469612, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2877, 'grad_norm': 0.03982899710536003, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2936, 'grad_norm': 0.04119272157549858, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2878, 'grad_norm': 0.042690955102443695, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
            "{'loss': 0.27, 'grad_norm': 0.04045286774635315, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.53}\n",
            "{'loss': 0.2901, 'grad_norm': 0.042437903583049774, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.55}\n",
            "{'loss': 0.2893, 'grad_norm': 0.03962692990899086, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2783, 'grad_norm': 0.041780684143304825, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2988, 'grad_norm': 0.04224047064781189, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.59}\n",
            "{'loss': 0.278, 'grad_norm': 0.040542569011449814, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2957, 'grad_norm': 0.04034178704023361, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2691, 'grad_norm': 0.03883595019578934, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2887, 'grad_norm': 0.03946587070822716, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.64}\n",
            "{'loss': 0.268, 'grad_norm': 0.04042750597000122, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2663, 'grad_norm': 0.04243028908967972, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2815, 'grad_norm': 0.039937954396009445, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2827, 'grad_norm': 0.03981532156467438, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.7}\n",
            "{'loss': 0.272, 'grad_norm': 0.04017646238207817, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2908, 'grad_norm': 0.03881995379924774, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04091988131403923, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2821, 'grad_norm': 0.040974073112010956, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2897, 'grad_norm': 0.04128747805953026, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2919, 'grad_norm': 0.040972184389829636, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2675, 'grad_norm': 0.04944310709834099, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.8}\n",
            "{'loss': 0.272, 'grad_norm': 0.04125469550490379, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2968, 'grad_norm': 0.0401521734893322, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2825, 'grad_norm': 0.040450725704431534, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.84}\n",
            "{'loss': 0.3087, 'grad_norm': 0.04155230149626732, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.85}\n",
            "{'loss': 0.2939, 'grad_norm': 0.03926434740424156, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04088444262742996, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2785, 'grad_norm': 0.0381874181330204, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2843, 'grad_norm': 0.03971501812338829, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2822, 'grad_norm': 0.04046123847365379, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.92}\n",
            "{'loss': 0.282, 'grad_norm': 0.041982244700193405, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.94}\n",
            "{'loss': 0.3026, 'grad_norm': 0.038715165108442307, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2819, 'grad_norm': 0.03806982561945915, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.97}\n",
            "{'loss': 0.283, 'grad_norm': 0.04031049460172653, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2945, 'grad_norm': 0.039586521685123444, 'learning_rate': 3.805301908254455e-08, 'epoch': 0.99}\n",
            "{'loss': 0.272, 'grad_norm': 0.05442313104867935, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 3695.9688, 'train_samples_per_second': 2.476, 'train_steps_per_second': 0.019, 'train_loss': 0.28730813124113613, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1 into base model; saving to ckpt/first-run/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/first-run/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1/checkpoint-72 into base model; saving to ckpt/first-run/models/model_iter_1/checkpoint-72-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mbrisk-forest-2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251114_115913-q55p38u7/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/first-run/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/first-run\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9338 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/first-run/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/first-run/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/first-run/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/first-run/models/model_iter_1-merged \\\n",
        "    --output_dir results/default/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAGHw1LvQDlw",
        "outputId": "6945cf35-b699-43e6-b4cb-fdc0d5d16417"
      },
      "id": "hAGHw1LvQDlw",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/first-run/models/model_iter_1-merged\n",
            "Output Directory: results/default/rollout1\n",
            "Run Name: model_iter_1-merged_20251115_070341\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "\n",
            "2025-11-15 07:03:48.709790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 07:03:48.727304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190228.749049    7645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190228.755619    7645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190228.772163    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772200    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772202    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772203    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 07:03:48.777076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 07:03:57 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 07:04:08 [utils.py:328] non-default args: {'tokenizer': 'ckpt/first-run/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/first-run/models/model_iter_1-merged'}\n",
            "INFO 11-15 07:04:27 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 07:04:27 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 07:04:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 07:04:30 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 07:04:35.023300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190275.044564    7941 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190275.050998    7941 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190275.066554    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066579    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066582    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066585    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 07:04:42 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:43 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/first-run/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/first-run/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/first-run/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 07:04:46.653598055 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:46 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m WARNING 11-15 07:04:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:46 [gpu_model_runner.py:2338] Starting to load model ckpt/first-run/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:47 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:47 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.21s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.21s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:06 [default_loader.py:268] Loading weights took 19.24 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:07 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 19.635288 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:16 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/cb7b993d40/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:16 [backends.py:550] Dynamo bytecode transform time: 8.47 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:22 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:48 [backends.py:215] Compiling a graph for dynamic shape takes 31.92 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:50 [monitor.py:34] torch.compile takes 40.39 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.01it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [core.py:218] init engine (profile, create kv cache, warmup model) took 49.65 seconds\n",
            "INFO 11-15 07:05:58 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 07:05:58 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1727.47it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [00:42<00:00, 30.85it/s, est. speed input: 2724.18 toks/s, output: 8006.32 toks/s]\n",
            "[rank0]:[W1115 07:06:42.374997787 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "\n",
            "Loading data from results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 789\n",
            "Overall Accuracy: 59.82%\n",
            "\n",
            "Results saved to results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "  - Evaluation results: results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "  - Log file: results/default/rollout1/model_iter_1-merged_20251115_070341.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/first-run/models/model_iter_1-merged \\\n",
        "    --output_dir results/default/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38CxfTf28m3o",
        "outputId": "e1ef892a-e256-4e95-a69b-a8bf9baaa31c"
      },
      "id": "38CxfTf28m3o",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/first-run/models/model_iter_1-merged\n",
            "Output Directory: results\n",
            "Run Name: model_iter_1-merged_20251115_060728\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "\n",
            "2025-11-15 06:07:32.677374: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 06:07:32.694729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763186852.716271   12172 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763186852.722785   12172 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763186852.739114   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739143   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739145   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739147   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 06:07:32.743987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 06:07:39 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 06:07:42 [utils.py:328] non-default args: {'tokenizer': 'ckpt/first-run/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/first-run/models/model_iter_1-merged'}\n",
            "INFO 11-15 06:07:57 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 06:07:57 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 06:07:59 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 06:08:00 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 06:08:04.665665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763186884.687371   12402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763186884.693851   12402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763186884.709465   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709499   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709501   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709503   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 06:08:12 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:13 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:13 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/first-run/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/first-run/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/first-run/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 06:08:15.236307023 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:15 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m WARNING 11-15 06:08:15 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [gpu_model_runner.py:2338] Starting to load model ckpt/first-run/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:17 [default_loader.py:268] Loading weights took 0.84 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:18 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.043268 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:26 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ff3d868903/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:26 [backends.py:550] Dynamo bytecode transform time: 7.57 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.774 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:30 [monitor.py:34] torch.compile takes 7.57 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:31 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:32 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:32 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 22.39it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.03 seconds\n",
            "INFO 11-15 06:08:37 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 06:08:37 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1148.54it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:36<00:00, 10.59it/s, est. speed input: 935.21 toks/s, output: 2752.92 toks/s]\n",
            "[rank0]:[W1115 06:25:15.745767483 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "\n",
            "Loading data from results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6349\n",
            "Overall Accuracy: 60.17%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5944          0.5944         \n",
            "2     0.5986          0.7119         \n",
            "3     0.6037          0.7650         \n",
            "4     0.6025          0.7885         \n",
            "5     0.6020          0.8074         \n",
            "6     0.6022          0.8271         \n",
            "7     0.6026          0.8370         \n",
            "8     0.6017          0.8423         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/model_iter_1-merged_20251115_060728_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "  - Evaluation results: results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "  - Log file: results/model_iter_1-merged_20251115_060728.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase batch size (128)"
      ],
      "metadata": {
        "id": "fYxvzs9YKOXj"
      },
      "id": "fYxvzs9YKOXj"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --total_batch_size 128 --run_name batch_128 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fviKo_zjKSA9",
        "outputId": "9b4b9835-d0f0-4645-aab7-4a357da74a35"
      },
      "id": "fviKo_zjKSA9",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: batch_128\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/batch_128\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 1\n",
            "  Gradient Accumulation Steps: 128 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/batch_128/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 07:12:35.709819: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 07:12:35.727502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190755.750143   10861 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190755.756826   10861 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190755.773454   10861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190755.773489   10861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190755.773492   10861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190755.773494   10861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 07:12:35.778351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 07:12:43 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 07:12:45 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 07:13:01 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 07:13:01 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 07:13:03 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 07:13:04 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 07:13:09.333161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190789.354934   11093 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190789.361365   11093 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190789.377151   11093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190789.377192   11093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190789.377194   11093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190789.377196   11093 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 07:13:16 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:18 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:18 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 07:13:20.934572183 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:20 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m WARNING 11-15 07:13:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:20 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:21 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:21 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:32<00:00, 32.24s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:32<00:00, 32.25s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:53 [default_loader.py:268] Loading weights took 32.27 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:13:54 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 32.480298 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:02 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:02 [backends.py:550] Dynamo bytecode transform time: 7.75 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:03 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:08 [backends.py:215] Compiling a graph for dynamic shape takes 5.64 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:10 [monitor.py:34] torch.compile takes 13.40 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:11 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:11 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:11 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 22.05it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:15 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:15 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=11093)\u001b[0;0m INFO 11-15 07:14:15 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.25 seconds\n",
            "INFO 11-15 07:14:17 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 07:14:17 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 880.60it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:39<00:00, 57.27it/s, est. speed input: 4957.60 toks/s, output: 14176.18 toks/s]\n",
            "[rank0]:[W1115 07:18:59.443404301 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/batch_128/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_128/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9299\n",
            "Overall Accuracy: 58.12%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5940          0.5940         \n",
            "2     0.5863          0.7255         \n",
            "3     0.5822          0.7780         \n",
            "4     0.5807          0.8115         \n",
            "5     0.5806          0.8300         \n",
            "6     0.5810          0.8490         \n",
            "7     0.5827          0.8620         \n",
            "8     0.5812          0.8720         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/batch_128/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/batch_128/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/batch_128/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_128/iteration_0/evaluation.jsonl\n",
            "Filtered 9299 correct examples out of 16000 total examples\n",
            "Accuracy: 58.12%\n",
            "Saved to ckpt/batch_128/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9299\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/batch_128/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/batch_128/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 07:19:11.621576: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 07:19:11.640556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763191151.663300   12823 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763191151.669917   12823 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763191151.686726   12823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763191151.686757   12823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763191151.686760   12823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763191151.686761   12823 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 07:19:11.691867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['q_proj', 'up_proj', 'gate_proj', 'v_proj', 'o_proj', 'down_proj', 'k_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9299 examples from ckpt/batch_128/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9299/9299 [00:20<00:00, 463.76it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_072015-d9bfq8i1\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run blooming-sun-3\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/d9bfq8i1\n",
            "Saved preprocessed features to ckpt/batch_128/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9113, #eval 186\n",
            "100%|██████████| 72/72 [1:00:24<00:00, 50.34s/it]\n",
            "{'loss': 0.2995, 'grad_norm': 0.042226988822221756, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2922, 'grad_norm': 0.04799119010567665, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2814, 'grad_norm': 0.041058458387851715, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2672, 'grad_norm': 0.039537061005830765, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2974, 'grad_norm': 0.04454932361841202, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2894, 'grad_norm': 0.041933540254831314, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2832, 'grad_norm': 0.04462236166000366, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2758, 'grad_norm': 0.03932309150695801, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2879, 'grad_norm': 0.0413123220205307, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2715, 'grad_norm': 0.03982563689351082, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2753, 'grad_norm': 0.04083671048283577, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2711, 'grad_norm': 0.041676878929138184, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2615, 'grad_norm': 0.03963429480791092, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2812, 'grad_norm': 0.042216721922159195, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.2}\n",
            "{'loss': 0.3036, 'grad_norm': 0.04190446063876152, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2967, 'grad_norm': 0.04150931537151337, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2876, 'grad_norm': 0.042664747685194016, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2833, 'grad_norm': 0.04008271545171738, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.25}\n",
            "{'loss': 0.282, 'grad_norm': 0.04107949137687683, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2949, 'grad_norm': 0.04237160459160805, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2805, 'grad_norm': 0.03968646749854088, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2823, 'grad_norm': 0.0406811460852623, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.31}\n",
            "{'loss': 0.299, 'grad_norm': 0.039774756878614426, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.32}\n",
            "{'loss': 0.284, 'grad_norm': 0.0397825688123703, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.34}\n",
            "{'loss': 0.3001, 'grad_norm': 0.03914254531264305, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}\n",
            "{'loss': 0.2876, 'grad_norm': 0.03987764194607735, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2878, 'grad_norm': 0.038704175502061844, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.38}\n",
            "{'loss': 0.277, 'grad_norm': 0.04020635783672333, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2931, 'grad_norm': 0.04018900915980339, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.41}\n",
            "{'loss': 0.2745, 'grad_norm': 0.03826232999563217, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2887, 'grad_norm': 0.03955291956663132, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2895, 'grad_norm': 0.03998816758394241, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2807, 'grad_norm': 0.04079308360815048, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2947, 'grad_norm': 0.04593688249588013, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2929, 'grad_norm': 0.041139569133520126, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.49}\n",
            "{'loss': 0.3007, 'grad_norm': 0.040444374084472656, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.51}\n",
            "{'loss': 0.29, 'grad_norm': 0.04014373943209648, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2865, 'grad_norm': 0.04112585633993149, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.53}\n",
            "{'loss': 0.284, 'grad_norm': 0.038396645337343216, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.55}\n",
            "{'loss': 0.3005, 'grad_norm': 0.03895274177193642, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2977, 'grad_norm': 0.039979927241802216, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2971, 'grad_norm': 0.042803846299648285, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2779, 'grad_norm': 0.04469168931245804, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2944, 'grad_norm': 0.03755061328411102, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2916, 'grad_norm': 0.04061325639486313, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2866, 'grad_norm': 0.043682489544153214, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2893, 'grad_norm': 0.03716009110212326, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2757, 'grad_norm': 0.03849879279732704, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2762, 'grad_norm': 0.038841187953948975, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2732, 'grad_norm': 0.041564229875802994, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2893, 'grad_norm': 0.044988904148340225, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2957, 'grad_norm': 0.04048853740096092, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2982, 'grad_norm': 0.04105979576706886, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}\n",
            "{'loss': 0.287, 'grad_norm': 0.039393242448568344, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.76}\n",
            "{'loss': 0.3032, 'grad_norm': 0.03787882626056671, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2872, 'grad_norm': 0.041913989931344986, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2869, 'grad_norm': 0.039279717952013016, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.8}\n",
            "{'loss': 0.2914, 'grad_norm': 0.040632449090480804, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2913, 'grad_norm': 0.040383219718933105, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2864, 'grad_norm': 0.03906608745455742, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2804, 'grad_norm': 0.03808913007378578, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2888, 'grad_norm': 0.040875889360904694, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2674, 'grad_norm': 0.03802746161818504, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2937, 'grad_norm': 0.040601570159196854, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2809, 'grad_norm': 0.040189504623413086, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2911, 'grad_norm': 0.03950371593236923, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2831, 'grad_norm': 0.03923819959163666, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2824, 'grad_norm': 0.04119211435317993, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2635, 'grad_norm': 0.03882983699440956, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.97}\n",
            "{'loss': 0.2857, 'grad_norm': 0.04005034640431404, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.98}\n",
            "{'loss': 0.306, 'grad_norm': 0.039831023663282394, 'learning_rate': 3.805301908254455e-08, 'epoch': 1.0}\n",
            "{'loss': 0.2813, 'grad_norm': 0.08954862505197525, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 3639.8201, 'train_samples_per_second': 2.504, 'train_steps_per_second': 0.02, 'train_loss': 0.2866300054722362, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/batch_128/models/model_iter_1 into base model; saving to ckpt/batch_128/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/batch_128/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/batch_128/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/batch_128/models/model_iter_1/checkpoint-72 into base model; saving to ckpt/batch_128/models/model_iter_1/checkpoint-72-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mblooming-sun-3\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_072015-d9bfq8i1/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/batch_128/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/batch_128\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9299 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/batch_128/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/batch_128/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/batch_128/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_128/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "aYbYLS1_Scvr"
      },
      "id": "aYbYLS1_Scvr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_128/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "8lRG831YSeEr"
      },
      "id": "8lRG831YSeEr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase batch size (256)"
      ],
      "metadata": {
        "id": "bNDay-KiIzGv"
      },
      "id": "bNDay-KiIzGv"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --total_batch_size 256 --run_name batch_256 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3UEKnTJMhJ",
        "outputId": "ab81c2fc-e311-493f-80da-1ed7145de99b"
      },
      "id": "MD3UEKnTJMhJ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: batch_256\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/batch_256\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 256\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 64 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/batch_256/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 09:11:10.826084: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:11:10.843352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763197870.864276   43275 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763197870.870662   43275 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763197870.886706   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886730   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886732   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886734   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:11:10.891479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 09:11:18 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 09:11:20 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 09:11:35 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 09:11:35 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 09:11:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 09:11:38 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 09:11:42.846849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763197902.867840   43529 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763197902.874276   43529 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763197902.889891   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889914   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889916   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889918   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 09:11:50 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:51 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:51 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 09:11:54.275392244 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m WARNING 11-15 09:11:54 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:55 [default_loader.py:268] Loading weights took 1.01 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:56 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.247750 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:04 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:04 [backends.py:550] Dynamo bytecode transform time: 7.42 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.724 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:08 [monitor.py:34] torch.compile takes 7.42 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:09 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:10 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:10 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 24.46it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.27 seconds\n",
            "INFO 11-15 09:12:15 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 09:12:15 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 825.78it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:16<00:00, 62.29it/s, est. speed input: 5391.89 toks/s, output: 15429.70 toks/s]\n",
            "[rank0]:[W1115 09:16:35.054693913 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_256/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9309\n",
            "Overall Accuracy: 58.18%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5820          0.5820         \n",
            "2     0.5813          0.7185         \n",
            "3     0.5810          0.7770         \n",
            "4     0.5816          0.8195         \n",
            "5     0.5814          0.8425         \n",
            "6     0.5797          0.8550         \n",
            "7     0.5795          0.8650         \n",
            "8     0.5818          0.8770         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/batch_256/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "Filtered 9309 correct examples out of 16000 total examples\n",
            "Accuracy: 58.18%\n",
            "Saved to ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9309\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/batch_256/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 09:16:44.115618: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:16:44.133291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763198204.154940   44950 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763198204.161532   44950 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763198204.177783   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177809   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177811   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177812   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:16:44.182611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['o_proj', 'gate_proj', 'v_proj', 'q_proj', 'k_proj', 'up_proj', 'down_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9309 examples from ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9309/9309 [00:19<00:00, 484.83it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run kpqxntam\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_091731-kpqxntam\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run stellar-music-6\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/kpqxntam\n",
            "Saved preprocessed features to ckpt/batch_256/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9122, #eval 187\n",
            "100%|██████████| 36/36 [15:11<00:00, 25.32s/it]\n",
            "{'loss': 0.2855, 'grad_norm': 0.02936842478811741, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2798, 'grad_norm': 0.02932041883468628, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2748, 'grad_norm': 0.029304608702659607, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2834, 'grad_norm': 0.030547577887773514, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.11}\n",
            "{'loss': 0.288, 'grad_norm': 0.029294265434145927, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2855, 'grad_norm': 0.030200418084859848, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.17}\n",
            "{'loss': 0.281, 'grad_norm': 0.031050531193614006, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2804, 'grad_norm': 0.031001780182123184, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2912, 'grad_norm': 0.03204762563109398, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2878, 'grad_norm': 0.02918592467904091, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.28}\n",
            "{'loss': 0.277, 'grad_norm': 0.02910374291241169, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2892, 'grad_norm': 0.030668534338474274, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.34}\n",
            "{'loss': 0.277, 'grad_norm': 0.028750890865921974, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2963, 'grad_norm': 0.028941581025719643, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2773, 'grad_norm': 0.030207257717847824, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2967, 'grad_norm': 0.028317589312791824, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2874, 'grad_norm': 0.030486783012747765, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2751, 'grad_norm': 0.027700379490852356, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2843, 'grad_norm': 0.028920220211148262, 'learning_rate': 1e-05, 'epoch': 0.53}\n",
            "{'loss': 0.2957, 'grad_norm': 0.03000880964100361, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2955, 'grad_norm': 0.029308581724762917, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2735, 'grad_norm': 0.030717018991708755, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2806, 'grad_norm': 0.030530961230397224, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2813, 'grad_norm': 0.02824261598289013, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.67}\n",
            "{'loss': 0.3039, 'grad_norm': 0.029841937124729156, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2796, 'grad_norm': 0.029519828036427498, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.73}\n",
            "{'loss': 0.283, 'grad_norm': 0.030242038890719414, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2901, 'grad_norm': 0.028266804292798042, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2834, 'grad_norm': 0.027982031926512718, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2868, 'grad_norm': 0.028976362198591232, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2799, 'grad_norm': 0.02982487343251705, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.87}\n",
            "{'loss': 0.28, 'grad_norm': 0.0277622789144516, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2915, 'grad_norm': 0.03353345766663551, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2999, 'grad_norm': 0.03154942765831947, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2822, 'grad_norm': 0.029232030734419823, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.98}\n",
            "{'loss': 0.2872, 'grad_norm': 0.03809868171811104, 'learning_rate': 3.805301908254455e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 913.4291, 'train_samples_per_second': 9.987, 'train_steps_per_second': 0.039, 'train_loss': 0.2853355672624376, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1 into base model; saving to ckpt/batch_256/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1/checkpoint-30 into base model; saving to ckpt/batch_256/models/model_iter_1/checkpoint-30-merged\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1/checkpoint-36 into base model; saving to ckpt/batch_256/models/model_iter_1/checkpoint-36-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-music-6\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_091731-kpqxntam/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/batch_256/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/batch_256\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9309 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/batch_256/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/batch_256/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/batch_256/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_256/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_256/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "_1YFTDPsRgT9"
      },
      "id": "_1YFTDPsRgT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_256/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_256/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "PTLNC951SPvP"
      },
      "id": "PTLNC951SPvP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase queries (3000)"
      ],
      "metadata": {
        "id": "ZQmWzY-OMBA9"
      },
      "id": "ZQmWzY-OMBA9"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --n_queries 3000 --run_name queries_3000 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s1qgYN-MD5W",
        "outputId": "1ce0e0fe-c3a3-4cec-d0cc-376a28f79852"
      },
      "id": "9s1qgYN-MD5W",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: queries_3000\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/queries_3000\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 3000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/queries_3000/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 08:37:09.974081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 08:37:09.990737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763195830.011773   34057 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763195830.018191   34057 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763195830.033905   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033930   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033932   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033933   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 08:37:10.038661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 08:37:17 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 08:37:19 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 08:37:34 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 08:37:34 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 08:37:36 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 08:37:37 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 08:37:41.736959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763195861.759942   34283 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763195861.766525   34283 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763195861.782378   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782405   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782407   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782410   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 08:37:49 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:50 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:50 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 08:37:52.905517802 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:52 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m WARNING 11-15 08:37:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:52 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:53 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:53 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:54 [default_loader.py:268] Loading weights took 0.91 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:55 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.116693 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:03 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:03 [backends.py:550] Dynamo bytecode transform time: 7.50 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.773 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:07 [monitor.py:34] torch.compile takes 7.50 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 22.29it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.73 seconds\n",
            "INFO 11-15 08:38:14 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 08:38:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 3000/3000 [00:03<00:00, 911.74it/s]\n",
            "Processed prompts: 100%|██████████| 24000/24000 [06:55<00:00, 57.75it/s, est. speed input: 4961.75 toks/s, output: 14249.20 toks/s]\n",
            "[rank0]:[W1115 08:45:13.899744591 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_3000/iteration_0/inference.jsonl\n",
            "Loaded 24000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 24000\n",
            "Correct answers: 14191\n",
            "Overall Accuracy: 59.13%\n",
            "\n",
            "Detected multiple rollouts: 3000 unique questions with 24000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5817          0.5817         \n",
            "2     0.5872          0.7167         \n",
            "3     0.5903          0.7733         \n",
            "4     0.5917          0.8130         \n",
            "5     0.5896          0.8333         \n",
            "6     0.5919          0.8560         \n",
            "7     0.5914          0.8680         \n",
            "8     0.5913          0.8800         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/queries_3000/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "Filtered 14191 correct examples out of 24000 total examples\n",
            "Accuracy: 59.13%\n",
            "Saved to ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 14191\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/queries_3000/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 08:45:24.536046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 08:45:24.554338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763196324.576218   36398 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763196324.582845   36398 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763196324.599725   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599754   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599756   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599758   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 08:45:24.604818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['up_proj', 'k_proj', 'q_proj', 'down_proj', 'gate_proj', 'v_proj', 'o_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 14191 examples from ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 14191/14191 [00:30<00:00, 465.50it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run shgyhjje\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_084630-shgyhjje\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run frosty-breeze-5\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/shgyhjje\n",
            "Saved preprocessed features to ckpt/queries_3000/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 13907, #eval 284\n",
            "{'loss': 0.2768, 'grad_norm': 0.04028002545237541, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2854, 'grad_norm': 0.04273822158575058, 'learning_rate': 1.9995846763238514e-05, 'epoch': 0.02}\n",
            "{'loss': 0.2759, 'grad_norm': 0.04066867008805275, 'learning_rate': 1.9983390502829168e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2784, 'grad_norm': 0.039802078157663345, 'learning_rate': 1.9962641565531694e-05, 'epoch': 0.04}\n",
            "{'loss': 0.292, 'grad_norm': 0.04141656309366226, 'learning_rate': 1.9933617186395917e-05, 'epoch': 0.05}\n",
            "{'loss': 0.2999, 'grad_norm': 0.04372560605406761, 'learning_rate': 1.9896341474445526e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2916, 'grad_norm': 0.04110953211784363, 'learning_rate': 1.985084539265195e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2879, 'grad_norm': 0.04345061257481575, 'learning_rate': 1.9797166732215078e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2751, 'grad_norm': 0.03979578614234924, 'learning_rate': 1.973535008117207e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2743, 'grad_norm': 0.041141096502542496, 'learning_rate': 1.9665446787360444e-05, 'epoch': 0.09}\n",
            "{'loss': 0.2959, 'grad_norm': 0.04029257968068123, 'learning_rate': 1.9587514915766124e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2928, 'grad_norm': 0.04131486639380455, 'learning_rate': 1.950161920029191e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2829, 'grad_norm': 0.04314209893345833, 'learning_rate': 1.940783098998643e-05, 'epoch': 0.12}\n",
            "{'loss': 0.304, 'grad_norm': 0.04469216614961624, 'learning_rate': 1.9306228189778255e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2861, 'grad_norm': 0.042133644223213196, 'learning_rate': 1.9196895195764363e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2803, 'grad_norm': 0.042241666465997696, 'learning_rate': 1.907992282510675e-05, 'epoch': 0.15}\n",
            "{'loss': 0.272, 'grad_norm': 0.04037926718592644, 'learning_rate': 1.8955408240595396e-05, 'epoch': 0.16}\n",
            "{'loss': 0.2963, 'grad_norm': 0.04457990452647209, 'learning_rate': 1.8823454869940243e-05, 'epoch': 0.17}\n",
            "{'loss': 0.275, 'grad_norm': 0.04307347163558006, 'learning_rate': 1.8684172319859258e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3016, 'grad_norm': 0.04214591905474663, 'learning_rate': 1.8537676285033886e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2907, 'grad_norm': 0.042574841529130936, 'learning_rate': 1.838408845200758e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2798, 'grad_norm': 0.03946307301521301, 'learning_rate': 1.8223536398107177e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2997, 'grad_norm': 0.04106869921088219, 'learning_rate': 1.8056153485471167e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2875, 'grad_norm': 0.04432719573378563, 'learning_rate': 1.788207875027274e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2969, 'grad_norm': 0.040162332355976105, 'learning_rate': 1.7701456787229805e-05, 'epoch': 0.23}\n",
            "{'loss': 0.2832, 'grad_norm': 0.03990985080599785, 'learning_rate': 1.751443762949772e-05, 'epoch': 0.24}\n",
            "{'loss': 0.302, 'grad_norm': 0.04155459627509117, 'learning_rate': 1.732117662404469e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2882, 'grad_norm': 0.04068106412887573, 'learning_rate': 1.712183430261319e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2864, 'grad_norm': 0.04048789665102959, 'learning_rate': 1.691657624837472e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2806, 'grad_norm': 0.039855845272541046, 'learning_rate': 1.6705572958388576e-05, 'epoch': 0.28}\n",
            "{'loss': 0.265, 'grad_norm': 0.04014814645051956, 'learning_rate': 1.6488999701978905e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2717, 'grad_norm': 0.04080072417855263, 'learning_rate': 1.6267036375147728e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2897, 'grad_norm': 0.040694400668144226, 'learning_rate': 1.6039867351144778e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2935, 'grad_norm': 0.04137460142374039, 'learning_rate': 1.5807681327318372e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2817, 'grad_norm': 0.043865349143743515, 'learning_rate': 1.557067116837444e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2876, 'grad_norm': 0.04406243562698364, 'learning_rate': 1.5329033746173975e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2865, 'grad_norm': 0.0383472703397274, 'learning_rate': 1.5082969776201948e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2792, 'grad_norm': 0.04001421481370926, 'learning_rate': 1.483268365084351e-05, 'epoch': 0.35}\n",
            "{'loss': 0.29, 'grad_norm': 0.044118039309978485, 'learning_rate': 1.4578383269606004e-05, 'epoch': 0.36}\n",
            "{'loss': 0.3094, 'grad_norm': 0.044070664793252945, 'learning_rate': 1.4320279866427798e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2651, 'grad_norm': 0.040482115000486374, 'learning_rate': 1.4058587834217356e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2848, 'grad_norm': 0.04045175388455391, 'learning_rate': 1.3793524546768358e-05, 'epoch': 0.39}\n",
            "{'loss': 0.291, 'grad_norm': 0.0408407598733902, 'learning_rate': 1.3525310178198707e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2752, 'grad_norm': 0.03954731673002243, 'learning_rate': 1.325416752006351e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2924, 'grad_norm': 0.03868754953145981, 'learning_rate': 1.2980321796293838e-05, 'epoch': 0.41}\n",
            "{'loss': 0.3134, 'grad_norm': 0.04140065237879753, 'learning_rate': 1.2704000476115079e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3099, 'grad_norm': 0.04064010828733444, 'learning_rate': 1.2425433085100224e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2833, 'grad_norm': 0.03956145420670509, 'learning_rate': 1.2144851014515055e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2653, 'grad_norm': 0.03889938443899155, 'learning_rate': 1.1862487329113606e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2826, 'grad_norm': 0.04027905687689781, 'learning_rate': 1.1578576573543541e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2894, 'grad_norm': 0.041782014071941376, 'learning_rate': 1.1293354577522264e-05, 'epoch': 0.47}\n",
            "{'loss': 0.3003, 'grad_norm': 0.043520957231521606, 'learning_rate': 1.1007058259945584e-05, 'epoch': 0.48}\n",
            "{'loss': 0.294, 'grad_norm': 0.041531480848789215, 'learning_rate': 1.0719925432091671e-05, 'epoch': 0.49}\n",
            "{'loss': 0.279, 'grad_norm': 0.03927183523774147, 'learning_rate': 1.043219460008374e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2791, 'grad_norm': 0.03828730806708336, 'learning_rate': 1.0144104766775574e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2788, 'grad_norm': 0.04105425998568535, 'learning_rate': 9.855895233224431e-06, 'epoch': 0.52}\n",
            "{'loss': 0.2829, 'grad_norm': 0.04085002839565277, 'learning_rate': 9.56780539991626e-06, 'epoch': 0.52}\n",
            "{'loss': 0.292, 'grad_norm': 0.0407135970890522, 'learning_rate': 9.28007456790833e-06, 'epoch': 0.53}\n",
            "{'loss': 0.277, 'grad_norm': 0.03943243250250816, 'learning_rate': 8.992941740054418e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2737, 'grad_norm': 0.037176404148340225, 'learning_rate': 8.706645422477739e-06, 'epoch': 0.55}\n",
            "{'loss': 0.2783, 'grad_norm': 0.03938686102628708, 'learning_rate': 8.42142342645646e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2848, 'grad_norm': 0.03843335434794426, 'learning_rate': 8.137512670886397e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2783, 'grad_norm': 0.037473294883966446, 'learning_rate': 7.855148985484946e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2866, 'grad_norm': 0.0399303063750267, 'learning_rate': 7.574566914899779e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2764, 'grad_norm': 0.03781687468290329, 'learning_rate': 7.295999523884921e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2873, 'grad_norm': 0.041501484811306, 'learning_rate': 7.019678203706164e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03989977389574051, 'learning_rate': 6.745832479936492e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2842, 'grad_norm': 0.04078992083668709, 'learning_rate': 6.474689821801295e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2848, 'grad_norm': 0.0419076532125473, 'learning_rate': 6.206475453231644e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2949, 'grad_norm': 0.04276753216981888, 'learning_rate': 5.941412165782645e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2801, 'grad_norm': 0.041444119065999985, 'learning_rate': 5.6797201335722064e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2879, 'grad_norm': 0.03962406516075134, 'learning_rate': 5.421616730394e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2901, 'grad_norm': 0.040491681545972824, 'learning_rate': 5.167316349156495e-06, 'epoch': 0.67}\n",
            "{'loss': 0.3025, 'grad_norm': 0.04124848544597626, 'learning_rate': 4.917030223798057e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2809, 'grad_norm': 0.03776826709508896, 'learning_rate': 4.670966253826027e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2802, 'grad_norm': 0.043126728385686874, 'learning_rate': 4.429328831625565e-06, 'epoch': 0.7}\n",
            "{'loss': 0.286, 'grad_norm': 0.039537299424409866, 'learning_rate': 4.192318672681631e-06, 'epoch': 0.71}\n",
            "100%|██████████| 109/109 [24:04<00:00, 13.25s/it]\n",
            "\n",
            "{'loss': 0.2684, 'grad_norm': 0.041475433856248856, 'learning_rate': 3.732963624852275e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2963, 'grad_norm': 0.04022267088294029, 'learning_rate': 3.511000298021098e-06, 'epoch': 0.74}\n",
            "{'loss': 0.3064, 'grad_norm': 0.04331429302692413, 'learning_rate': 3.2944270416114256e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2829, 'grad_norm': 0.03968895971775055, 'learning_rate': 3.0834237516252817e-06, 'epoch': 0.75}\n",
            "{'loss': 0.3044, 'grad_norm': 0.04018973559141159, 'learning_rate': 2.878165697386812e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2852, 'grad_norm': 0.039383452385663986, 'learning_rate': 2.678823375955314e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2927, 'grad_norm': 0.03943788632750511, 'learning_rate': 2.485562370502279e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2698, 'grad_norm': 0.03982218727469444, 'learning_rate': 2.2985432127701945e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2868, 'grad_norm': 0.04245101287961006, 'learning_rate': 2.1179212497272582e-06, 'epoch': 0.8}\n",
            "{'loss': 0.2908, 'grad_norm': 0.039420031011104584, 'learning_rate': 1.9438465145288377e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2891, 'grad_norm': 0.038497623056173325, 'learning_rate': 1.7764636018928249e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2844, 'grad_norm': 0.03872718662023544, 'learning_rate': 1.6159115479924259e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04169194772839546, 'learning_rate': 1.462323714966114e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2825, 'grad_norm': 0.04053223133087158, 'learning_rate': 1.3158276801407432e-06, 'epoch': 0.85}\n",
            "{'loss': 0.284, 'grad_norm': 0.04199393093585968, 'learning_rate': 1.1765451300597574e-06, 'epoch': 0.86}\n",
            "{'loss': 0.3002, 'grad_norm': 0.039502184838056564, 'learning_rate': 1.0445917594046073e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2709, 'grad_norm': 0.040075164288282394, 'learning_rate': 9.200771748932513e-07, 'epoch': 0.87}\n",
            "{'loss': 0.3057, 'grad_norm': 0.03965142369270325, 'learning_rate': 8.031048042356393e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2667, 'grad_norm': 0.037885427474975586, 'learning_rate': 6.937718102217461e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2887, 'grad_norm': 0.03893808275461197, 'learning_rate': 5.921690100135713e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2807, 'grad_norm': 0.0394974909722805, 'learning_rate': 4.983807997080925e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2944, 'grad_norm': 0.040340542793273926, 'learning_rate': 4.124850842338779e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03918294236063957, 'learning_rate': 3.345532126395579e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2945, 'grad_norm': 0.04081978276371956, 'learning_rate': 2.646499188279328e-07, 'epoch': 0.94}\n",
            "{'loss': 0.286, 'grad_norm': 0.039872072637081146, 'learning_rate': 2.028332677849254e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2814, 'grad_norm': 0.03757293149828911, 'learning_rate': 1.49154607348051e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2904, 'grad_norm': 0.040040452033281326, 'learning_rate': 1.0365852555447642e-07, 'epoch': 0.97}\n",
            "{'loss': 0.2885, 'grad_norm': 0.041506361216306686, 'learning_rate': 6.638281360408339e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2731, 'grad_norm': 0.04023900255560875, 'learning_rate': 3.735843446830867e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2763, 'grad_norm': 0.03877480328083038, 'learning_rate': 1.6609497170834154e-08, 'epoch': 0.99}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04942460358142853, 'learning_rate': 4.153236761488266e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 1446.6365, 'train_samples_per_second': 9.613, 'train_steps_per_second': 0.075, 'train_loss': 0.2863097934547914, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1 into base model; saving to ckpt/queries_3000/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1/checkpoint-109 into base model; saving to ckpt/queries_3000/models/model_iter_1/checkpoint-109-merged\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1/checkpoint-90 into base model; saving to ckpt/queries_3000/models/model_iter_1/checkpoint-90-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mfrosty-breeze-5\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_084630-shgyhjje/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/queries_3000/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/queries_3000\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 14191 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/queries_3000/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/queries_3000/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/queries_3000/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_3000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_3000/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "ekoe-AGTRjw6"
      },
      "id": "ekoe-AGTRjw6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_3000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_3000/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "CC1OrtejSNGK"
      },
      "id": "CC1OrtejSNGK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease queries (1000)"
      ],
      "metadata": {
        "id": "lM1N-KByMSzQ"
      },
      "id": "lM1N-KByMSzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --n_queries 1000 --run_name queries_1000 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW9LAzaaMWaH",
        "outputId": "5229f6d0-34d3-48de-c06d-123a8218b9be"
      },
      "id": "vW9LAzaaMWaH",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: queries_1000\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/queries_1000\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 1000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/queries_1000/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 09:33:20.536588: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:33:20.554794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199200.575990   49383 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199200.582448   49383 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199200.599023   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599049   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599051   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599052   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:33:20.603849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 09:33:28 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 09:33:30 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 09:33:45 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 09:33:45 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 09:33:47 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 09:33:48 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 09:33:52.516880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199232.538171   49606 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199232.544641   49606 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199232.560380   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560406   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560408   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560409   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 09:33:59 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:01 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:01 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 09:34:03.774311606 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:03 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m WARNING 11-15 09:34:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:03 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:05 [default_loader.py:268] Loading weights took 0.88 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:06 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.084327 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:13 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:13 [backends.py:550] Dynamo bytecode transform time: 7.51 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.740 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:18 [monitor.py:34] torch.compile takes 7.51 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 23.94it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.48 seconds\n",
            "INFO 11-15 09:34:24 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 09:34:24 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1000/1000 [00:01<00:00, 971.33it/s]\n",
            "Processed prompts: 100%|██████████| 8000/8000 [02:10<00:00, 61.32it/s, est. speed input: 5303.64 toks/s, output: 15130.49 toks/s]\n",
            "[rank0]:[W1115 09:36:36.534600751 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_1000/iteration_0/inference.jsonl\n",
            "Loaded 8000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 8000\n",
            "Correct answers: 4697\n",
            "Overall Accuracy: 58.71%\n",
            "\n",
            "Detected multiple rollouts: 1000 unique questions with 8000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5900          0.5900         \n",
            "2     0.5945          0.7240         \n",
            "3     0.5963          0.7780         \n",
            "4     0.5952          0.8140         \n",
            "5     0.5902          0.8330         \n",
            "6     0.5950          0.8570         \n",
            "7     0.5907          0.8670         \n",
            "8     0.5871          0.8720         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/queries_1000/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "Filtered 4697 correct examples out of 8000 total examples\n",
            "Accuracy: 58.71%\n",
            "Saved to ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 4697\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/queries_1000/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 09:36:45.081801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:36:45.099091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199405.120200   50499 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199405.126628   50499 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199405.142873   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142897   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142899   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142901   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:36:45.147631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['k_proj', 'down_proj', 'o_proj', 'q_proj', 'up_proj', 'gate_proj', 'v_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 4697 examples from ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 4697/4697 [00:09<00:00, 482.68it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run rnldl98b\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_093722-rnldl98b\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run stellar-salad-7\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/rnldl98b\n",
            "Saved preprocessed features to ckpt/queries_1000/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 4603, #eval 94\n",
            "100%|██████████| 36/36 [07:37<00:00, 12.70s/it]\n",
            "{'loss': 0.2839, 'grad_norm': 0.04044223576784134, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2929, 'grad_norm': 0.03989022597670555, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2843, 'grad_norm': 0.03935060650110245, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2888, 'grad_norm': 0.04335852712392807, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2865, 'grad_norm': 0.04102490469813347, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2828, 'grad_norm': 0.039688486605882645, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.17}\n",
            "{'loss': 0.283, 'grad_norm': 0.04040113463997841, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2859, 'grad_norm': 0.040744468569755554, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2805, 'grad_norm': 0.04010471701622009, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2747, 'grad_norm': 0.03895456716418266, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3227, 'grad_norm': 0.04341733828186989, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2929, 'grad_norm': 0.03923780471086502, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2929, 'grad_norm': 0.04196038097143173, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2993, 'grad_norm': 0.04025633633136749, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.39}\n",
            "{'loss': 0.3014, 'grad_norm': 0.04030328616499901, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2899, 'grad_norm': 0.04067549854516983, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2954, 'grad_norm': 0.03969379514455795, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2856, 'grad_norm': 0.040728725492954254, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2744, 'grad_norm': 0.04190267622470856, 'learning_rate': 1e-05, 'epoch': 0.53}\n",
            "{'loss': 0.3124, 'grad_norm': 0.04055052250623703, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2933, 'grad_norm': 0.03988247737288475, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.58}\n",
            "{'loss': 0.3022, 'grad_norm': 0.039171647280454636, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2908, 'grad_norm': 0.037890754640102386, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2784, 'grad_norm': 0.04115995392203331, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2788, 'grad_norm': 0.04423469677567482, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2935, 'grad_norm': 0.04607812315225601, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2928, 'grad_norm': 0.04000306874513626, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2909, 'grad_norm': 0.040131762623786926, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2704, 'grad_norm': 0.042681984603405, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2897, 'grad_norm': 0.04200751706957817, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2805, 'grad_norm': 0.04028240218758583, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2791, 'grad_norm': 0.03913095220923424, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2937, 'grad_norm': 0.041734568774700165, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2865, 'grad_norm': 0.043033815920352936, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2786, 'grad_norm': 0.041809841990470886, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.97}\n",
            "{'loss': 0.3009, 'grad_norm': 0.03993520140647888, 'learning_rate': 3.805301908254455e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 459.3239, 'train_samples_per_second': 10.021, 'train_steps_per_second': 0.078, 'train_loss': 0.2891683735781246, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1 into base model; saving to ckpt/queries_1000/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1/checkpoint-30 into base model; saving to ckpt/queries_1000/models/model_iter_1/checkpoint-30-merged\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1/checkpoint-36 into base model; saving to ckpt/queries_1000/models/model_iter_1/checkpoint-36-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-salad-7\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_093722-rnldl98b/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/queries_1000/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/queries_1000\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 4697 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/queries_1000/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/queries_1000/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/queries_1000/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_1000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_1000/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "Gnb-oRv9RoBl"
      },
      "id": "Gnb-oRv9RoBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_1000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_1000/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "V30vRcT4SL_Y"
      },
      "id": "V30vRcT4SL_Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase lr (4e-5)"
      ],
      "metadata": {
        "id": "CMSprCe0MaAm"
      },
      "id": "CMSprCe0MaAm"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --learning_rate 4e-5 --run_name \"lr_4e-5\" --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5dD8PT2Nj4W",
        "outputId": "04279608-05ae-48b3-fcb7-90a7a951beae"
      },
      "id": "e5dD8PT2Nj4W",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: lr_4e-5\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/lr_4e-5\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 4e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/lr_4e-5/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 13:29:18.218469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:29:18.235655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213358.257456    1525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213358.264010    1525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213358.280685    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280718    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280720    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280722    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:29:18.285680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 13:29:26 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 13:29:35 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 13:29:54 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 13:29:54 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 13:29:56 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 13:29:57 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 13:30:02.101225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213402.123911    1815 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213402.130545    1815 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213402.147818    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147849    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147851    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147853    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 13:30:09 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:10 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:11 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 13:30:13.761900733 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:13 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m WARNING 11-15 13:30:13 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:13 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:14 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:14 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.35s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.35s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:37 [default_loader.py:268] Loading weights took 23.37 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:38 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 23.713307 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:47 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:47 [backends.py:550] Dynamo bytecode transform time: 8.43 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:52 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:18 [backends.py:215] Compiling a graph for dynamic shape takes 30.76 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:20 [monitor.py:34] torch.compile takes 39.19 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:21 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:22 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:22 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.60it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [core.py:218] init engine (profile, create kv cache, warmup model) took 48.13 seconds\n",
            "INFO 11-15 13:31:27 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 13:31:27 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:01<00:00, 1120.48it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:30<00:00, 59.14it/s, est. speed input: 5119.56 toks/s, output: 14691.55 toks/s]\n",
            "[rank0]:[W1115 13:36:00.037337397 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_4e-5/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9441\n",
            "Overall Accuracy: 59.01%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.6030          0.6030         \n",
            "2     0.5970          0.7340         \n",
            "3     0.5972          0.7915         \n",
            "4     0.5972          0.8235         \n",
            "5     0.5945          0.8445         \n",
            "6     0.5936          0.8585         \n",
            "7     0.5929          0.8680         \n",
            "8     0.5901          0.8730         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/lr_4e-5/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "Filtered 9441 correct examples out of 16000 total examples\n",
            "Accuracy: 59.01%\n",
            "Saved to ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9441\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/lr_4e-5/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 13:36:14.975068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:36:14.993175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213775.014907    4054 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213775.021606    4054 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213775.038285    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038313    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038315    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038318    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:36:15.043281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['gate_proj', 'up_proj', 'v_proj', 'o_proj', 'k_proj', 'q_proj', 'down_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9441 examples from ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9441/9441 [00:20<00:00, 464.46it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_133710-r4c7xt12\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run icy-glade-8\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/r4c7xt12\n",
            "Saved preprocessed features to ckpt/lr_4e-5/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9252, #eval 189\n",
            "100%|██████████| 73/73 [15:35<00:00, 12.82s/it]\n",
            "{'loss': 0.2947, 'grad_norm': 0.04046342521905899, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2783, 'grad_norm': 0.04024248197674751, 'learning_rate': 3.9981482302044604e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2917, 'grad_norm': 0.04054071009159088, 'learning_rate': 3.992596349869216e-05, 'epoch': 0.04}\n",
            "{'loss': 0.3012, 'grad_norm': 0.04290148988366127, 'learning_rate': 3.98335463979858e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2806, 'grad_norm': 0.03945600241422653, 'learning_rate': 3.9704402135121214e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2945, 'grad_norm': 0.04307609796524048, 'learning_rate': 3.953876985554364e-05, 'epoch': 0.08}\n",
            "{'loss': 0.3165, 'grad_norm': 0.043376438319683075, 'learning_rate': 3.933695627210555e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2904, 'grad_norm': 0.04156087338924408, 'learning_rate': 3.909933509710511e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2939, 'grad_norm': 0.04310508444905281, 'learning_rate': 3.8826346350256943e-05, 'epoch': 0.12}\n",
            "{'loss': 0.2981, 'grad_norm': 0.03999540954828262, 'learning_rate': 3.8518495543877e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2854, 'grad_norm': 0.040826570242643356, 'learning_rate': 3.817635274679006e-05, 'epoch': 0.15}\n",
            "{'loss': 0.304, 'grad_norm': 0.04155397787690163, 'learning_rate': 3.780055152869354e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2788, 'grad_norm': 0.03806138038635254, 'learning_rate': 3.739178778693222e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2876, 'grad_norm': 0.043116651475429535, 'learning_rate': 3.695081845785663e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2958, 'grad_norm': 0.03857306018471718, 'learning_rate': 3.6478460115151084e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3102, 'grad_norm': 0.04275401309132576, 'learning_rate': 3.59755874577273e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2956, 'grad_norm': 0.040434326976537704, 'learning_rate': 3.5443131689983285e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2889, 'grad_norm': 0.0412430614233017, 'learning_rate': 3.488207879742722e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2921, 'grad_norm': 0.038284193724393845, 'learning_rate': 3.429346772085923e-05, 'epoch': 0.26}\n",
            "{'loss': 0.291, 'grad_norm': 0.03850811347365379, 'learning_rate': 3.367838843249222e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2798, 'grad_norm': 0.03965120017528534, 'learning_rate': 3.303797991757425e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2911, 'grad_norm': 0.041029468178749084, 'learning_rate': 3.237342806525007e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2715, 'grad_norm': 0.0392947718501091, 'learning_rate': 3.168596347256737e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2745, 'grad_norm': 0.04070858284831047, 'learning_rate': 3.097685916569439e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2895, 'grad_norm': 0.041382696479558945, 'learning_rate': 3.024742824256848e-05, 'epoch': 0.35}\n",
            "{'loss': 0.2771, 'grad_norm': 0.038860443979501724, 'learning_rate': 2.9499021441341012e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2844, 'grad_norm': 0.03811751306056976, 'learning_rate': 2.8733024639121283e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2861, 'grad_norm': 0.040622446686029434, 'learning_rate': 2.7950856285651124e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2768, 'grad_norm': 0.03956688940525055, 'learning_rate': 2.7153964776662517e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2932, 'grad_norm': 0.039295095950365067, 'learning_rate': 2.6343825771782125e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2963, 'grad_norm': 0.03799743205308914, 'learning_rate': 2.5521939461949384e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2545, 'grad_norm': 0.03534664213657379, 'learning_rate': 2.4689827791408198e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04099860042333603, 'learning_rate': 2.38490316394166e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2856, 'grad_norm': 0.03930551931262016, 'learning_rate': 2.3001107966893054e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2884, 'grad_norm': 0.038180407136678696, 'learning_rate': 2.2147626933283265e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03729045018553734, 'learning_rate': 2.1290168988986332e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2796, 'grad_norm': 0.036218106746673584, 'learning_rate': 2.0430321948724447e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2815, 'grad_norm': 0.03836500644683838, 'learning_rate': 1.956967805127556e-05, 'epoch': 0.53}\n",
            "{'loss': 0.2722, 'grad_norm': 0.03780762106180191, 'learning_rate': 1.8709831011013678e-05, 'epoch': 0.54}\n",
            "{'loss': 0.2857, 'grad_norm': 0.03769194707274437, 'learning_rate': 1.785237306671674e-05, 'epoch': 0.55}\n",
            "{'loss': 0.2945, 'grad_norm': 0.03590088710188866, 'learning_rate': 1.699889203310695e-05, 'epoch': 0.57}\n",
            "{'loss': 0.2839, 'grad_norm': 0.038144227117300034, 'learning_rate': 1.6150968360583404e-05, 'epoch': 0.58}\n",
            "{'loss': 0.2821, 'grad_norm': 0.038355764001607895, 'learning_rate': 1.531017220859181e-05, 'epoch': 0.59}\n",
            "{'loss': 0.2789, 'grad_norm': 0.03852032124996185, 'learning_rate': 1.4478060538050622e-05, 'epoch': 0.61}\n",
            "{'loss': 0.2825, 'grad_norm': 0.03709693253040314, 'learning_rate': 1.3656174228217883e-05, 'epoch': 0.62}\n",
            "{'loss': 0.2839, 'grad_norm': 0.03669588267803192, 'learning_rate': 1.284603522333749e-05, 'epoch': 0.64}\n",
            "{'loss': 0.2961, 'grad_norm': 0.03711657226085663, 'learning_rate': 1.2049143714348886e-05, 'epoch': 0.65}\n",
            "{'loss': 0.2926, 'grad_norm': 0.037974730134010315, 'learning_rate': 1.1266975360878723e-05, 'epoch': 0.66}\n",
            "{'loss': 0.2823, 'grad_norm': 0.038249656558036804, 'learning_rate': 1.0500978558659001e-05, 'epoch': 0.68}\n",
            "{'loss': 0.2799, 'grad_norm': 0.03641022369265556, 'learning_rate': 9.752571757431528e-06, 'epoch': 0.69}\n",
            "{'loss': 0.3002, 'grad_norm': 0.03839483857154846, 'learning_rate': 9.023140834305621e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03784503787755966, 'learning_rate': 8.314036527432631e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2712, 'grad_norm': 0.03670426085591316, 'learning_rate': 7.6265719347499376e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2694, 'grad_norm': 0.03493485972285271, 'learning_rate': 6.962020082425749e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2821, 'grad_norm': 0.03648994490504265, 'learning_rate': 6.321611567507795e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2905, 'grad_norm': 0.037305064499378204, 'learning_rate': 5.706532279140782e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2913, 'grad_norm': 0.037264470010995865, 'learning_rate': 5.1179212025727935e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2873, 'grad_norm': 0.035737309604883194, 'learning_rate': 4.556868310016715e-06, 'epoch': 0.8}\n",
            "{'loss': 0.2575, 'grad_norm': 0.0357542522251606, 'learning_rate': 4.024412542272706e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2907, 'grad_norm': 0.03496134281158447, 'learning_rate': 3.5215398848489167e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2881, 'grad_norm': 0.03901565819978714, 'learning_rate': 3.0491815421433825e-06, 'epoch': 0.84}\n",
            "{'loss': 0.3001, 'grad_norm': 0.03671937435865402, 'learning_rate': 2.60821221306778e-06, 'epoch': 0.86}\n",
            "{'loss': 0.3048, 'grad_norm': 0.03727235645055771, 'learning_rate': 2.199448471306467e-06, 'epoch': 0.87}\n",
            "{'loss': 0.286, 'grad_norm': 0.04000743106007576, 'learning_rate': 1.8236472532099413e-06, 'epoch': 0.89}\n",
            "{'loss': 0.2688, 'grad_norm': 0.03843948617577553, 'learning_rate': 1.481504456123004e-06, 'epoch': 0.9}\n",
            "{'loss': 0.2933, 'grad_norm': 0.03710481524467468, 'learning_rate': 1.1736536497430584e-06, 'epoch': 0.91}\n",
            "{'loss': 0.3056, 'grad_norm': 0.04032738134264946, 'learning_rate': 9.006649028948966e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2638, 'grad_norm': 0.0361216776072979, 'learning_rate': 6.630437278944501e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03626224771142006, 'learning_rate': 4.6123014445636605e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2813, 'grad_norm': 0.034978412091732025, 'learning_rate': 2.9559786487878716e-07, 'epoch': 0.97}\n",
            "{'loss': 0.2726, 'grad_norm': 0.036137793213129044, 'learning_rate': 1.6645360201420046e-07, 'epoch': 0.98}\n",
            "{'loss': 0.2886, 'grad_norm': 0.040458787232637405, 'learning_rate': 7.403650130784368e-08, 'epoch': 1.0}\n",
            "{'loss': 0.2596, 'grad_norm': 0.06696213781833649, 'learning_rate': 1.851769795540026e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 947.7788, 'train_samples_per_second': 9.762, 'train_steps_per_second': 0.077, 'train_loss': 0.28611210029419154, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1 into base model; saving to ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/lr_4e-5/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1/checkpoint-73 into base model; saving to ckpt/lr_4e-5/models/model_iter_1/checkpoint-73-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33micy-glade-8\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_133710-r4c7xt12/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/lr_4e-5\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9441 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/lr_4e-5/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/lr_4e-5/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_4e-5/rollout1\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "O-bVo6-LRqgS"
      },
      "id": "O-bVo6-LRqgS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_4e-5/rollout8\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "W0GQ1QYVSKWp"
      },
      "id": "W0GQ1QYVSKWp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase lr (1e-5)"
      ],
      "metadata": {
        "id": "Mtlkrf5nNycx"
      },
      "id": "Mtlkrf5nNycx"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --learning_rate 1e-5 --run_name \"lr_1e-5\" --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9-qsGO3N29Z",
        "outputId": "c58a02bb-d863-4eb8-80ad-9d44a63d20c1"
      },
      "id": "O9-qsGO3N29Z",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: lr_1e-5\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/lr_1e-5\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 1e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/lr_1e-5/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 13:53:32.948355: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:53:32.966430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763214812.988366    8870 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763214812.994983    8870 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763214813.012181    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012212    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012214    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012216    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:53:33.017155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 13:53:40 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 13:53:42 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 13:53:59 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 13:53:59 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 13:54:02 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 13:54:02 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 13:54:07.246521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763214847.267603    9128 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763214847.273928    9128 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763214847.289686    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289716    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289718    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289720    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 13:54:14 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:16 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:16 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 13:54:18.852469933 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:18 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m WARNING 11-15 13:54:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:18 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:19 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:19 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.09it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.09it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:20 [default_loader.py:268] Loading weights took 0.94 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:21 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.157260 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:29 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:29 [backends.py:550] Dynamo bytecode transform time: 7.63 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.115 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:33 [monitor.py:34] torch.compile takes 7.63 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:34 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:35 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:35 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.85it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.37 seconds\n",
            "INFO 11-15 13:54:40 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 13:54:40 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 798.38it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:32<00:00, 58.81it/s, est. speed input: 5090.41 toks/s, output: 14579.85 toks/s]\n",
            "[rank0]:[W1115 13:59:15.078110102 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_1e-5/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9412\n",
            "Overall Accuracy: 58.83%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5905          0.5905         \n",
            "2     0.5905          0.7275         \n",
            "3     0.5885          0.7845         \n",
            "4     0.5895          0.8145         \n",
            "5     0.5894          0.8390         \n",
            "6     0.5897          0.8540         \n",
            "7     0.5886          0.8675         \n",
            "8     0.5883          0.8780         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/lr_1e-5/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "Filtered 9412 correct examples out of 16000 total examples\n",
            "Accuracy: 58.83%\n",
            "Saved to ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9412\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/lr_1e-5/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 13:59:25.623569: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:59:25.642370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763215165.665067   10674 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763215165.671836   10674 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763215165.688885   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688918   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688920   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688922   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:59:25.694096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['k_proj', 'up_proj', 'gate_proj', 'v_proj', 'q_proj', 'down_proj', 'o_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9412 examples from ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9412/9412 [00:19<00:00, 475.60it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run mqr18hn0\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_140016-mqr18hn0\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run dainty-blaze-9\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/mqr18hn0\n",
            "Saved preprocessed features to ckpt/lr_1e-5/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9223, #eval 189\n",
            "100%|██████████| 73/73 [15:11<00:00, 12.48s/it]\n",
            "{'loss': 0.2697, 'grad_norm': 0.04213186353445053, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2882, 'grad_norm': 0.040083013474941254, 'learning_rate': 9.995370575511151e-06, 'epoch': 0.03}\n",
            "{'loss': 0.2821, 'grad_norm': 0.03980862721800804, 'learning_rate': 9.98149087467304e-06, 'epoch': 0.04}\n",
            "{'loss': 0.2831, 'grad_norm': 0.04026975855231285, 'learning_rate': 9.95838659949645e-06, 'epoch': 0.06}\n",
            "{'loss': 0.2713, 'grad_norm': 0.037654511630535126, 'learning_rate': 9.926100533780304e-06, 'epoch': 0.07}\n",
            "{'loss': 0.2994, 'grad_norm': 0.04277901351451874, 'learning_rate': 9.88469246388591e-06, 'epoch': 0.08}\n",
            "{'loss': 0.2954, 'grad_norm': 0.04307286813855171, 'learning_rate': 9.834239068026388e-06, 'epoch': 0.1}\n",
            "{'loss': 0.2854, 'grad_norm': 0.04598747938871384, 'learning_rate': 9.774833774276278e-06, 'epoch': 0.11}\n",
            "{'loss': 0.2869, 'grad_norm': 0.04217357933521271, 'learning_rate': 9.706586587564236e-06, 'epoch': 0.12}\n",
            "{'loss': 0.2881, 'grad_norm': 0.03806336596608162, 'learning_rate': 9.62962388596925e-06, 'epoch': 0.14}\n",
            "{'loss': 0.2878, 'grad_norm': 0.0416540764272213, 'learning_rate': 9.544088186697515e-06, 'epoch': 0.15}\n",
            "{'loss': 0.2747, 'grad_norm': 0.041370321065187454, 'learning_rate': 9.450137882173385e-06, 'epoch': 0.17}\n",
            "{'loss': 0.283, 'grad_norm': 0.04098854213953018, 'learning_rate': 9.347946946733055e-06, 'epoch': 0.18}\n",
            "{'loss': 0.2878, 'grad_norm': 0.04159264639019966, 'learning_rate': 9.237704614464157e-06, 'epoch': 0.19}\n",
            "{'loss': 0.303, 'grad_norm': 0.04671835899353027, 'learning_rate': 9.119615028787771e-06, 'epoch': 0.21}\n",
            "{'loss': 0.291, 'grad_norm': 0.04431857541203499, 'learning_rate': 8.993896864431825e-06, 'epoch': 0.22}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04016101732850075, 'learning_rate': 8.860782922495821e-06, 'epoch': 0.24}\n",
            "{'loss': 0.275, 'grad_norm': 0.04002957418560982, 'learning_rate': 8.720519699356804e-06, 'epoch': 0.25}\n",
            "{'loss': 0.2851, 'grad_norm': 0.039889223873615265, 'learning_rate': 8.573366930214807e-06, 'epoch': 0.26}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04091254249215126, 'learning_rate': 8.419597108123054e-06, 'epoch': 0.28}\n",
            "{'loss': 0.2915, 'grad_norm': 0.04555299133062363, 'learning_rate': 8.259494979393563e-06, 'epoch': 0.29}\n",
            "{'loss': 0.2868, 'grad_norm': 0.04143838211894035, 'learning_rate': 8.093357016312518e-06, 'epoch': 0.31}\n",
            "{'loss': 0.2949, 'grad_norm': 0.04202131927013397, 'learning_rate': 7.921490868141843e-06, 'epoch': 0.32}\n",
            "{'loss': 0.277, 'grad_norm': 0.046570565551519394, 'learning_rate': 7.744214791423597e-06, 'epoch': 0.33}\n",
            "{'loss': 0.2672, 'grad_norm': 0.04326683655381203, 'learning_rate': 7.56185706064212e-06, 'epoch': 0.35}\n",
            "{'loss': 0.2997, 'grad_norm': 0.041319336742162704, 'learning_rate': 7.374755360335253e-06, 'epoch': 0.36}\n",
            "{'loss': 0.2728, 'grad_norm': 0.03881301358342171, 'learning_rate': 7.183256159780321e-06, 'epoch': 0.37}\n",
            "{'loss': 0.2779, 'grad_norm': 0.04066222533583641, 'learning_rate': 6.987714071412781e-06, 'epoch': 0.39}\n",
            "{'loss': 0.2748, 'grad_norm': 0.04284864291548729, 'learning_rate': 6.788491194165629e-06, 'epoch': 0.4}\n",
            "{'loss': 0.2824, 'grad_norm': 0.04069862514734268, 'learning_rate': 6.585956442945531e-06, 'epoch': 0.42}\n",
            "{'loss': 0.2651, 'grad_norm': 0.03951702266931534, 'learning_rate': 6.380484865487346e-06, 'epoch': 0.43}\n",
            "{'loss': 0.3054, 'grad_norm': 0.04261735454201698, 'learning_rate': 6.1724569478520495e-06, 'epoch': 0.44}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04011780396103859, 'learning_rate': 5.96225790985415e-06, 'epoch': 0.46}\n",
            "{'loss': 0.2888, 'grad_norm': 0.04307740181684494, 'learning_rate': 5.7502769917232635e-06, 'epoch': 0.47}\n",
            "{'loss': 0.2701, 'grad_norm': 0.04128422960639, 'learning_rate': 5.536906733320816e-06, 'epoch': 0.49}\n",
            "{'loss': 0.2945, 'grad_norm': 0.06433846056461334, 'learning_rate': 5.322542247246583e-06, 'epoch': 0.5}\n",
            "{'loss': 0.3021, 'grad_norm': 0.041216108947992325, 'learning_rate': 5.107580487181112e-06, 'epoch': 0.51}\n",
            "{'loss': 0.2861, 'grad_norm': 0.04107237234711647, 'learning_rate': 4.89241951281889e-06, 'epoch': 0.53}\n",
            "{'loss': 0.304, 'grad_norm': 0.04232661798596382, 'learning_rate': 4.6774577527534195e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2816, 'grad_norm': 0.03968316316604614, 'learning_rate': 4.463093266679185e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2982, 'grad_norm': 0.04115461930632591, 'learning_rate': 4.249723008276737e-06, 'epoch': 0.57}\n",
            "{'loss': 0.29, 'grad_norm': 0.044513799250125885, 'learning_rate': 4.037742090145851e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04048357158899307, 'learning_rate': 3.827543052147952e-06, 'epoch': 0.6}\n",
            "{'loss': 0.286, 'grad_norm': 0.04381868615746498, 'learning_rate': 3.6195151345126556e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2672, 'grad_norm': 0.0423961877822876, 'learning_rate': 3.4140435570544708e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2935, 'grad_norm': 0.040342457592487335, 'learning_rate': 3.2115088058343725e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2939, 'grad_norm': 0.0420951321721077, 'learning_rate': 3.0122859285872214e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2958, 'grad_norm': 0.03989747539162636, 'learning_rate': 2.816743840219681e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2883, 'grad_norm': 0.04243665933609009, 'learning_rate': 2.6252446396647503e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2709, 'grad_norm': 0.042038850486278534, 'learning_rate': 2.438142939357882e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2908, 'grad_norm': 0.041530542075634, 'learning_rate': 2.2557852085764053e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2851, 'grad_norm': 0.04001827538013458, 'learning_rate': 2.0785091318581577e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2922, 'grad_norm': 0.04072616621851921, 'learning_rate': 1.9066429836874844e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2755, 'grad_norm': 0.0405224934220314, 'learning_rate': 1.7405050206064372e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2923, 'grad_norm': 0.04025265574455261, 'learning_rate': 1.5804028918769488e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2813, 'grad_norm': 0.04130794107913971, 'learning_rate': 1.4266330697851955e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2802, 'grad_norm': 0.040605466812849045, 'learning_rate': 1.2794803006431984e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2975, 'grad_norm': 0.04052789509296417, 'learning_rate': 1.1392170775041788e-06, 'epoch': 0.8}\n",
            "{'loss': 0.3015, 'grad_norm': 0.041520487517118454, 'learning_rate': 1.0061031355681766e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2772, 'grad_norm': 0.03887774795293808, 'learning_rate': 8.803849712122292e-07, 'epoch': 0.83}\n",
            "{'loss': 0.2771, 'grad_norm': 0.039884015917778015, 'learning_rate': 7.622953855358456e-07, 'epoch': 0.85}\n",
            "{'loss': 0.2947, 'grad_norm': 0.042108204215765, 'learning_rate': 6.52053053266945e-07, 'epoch': 0.86}\n",
            "{'loss': 0.2706, 'grad_norm': 0.04087476804852486, 'learning_rate': 5.498621178266167e-07, 'epoch': 0.87}\n",
            "{'loss': 0.3018, 'grad_norm': 0.043477654457092285, 'learning_rate': 4.5591181330248534e-07, 'epoch': 0.89}\n",
            "{'loss': 0.287, 'grad_norm': 0.040444277226924896, 'learning_rate': 3.70376114030751e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2801, 'grad_norm': 0.04282611608505249, 'learning_rate': 2.934134124357646e-07, 'epoch': 0.92}\n",
            "{'loss': 0.3238, 'grad_norm': 0.041937582194805145, 'learning_rate': 2.2516622572372416e-07, 'epoch': 0.93}\n",
            "{'loss': 0.292, 'grad_norm': 0.04169221222400665, 'learning_rate': 1.6576093197361253e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2998, 'grad_norm': 0.04154568165540695, 'learning_rate': 1.1530753611409151e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04479105770587921, 'learning_rate': 7.389946621969679e-08, 'epoch': 0.97}\n",
            "{'loss': 0.2918, 'grad_norm': 0.040006961673498154, 'learning_rate': 4.1613400503550114e-08, 'epoch': 0.99}\n",
            "{'loss': 0.3057, 'grad_norm': 0.04510121047496796, 'learning_rate': 1.850912532696092e-08, 'epoch': 1.0}\n",
            "{'loss': 0.3602, 'grad_norm': 0.18706075847148895, 'learning_rate': 4.629424488850065e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 913.3499, 'train_samples_per_second': 10.098, 'train_steps_per_second': 0.08, 'train_loss': 0.2881258701624936, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1 into base model; saving to ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/lr_1e-5/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1/checkpoint-73 into base model; saving to ckpt/lr_1e-5/models/model_iter_1/checkpoint-73-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mdainty-blaze-9\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_140016-mqr18hn0/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/lr_1e-5\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9412 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/lr_1e-5/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/lr_1e-5/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_1e-5/rollout1\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "ib03Ntu7RyQI"
      },
      "id": "ib03Ntu7RyQI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_1e-5/rollout8\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "7cKISfoiSEsm"
      },
      "id": "7cKISfoiSEsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase LoRA rank (128)"
      ],
      "metadata": {
        "id": "_lldD-rLOM9F"
      },
      "id": "_lldD-rLOM9F"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --lora_r 128 --run_name rank_128 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYmbAiCRORl0",
        "outputId": "9426c7b0-4217-4f2a-be61-741d0b794702"
      },
      "id": "rYmbAiCRORl0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: rank_128\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/rank_128\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 128\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/rank_128/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 14:16:02.426188: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 14:16:02.445239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763216162.466617   15251 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763216162.473108   15251 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763216162.490062   15251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216162.490087   15251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216162.490089   15251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216162.490090   15251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 14:16:02.495059: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 14:16:09 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 14:16:11 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 14:16:27 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 14:16:27 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 14:16:30 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 14:16:31 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 14:16:35.722524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763216195.751212   15499 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763216195.760180   15499 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763216195.783373   15499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216195.783410   15499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216195.783414   15499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216195.783416   15499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 14:16:43 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:44 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:44 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 14:16:47.564372990 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:47 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m WARNING 11-15 14:16:47 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:47 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:47 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:47 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:48 [default_loader.py:268] Loading weights took 0.89 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:49 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.094239 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:57 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:16:57 [backends.py:550] Dynamo bytecode transform time: 7.43 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:00 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.835 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:01 [monitor.py:34] torch.compile takes 7.43 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:02 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:03 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:03 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.60it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:07 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:07 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=15499)\u001b[0;0m INFO 11-15 14:17:07 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.98 seconds\n",
            "INFO 11-15 14:17:08 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 14:17:08 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 879.64it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:35<00:00, 58.08it/s, est. speed input: 5027.75 toks/s, output: 14380.39 toks/s]\n",
            "[rank0]:[W1115 14:21:47.495159679 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_128/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9334\n",
            "Overall Accuracy: 58.34%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5840          0.5840         \n",
            "2     0.5850          0.7210         \n",
            "3     0.5810          0.7815         \n",
            "4     0.5824          0.8065         \n",
            "5     0.5834          0.8345         \n",
            "6     0.5833          0.8520         \n",
            "7     0.5826          0.8590         \n",
            "8     0.5834          0.8685         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/rank_128/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "Filtered 9334 correct examples out of 16000 total examples\n",
            "Accuracy: 58.34%\n",
            "Saved to ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9334\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/rank_128/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 14:21:57.242060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 14:21:57.261090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763216517.283839   16992 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763216517.290697   16992 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763216517.308211   16992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216517.308245   16992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216517.308248   16992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763216517.308249   16992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 14:21:57.313307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9334 examples from ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9334/9334 [00:19<00:00, 477.12it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run gi90cpix\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_142252-gi90cpix\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run stilted-pine-10\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/gi90cpix\n",
            "Saved preprocessed features to ckpt/rank_128/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9147, #eval 187\n",
            " 14%|█▍        | 10/72 [02:12<13:39, 13.22s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_128/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "FOtwG7IbR1ek"
      },
      "id": "FOtwG7IbR1ek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_128/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "rjh0rzoZSD3l"
      },
      "id": "rjh0rzoZSD3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease LoRA rank (32)"
      ],
      "metadata": {
        "id": "8mCBekXZOhxf"
      },
      "id": "8mCBekXZOhxf"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --lora_r 32 --run_name rank_32 --batch_size_per_dev 4"
      ],
      "metadata": {
        "id": "iflfrd2GOkdW"
      },
      "id": "iflfrd2GOkdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_32/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_32/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "pCP5kCyaR_or"
      },
      "id": "pCP5kCyaR_or",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_32/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_32/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "qjiwnnfJSDM3"
      },
      "id": "qjiwnnfJSDM3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}