{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d3b43e0",
      "metadata": {
        "id": "8d3b43e0"
      },
      "source": [
        "###  Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45c19e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45c19e77",
        "outputId": "bd32f375-0d9e-4bad-8f33-4e3af54a929b",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420dc8b7",
      "metadata": {
        "id": "420dc8b7"
      },
      "source": [
        "### Clone Codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20bdbde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20bdbde",
        "outputId": "d69184eb-b0e9-4087-8326-0ac8d98a1bae",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMP4901B-LLMs'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 232 (delta 40), reused 24 (delta 24), pack-reused 169 (from 1)\u001b[K\n",
            "Receiving objects: 100% (232/232), 798.55 KiB | 1.87 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ],
      "source": [
        "! mkdir -p /content/drive/MyDrive/COMP4901B-Homework3\n",
        "! cd /content/drive/MyDrive/COMP4901B-Homework3 && git clone https://github.com/hkust-nlp/COMP4901B-LLMs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c6cfe0",
      "metadata": {
        "id": "29c6cfe0"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5510016",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5510016",
        "outputId": "42ce89d3-9be8-4cb4-c2db-63939faeba73",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (1.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]==4.57.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]==4.57.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]==4.57.1) (3.0.3)\n",
            "Collecting vllm==0.10.2\n",
            "  Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.67.1)\n",
            "Collecting blake3 (from vllm==0.10.2)\n",
            "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.121.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (11.3.0)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.12.0)\n",
            "Collecting lm-format-enforcer==0.11.3 (from vllm==0.10.2)\n",
            "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.2)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.11 (from vllm==0.10.2)\n",
            "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm==0.10.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm==0.10.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.23 (from vllm==0.10.2)\n",
            "  Downloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.20.0)\n",
            "Collecting partial-json-parser (from vllm==0.10.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (26.2.1)\n",
            "Collecting msgspec (from vllm==0.10.2)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm==0.10.2)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.17.0)\n",
            "Collecting setuptools<80,>=77.0.3 (from vllm==0.10.2)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.8.1)\n",
            "Collecting compressed-tensors==0.11.0 (from vllm==0.10.2)\n",
            "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm==0.10.2)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (3.1.2)\n",
            "Collecting watchfiles (from vllm==0.10.2)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (1.16.3)\n",
            "Collecting ninja (from vllm==0.10.2)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm==0.10.2)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm==0.10.2)\n",
            "  Downloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting setproctitle (from vllm==0.10.2)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Collecting openai-harmony>=0.0.3 (from vllm==0.10.2)\n",
            "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting numba==0.61.2 (from vllm==0.10.2)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.10.2) (0.23.0+cu126)\n",
            "Collecting xformers==0.0.32.post1 (from vllm==0.10.2)\n",
            "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm==0.10.2) (2.4.6)\n",
            "Collecting astor (from depyf==0.19.0->vllm==0.10.2)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm==0.10.2) (0.3.8)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm==0.10.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm==0.10.2) (25.0)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.2)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm==0.10.2) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.49.3)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.4)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cli-0.0.16-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.10.2) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (4.25.1)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm==0.10.2) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm==0.10.2) (0.4.2)\n",
            "Collecting click!=8.3.0,>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.2) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm==0.10.2) (13.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.10.2) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm==0.10.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm==0.10.2) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.10.2) (1.22.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.20.0)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.2) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm==0.10.2) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.28.0)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm==0.10.2) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.2) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (1.0.0)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2)\n",
            "  Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.44.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.2) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.2) (0.1.2)\n",
            "Downloading vllm-0.10.2-cp38-abi3-manylinux1_x86_64.whl (436.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.11.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.23-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl (71.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading fastapi_cli-0.0.16-py3-none-any.whl (12 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, click, cbor2, blake3, astor, watchfiles, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, lm-format-enforcer, xformers, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: lark\n",
            "    Found existing installation: lark 1.3.1\n",
            "    Uninstalling lark-1.3.1:\n",
            "      Successfully uninstalled lark-1.3.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 blake3-1.0.8 cbor2-5.7.1 click-8.3.1 compressed-tensors-0.11.0 depyf-0.19.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.16 fastapi-cloud-cli-0.3.1 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.11.3 mistral_common-1.8.5 msgspec-0.19.0 ninja-1.13.0 numba-0.61.2 openai-harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.51.1 rich-toolkit-0.15.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-79.0.1 uvloop-0.22.1 vllm-0.10.2 watchfiles-1.1.1 xformers-0.0.32.post1 xgrammar-0.1.23\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763447 sha256=1ac48edebfe9cbed2024e3157b1b5b67477fa8a4e6cf92f1f50f4fe68facd6d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/ad/2e/e03d4739ddc0417efd8a120c2b9e784005aa226037e558c163\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: deepspeed\n",
            "Successfully installed deepspeed-0.18.2\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hjson\n",
            "Successfully installed hjson-3.1.0\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (4.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=c67fb69428b8f4860dffb1ad53376f6f5c45c2d0a76192564c43647347cb344e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (79.0.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
            "Fetching 10 files: 100% 10/10 [00:03<00:00,  2.76it/s]\n",
            "Fetching 6 files: 100% 6/6 [00:03<00:00,  1.72it/s]\n"
          ]
        }
      ],
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/setup.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1 code evaulation for q1"
      ],
      "metadata": {
        "id": "1g5V_geSlmBY"
      },
      "id": "1g5V_geSlmBY"
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/ && python tests/test_format_prompts.py"
      ],
      "metadata": {
        "id": "MmryTBAWlrtb",
        "outputId": "ef7f48ea-131f-4f00-bc11-ccbd8f8d76a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "MmryTBAWlrtb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SANITY CHECK: format_prompts()\n",
            "======================================================================\n",
            "\n",
            "This script tests your implementation of the format_prompts function.\n",
            "It compares your outputs with expected reference outputs.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TEST 2: Few-shot without chat template\n",
            "======================================================================\n",
            "2025-11-13 09:43:54.067356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763027034.487824    6130 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763027034.594252    6130 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763027035.283583    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283628    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283633    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763027035.283637    6130 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-13 09:43:55.353037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-13 09:44:17 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform\n",
            "WARNING 11-13 09:44:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
            "✅ PASSED: Few-shot prompt contains example problems\n",
            "\n",
            "======================================================================\n",
            "TEST 3: With chat template (using Qwen tokenizer)\n",
            "======================================================================\n",
            "Loading tokenizer: Qwen/Qwen2.5-0.5B-Instruct\n",
            "tokenizer_config.json: 7.30kB [00:00, 9.93MB/s]\n",
            "vocab.json: 2.78MB [00:00, 27.6MB/s]\n",
            "merges.txt: 1.67MB [00:00, 30.8MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 55.6MB/s]\n",
            "✅ PASSED: Chat template markers found in prompt\n",
            "\n",
            "Formatted prompt preview:\n",
            "<|im_start|>system\n",
            "You are a helpful math tutor.<|im_end|>\n",
            "<|im_start|>user\n",
            "If John has 5 apples and gives 2 to Mary, how many apples does he have left?\n",
            "Please reason step by step, and put your final ...\n",
            "\n",
            "======================================================================\n",
            "TEST 4: System message inclusion\n",
            "======================================================================\n",
            "✅ PASSED: System message appears to be included\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "✅ PASSED: Few-shot without chat\n",
            "✅ PASSED: With chat template\n",
            "✅ PASSED: System message\n",
            "\n",
            "Total: 3/3 passed\n",
            "\n",
            "🎉 All tests passed! Your format_prompts implementation looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe18aa7",
      "metadata": {
        "id": "dfe18aa7"
      },
      "source": [
        "### Q4 Start The Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Default"
      ],
      "metadata": {
        "id": "cwGpX4aNIjXF"
      },
      "id": "cwGpX4aNIjXF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f6abed",
      "metadata": {
        "id": "76f6abed",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68305762-e469-4d37-a6fa-d8218f85deae",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: first-run\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/first-run\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 1\n",
            "  Gradient Accumulation Steps: 128 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/first-run/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-14 11:42:24.446372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 11:42:24.463881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763120544.485531    3843 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763120544.492055    3843 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763120544.508552    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508578    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508580    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120544.508582    3843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 11:42:24.513471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-14 11:42:33 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-14 11:42:46 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-14 11:43:04 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-14 11:43:04 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-14 11:43:06 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-14 11:43:07 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-14 11:43:12.073903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763120592.095164    4156 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763120592.101696    4156 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763120592.117735    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117761    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117764    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763120592.117766    4156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-14 11:43:19 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:20 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:20 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1114 11:43:23.497668660 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m WARNING 11-14 11:43:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:23 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:21<00:00, 21.86s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:21<00:00, 21.86s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:45 [default_loader.py:268] Loading weights took 21.88 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:46 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 22.256874 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:55 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/44c42810b9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:43:55 [backends.py:550] Dynamo bytecode transform time: 8.30 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m [rank0]:W1114 11:43:56.675000 4156 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:01 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:28 [backends.py:215] Compiling a graph for dynamic shape takes 32.38 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:31 [monitor.py:34] torch.compile takes 40.68 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:33 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 22.01it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15646582988` to fit into requested memory, or `--kv-cache-memory=20175461376` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=4156)\u001b[0;0m INFO 11-14 11:44:37 [core.py:218] init engine (profile, create kv cache, warmup model) took 51.39 seconds\n",
            "INFO 11-14 11:44:39 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-14 11:44:39 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 893.40it/s] \n",
            "Processed prompts: 100%|██████████| 16000/16000 [13:16<00:00, 20.10it/s, est. speed input: 1739.72 toks/s, output: 4991.45 toks/s]\n",
            "[rank0]:[W1114 11:57:59.326135498 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/first-run/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9338\n",
            "Overall Accuracy: 58.36%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5805          0.5805         \n",
            "2     0.5755          0.7065         \n",
            "3     0.5782          0.7790         \n",
            "4     0.5787          0.8065         \n",
            "5     0.5809          0.8285         \n",
            "6     0.5820          0.8460         \n",
            "7     0.5822          0.8600         \n",
            "8     0.5836          0.8755         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/first-run/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/first-run/iteration_0/evaluation.jsonl\n",
            "Filtered 9338 correct examples out of 16000 total examples\n",
            "Accuracy: 58.36%\n",
            "Saved to ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9338\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/first-run/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-14 11:58:16.249834: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-14 11:58:16.268720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763121496.290789    8657 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763121496.297503    8657 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763121496.314997    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315026    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315028    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763121496.315030    8657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-14 11:58:16.320411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['q_proj', 'down_proj', 'k_proj', 'up_proj', 'gate_proj', 'o_proj', 'v_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9338 examples from ckpt/first-run/iteration_0/correct_examples.jsonl\n",
            "Cache metadata mismatch for key 'data_mtime': expected 1763121490.0, found 1763102020.0. Regenerating.\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9338/9338 [00:19<00:00, 481.04it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251114_115913-q55p38u7\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run brisk-forest-2\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/q55p38u7\n",
            "Saved preprocessed features to ckpt/first-run/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9151, #eval 187\n",
            "100%|██████████| 72/72 [1:01:24<00:00, 51.17s/it]\n",
            "{'loss': 0.2842, 'grad_norm': 0.0412321574985981, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2738, 'grad_norm': 0.040876783430576324, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2756, 'grad_norm': 0.039690688252449036, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2788, 'grad_norm': 0.042680516839027405, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2809, 'grad_norm': 0.04258733242750168, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.07}\n",
            "{'loss': 0.3023, 'grad_norm': 0.040956296026706696, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2837, 'grad_norm': 0.046012185513973236, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04344983398914337, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.11}\n",
            "{'loss': 0.3043, 'grad_norm': 0.04215284064412117, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2864, 'grad_norm': 0.041301827877759933, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.14}\n",
            "{'loss': 0.3106, 'grad_norm': 0.0407261960208416, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2848, 'grad_norm': 0.040137290954589844, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3131, 'grad_norm': 0.0441724956035614, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.18}\n",
            "{'loss': 0.3004, 'grad_norm': 0.04114912077784538, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.2}\n",
            "{'loss': 0.3012, 'grad_norm': 0.041343312710523605, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3138, 'grad_norm': 0.045458462089300156, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2901, 'grad_norm': 0.04221639037132263, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.24}\n",
            "{'loss': 0.3004, 'grad_norm': 0.041205115616321564, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2917, 'grad_norm': 0.04758389666676521, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2836, 'grad_norm': 0.045194391161203384, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3034, 'grad_norm': 0.04089658707380295, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.279, 'grad_norm': 0.041532088071107864, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2823, 'grad_norm': 0.039261385798454285, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2984, 'grad_norm': 0.04346533119678497, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04092961177229881, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3003, 'grad_norm': 0.04204783961176872, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2885, 'grad_norm': 0.04121629521250725, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2815, 'grad_norm': 0.04102322831749916, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03983001038432121, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.41}\n",
            "{'loss': 0.2989, 'grad_norm': 0.03859668970108032, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2837, 'grad_norm': 0.0408283956348896, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2856, 'grad_norm': 0.040170829743146896, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2697, 'grad_norm': 0.03837982937693596, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.46}\n",
            "{'loss': 0.3013, 'grad_norm': 0.04542221501469612, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2877, 'grad_norm': 0.03982899710536003, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2936, 'grad_norm': 0.04119272157549858, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2878, 'grad_norm': 0.042690955102443695, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
            "{'loss': 0.27, 'grad_norm': 0.04045286774635315, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.53}\n",
            "{'loss': 0.2901, 'grad_norm': 0.042437903583049774, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.55}\n",
            "{'loss': 0.2893, 'grad_norm': 0.03962692990899086, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2783, 'grad_norm': 0.041780684143304825, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2988, 'grad_norm': 0.04224047064781189, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.59}\n",
            "{'loss': 0.278, 'grad_norm': 0.040542569011449814, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2957, 'grad_norm': 0.04034178704023361, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2691, 'grad_norm': 0.03883595019578934, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2887, 'grad_norm': 0.03946587070822716, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.64}\n",
            "{'loss': 0.268, 'grad_norm': 0.04042750597000122, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2663, 'grad_norm': 0.04243028908967972, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2815, 'grad_norm': 0.039937954396009445, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2827, 'grad_norm': 0.03981532156467438, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.7}\n",
            "{'loss': 0.272, 'grad_norm': 0.04017646238207817, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2908, 'grad_norm': 0.03881995379924774, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04091988131403923, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2821, 'grad_norm': 0.040974073112010956, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2897, 'grad_norm': 0.04128747805953026, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2919, 'grad_norm': 0.040972184389829636, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2675, 'grad_norm': 0.04944310709834099, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.8}\n",
            "{'loss': 0.272, 'grad_norm': 0.04125469550490379, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2968, 'grad_norm': 0.0401521734893322, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2825, 'grad_norm': 0.040450725704431534, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.84}\n",
            "{'loss': 0.3087, 'grad_norm': 0.04155230149626732, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.85}\n",
            "{'loss': 0.2939, 'grad_norm': 0.03926434740424156, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04088444262742996, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2785, 'grad_norm': 0.0381874181330204, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2843, 'grad_norm': 0.03971501812338829, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2822, 'grad_norm': 0.04046123847365379, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.92}\n",
            "{'loss': 0.282, 'grad_norm': 0.041982244700193405, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.94}\n",
            "{'loss': 0.3026, 'grad_norm': 0.038715165108442307, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2819, 'grad_norm': 0.03806982561945915, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.97}\n",
            "{'loss': 0.283, 'grad_norm': 0.04031049460172653, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2945, 'grad_norm': 0.039586521685123444, 'learning_rate': 3.805301908254455e-08, 'epoch': 0.99}\n",
            "{'loss': 0.272, 'grad_norm': 0.05442313104867935, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 3695.9688, 'train_samples_per_second': 2.476, 'train_steps_per_second': 0.019, 'train_loss': 0.28730813124113613, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1 into base model; saving to ckpt/first-run/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/first-run/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/first-run/models/model_iter_1/checkpoint-72 into base model; saving to ckpt/first-run/models/model_iter_1/checkpoint-72-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mbrisk-forest-2\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251114_115913-q55p38u7/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/first-run/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/first-run\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9338 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/first-run/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/first-run/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/first-run/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/first-run/models/model_iter_1-merged \\\n",
        "    --output_dir results/default/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAGHw1LvQDlw",
        "outputId": "6945cf35-b699-43e6-b4cb-fdc0d5d16417"
      },
      "id": "hAGHw1LvQDlw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/first-run/models/model_iter_1-merged\n",
            "Output Directory: results/default/rollout1\n",
            "Run Name: model_iter_1-merged_20251115_070341\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "\n",
            "2025-11-15 07:03:48.709790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 07:03:48.727304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190228.749049    7645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190228.755619    7645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190228.772163    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772200    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772202    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190228.772203    7645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 07:03:48.777076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 07:03:57 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 07:04:08 [utils.py:328] non-default args: {'tokenizer': 'ckpt/first-run/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/first-run/models/model_iter_1-merged'}\n",
            "INFO 11-15 07:04:27 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 07:04:27 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 07:04:29 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 07:04:30 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 07:04:35.023300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763190275.044564    7941 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763190275.050998    7941 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763190275.066554    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066579    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066582    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763190275.066585    7941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 07:04:42 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:43 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/first-run/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/first-run/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/first-run/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 07:04:46.653598055 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:46 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m WARNING 11-15 07:04:46 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:46 [gpu_model_runner.py:2338] Starting to load model ckpt/first-run/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:47 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:04:47 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.21s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.21s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:06 [default_loader.py:268] Loading weights took 19.24 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:07 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 19.635288 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:16 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/cb7b993d40/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:16 [backends.py:550] Dynamo bytecode transform time: 8.47 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:22 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:48 [backends.py:215] Compiling a graph for dynamic shape takes 31.92 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:50 [monitor.py:34] torch.compile takes 40.39 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:52 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.01it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=7941)\u001b[0;0m INFO 11-15 07:05:57 [core.py:218] init engine (profile, create kv cache, warmup model) took 49.65 seconds\n",
            "INFO 11-15 07:05:58 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 07:05:58 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1727.47it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [00:42<00:00, 30.85it/s, est. speed input: 2724.18 toks/s, output: 8006.32 toks/s]\n",
            "[rank0]:[W1115 07:06:42.374997787 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "\n",
            "Loading data from results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 789\n",
            "Overall Accuracy: 59.82%\n",
            "\n",
            "Results saved to results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/default/rollout1/model_iter_1-merged_20251115_070341_inference.jsonl\n",
            "  - Evaluation results: results/default/rollout1/model_iter_1-merged_20251115_070341_evaluation.jsonl\n",
            "  - Log file: results/default/rollout1/model_iter_1-merged_20251115_070341.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/first-run/models/model_iter_1-merged \\\n",
        "    --output_dir results/default/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38CxfTf28m3o",
        "outputId": "e1ef892a-e256-4e95-a69b-a8bf9baaa31c"
      },
      "id": "38CxfTf28m3o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/first-run/models/model_iter_1-merged\n",
            "Output Directory: results\n",
            "Run Name: model_iter_1-merged_20251115_060728\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "\n",
            "2025-11-15 06:07:32.677374: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 06:07:32.694729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763186852.716271   12172 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763186852.722785   12172 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763186852.739114   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739143   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739145   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186852.739147   12172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 06:07:32.743987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 06:07:39 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 06:07:42 [utils.py:328] non-default args: {'tokenizer': 'ckpt/first-run/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/first-run/models/model_iter_1-merged'}\n",
            "INFO 11-15 06:07:57 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 06:07:57 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 06:07:59 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 06:08:00 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 06:08:04.665665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763186884.687371   12402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763186884.693851   12402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763186884.709465   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709499   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709501   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763186884.709503   12402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 06:08:12 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:13 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:13 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/first-run/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/first-run/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/first-run/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 06:08:15.236307023 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:15 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m WARNING 11-15 06:08:15 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [gpu_model_runner.py:2338] Starting to load model ckpt/first-run/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:16 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.23it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:17 [default_loader.py:268] Loading weights took 0.84 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:18 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.043268 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:26 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ff3d868903/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:26 [backends.py:550] Dynamo bytecode transform time: 7.57 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:29 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.774 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:30 [monitor.py:34] torch.compile takes 7.57 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:31 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:32 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:32 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 22.39it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=12402)\u001b[0;0m INFO 11-15 06:08:36 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.03 seconds\n",
            "INFO 11-15 06:08:37 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 06:08:37 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1148.54it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:36<00:00, 10.59it/s, est. speed input: 935.21 toks/s, output: 2752.92 toks/s]\n",
            "[rank0]:[W1115 06:25:15.745767483 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "\n",
            "Loading data from results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6349\n",
            "Overall Accuracy: 60.17%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5944          0.5944         \n",
            "2     0.5986          0.7119         \n",
            "3     0.6037          0.7650         \n",
            "4     0.6025          0.7885         \n",
            "5     0.6020          0.8074         \n",
            "6     0.6022          0.8271         \n",
            "7     0.6026          0.8370         \n",
            "8     0.6017          0.8423         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/model_iter_1-merged_20251115_060728_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/model_iter_1-merged_20251115_060728_inference.jsonl\n",
            "  - Evaluation results: results/model_iter_1-merged_20251115_060728_evaluation.jsonl\n",
            "  - Log file: results/model_iter_1-merged_20251115_060728.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease batch size (64)"
      ],
      "metadata": {
        "id": "fYxvzs9YKOXj"
      },
      "id": "fYxvzs9YKOXj"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --total_batch_size 64 --run_name batch_64 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fviKo_zjKSA9",
        "outputId": "947d4e58-c6fa-4d83-9737-dc7ac3dd98ec"
      },
      "id": "fviKo_zjKSA9",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: batch_64\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/batch_64\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 64\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 16 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/batch_64/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-17 09:14:53.547755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 09:14:53.565441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763370893.587309   29734 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763370893.593806   29734 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763370893.610405   29734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370893.610442   29734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370893.610444   29734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370893.610446   29734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 09:14:53.615362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 09:15:01 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 09:15:03 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-17 09:15:19 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 09:15:19 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 09:15:21 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 09:15:22 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 09:15:26.620614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763370926.642217   29964 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763370926.648708   29964 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763370926.664664   29964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370926.664699   29964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370926.664702   29964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763370926.664703   29964 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 09:15:34 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:35 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:35 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 09:15:38.267710616 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m WARNING 11-17 09:15:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:38 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:38 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:38 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.19it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.19it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:39 [default_loader.py:268] Loading weights took 0.87 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:40 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.081595 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:48 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/44c42810b9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:48 [backends.py:550] Dynamo bytecode transform time: 7.82 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:52 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.835 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:53 [monitor.py:34] torch.compile takes 7.82 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:54 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:54 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:54 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.19it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:59 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:59 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=29964)\u001b[0;0m INFO 11-17 09:15:59 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.56 seconds\n",
            "INFO 11-17 09:16:00 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 09:16:00 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 863.90it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [13:12<00:00, 20.18it/s, est. speed input: 1746.51 toks/s, output: 5019.79 toks/s]\n",
            "[rank0]:[W1117 09:29:16.375807647 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/batch_64/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_64/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9377\n",
            "Overall Accuracy: 58.61%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5850          0.5850         \n",
            "2     0.5813          0.7190         \n",
            "3     0.5835          0.7830         \n",
            "4     0.5853          0.8165         \n",
            "5     0.5833          0.8415         \n",
            "6     0.5834          0.8560         \n",
            "7     0.5840          0.8725         \n",
            "8     0.5861          0.8825         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/batch_64/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/batch_64/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/batch_64/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_64/iteration_0/evaluation.jsonl\n",
            "Filtered 9377 correct examples out of 16000 total examples\n",
            "Accuracy: 58.61%\n",
            "Saved to ckpt/batch_64/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9377\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/batch_64/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/batch_64/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-17 09:29:26.108659: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 09:29:26.126416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763371766.148981   33591 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763371766.155717   33591 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763371766.172845   33591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763371766.172880   33591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763371766.172882   33591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763371766.172884   33591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 09:29:26.177902: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['v_proj', 'up_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'q_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9377 examples from ckpt/batch_64/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9377/9377 [00:19<00:00, 470.40it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run 56cxmxbz\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251117_093021-56cxmxbz\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run visionary-aardvark-13\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/56cxmxbz\n",
            "Saved preprocessed features to ckpt/batch_64/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9189, #eval 188\n",
            "{'loss': 0.2845, 'grad_norm': 0.059472016990184784, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2898, 'grad_norm': 0.05865606665611267, 'learning_rate': 1.999762027079909e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2944, 'grad_norm': 0.06306018680334091, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.02}\n",
            "{'loss': 0.2777, 'grad_norm': 0.05730181932449341, 'learning_rate': 1.9978589232386036e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2821, 'grad_norm': 0.05753399059176445, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2866, 'grad_norm': 0.057443488389253616, 'learning_rate': 1.9940563382223196e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2699, 'grad_norm': 0.061580296605825424, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.05}\n",
            "{'loss': 0.2799, 'grad_norm': 0.05819419026374817, 'learning_rate': 1.988361510467761e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2942, 'grad_norm': 0.0633404478430748, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2676, 'grad_norm': 0.06703311949968338, 'learning_rate': 1.9807852804032306e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2816, 'grad_norm': 0.05973420292139053, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2935, 'grad_norm': 0.05536540225148201, 'learning_rate': 1.9713420698132614e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2663, 'grad_norm': 0.05616707354784012, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.09}\n",
            "{'loss': 0.2829, 'grad_norm': 0.05515280365943909, 'learning_rate': 1.960049854385929e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2668, 'grad_norm': 0.056196704506874084, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2662, 'grad_norm': 0.058617476373910904, 'learning_rate': 1.946930129495106e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2963, 'grad_norm': 0.05544188246130943, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.12}\n",
            "{'loss': 0.2957, 'grad_norm': 0.058967385441064835, 'learning_rate': 1.932007869282799e-05, 'epoch': 0.13}\n",
            "{'loss': 0.288, 'grad_norm': 0.0567169189453125, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2772, 'grad_norm': 0.05919715389609337, 'learning_rate': 1.9153114791194475e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2968, 'grad_norm': 0.0575842559337616, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2906, 'grad_norm': 0.06192254275083542, 'learning_rate': 1.8968727415326885e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2721, 'grad_norm': 0.06193443760275841, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.16}\n",
            "{'loss': 0.2946, 'grad_norm': 0.05799903720617294, 'learning_rate': 1.876726755707508e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2763, 'grad_norm': 0.05442965403199196, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2766, 'grad_norm': 0.05539415776729584, 'learning_rate': 1.854911870672947e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2949, 'grad_norm': 0.05511412024497986, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2933, 'grad_norm': 0.06117028743028641, 'learning_rate': 1.8314696123025456e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2983, 'grad_norm': 0.057931575924158096, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2541, 'grad_norm': 0.056206054985523224, 'learning_rate': 1.806444604267483e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2621, 'grad_norm': 0.0633300170302391, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2903, 'grad_norm': 0.05556870996952057, 'learning_rate': 1.7798844830928818e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2783, 'grad_norm': 0.058074355125427246, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.23}\n",
            "{'loss': 0.2629, 'grad_norm': 0.054769597947597504, 'learning_rate': 1.7518398074789776e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2755, 'grad_norm': 0.05912279337644577, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.24}\n",
            "{'loss': 0.267, 'grad_norm': 0.0625743716955185, 'learning_rate': 1.7223639620597556e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2737, 'grad_norm': 0.06014848127961159, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2965, 'grad_norm': 0.05617108196020126, 'learning_rate': 1.6915130557822698e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2925, 'grad_norm': 0.0575309582054615, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2722, 'grad_norm': 0.052279431372880936, 'learning_rate': 1.659345815100069e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2896, 'grad_norm': 0.05947984755039215, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2903, 'grad_norm': 0.056158602237701416, 'learning_rate': 1.6259234721840595e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2789, 'grad_norm': 0.053517166525125504, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2822, 'grad_norm': 0.05364544689655304, 'learning_rate': 1.5913096483635827e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2869, 'grad_norm': 0.053744081407785416, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2957, 'grad_norm': 0.059896379709243774, 'learning_rate': 1.5555702330196024e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2921, 'grad_norm': 0.05622372776269913, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2987, 'grad_norm': 0.05946248024702072, 'learning_rate': 1.5187732581605217e-05, 'epoch': 0.33}\n",
            "{'loss': 0.282, 'grad_norm': 0.056580815464258194, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2668, 'grad_norm': 0.05926692858338356, 'learning_rate': 1.4809887689193878e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3123, 'grad_norm': 0.05699467658996582, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.36}\n",
            "{'loss': 0.3194, 'grad_norm': 0.05643579736351967, 'learning_rate': 1.4422886902190014e-05, 'epoch': 0.36}\n",
            "{'loss': 0.285, 'grad_norm': 0.0548517107963562, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2927, 'grad_norm': 0.05780665576457977, 'learning_rate': 1.4027466898587375e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2795, 'grad_norm': 0.05481569096446037, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2879, 'grad_norm': 0.05548779293894768, 'learning_rate': 1.3624380382837017e-05, 'epoch': 0.39}\n",
            "{'loss': 0.3004, 'grad_norm': 0.05558071285486221, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.4}\n",
            "{'loss': 0.3065, 'grad_norm': 0.05357418954372406, 'learning_rate': 1.3214394653031616e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2673, 'grad_norm': 0.05475786700844765, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.41}\n",
            "{'loss': 0.3075, 'grad_norm': 0.05720324069261551, 'learning_rate': 1.2798290140309924e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2671, 'grad_norm': 0.05957111716270447, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2767, 'grad_norm': 0.05628242716193199, 'learning_rate': 1.2376858923261732e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2776, 'grad_norm': 0.055666521191596985, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.44}\n",
            "{'loss': 0.309, 'grad_norm': 0.05518022179603577, 'learning_rate': 1.1950903220161286e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2689, 'grad_norm': 0.05130765959620476, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2967, 'grad_norm': 0.059445928782224655, 'learning_rate': 1.1521233861899168e-05, 'epoch': 0.46}\n",
            "{'loss': 0.3108, 'grad_norm': 0.0547553226351738, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2913, 'grad_norm': 0.05720296502113342, 'learning_rate': 1.1088668748519646e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2844, 'grad_norm': 0.05417529493570328, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.48}\n",
            "{'loss': 0.302, 'grad_norm': 0.05690542608499527, 'learning_rate': 1.0654031292301432e-05, 'epoch': 0.49}\n",
            "{'loss': 0.3006, 'grad_norm': 0.05502435564994812, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.49}\n",
            "{'loss': 0.3021, 'grad_norm': 0.05743557959794998, 'learning_rate': 1.0218148850345613e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2919, 'grad_norm': 0.05309750512242317, 'learning_rate': 1e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2921, 'grad_norm': 0.052909739315509796, 'learning_rate': 9.78185114965439e-06, 'epoch': 0.52}\n",
            "{'loss': 0.254, 'grad_norm': 0.055791739374399185, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.52}\n",
            "{'loss': 0.2819, 'grad_norm': 0.05310549587011337, 'learning_rate': 9.34596870769857e-06, 'epoch': 0.53}\n",
            "{'loss': 0.2703, 'grad_norm': 0.05506691336631775, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.54}\n",
            "100%|██████████| 144/144 [35:07<00:00, 14.64s/it]\n",
            "\n",
            "{'loss': 0.2784, 'grad_norm': 0.05547535791993141, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.55}\n",
            "{'loss': 0.2936, 'grad_norm': 0.05707421153783798, 'learning_rate': 8.478766138100834e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2769, 'grad_norm': 0.05581750348210335, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2864, 'grad_norm': 0.054904207587242126, 'learning_rate': 8.04909677983872e-06, 'epoch': 0.57}\n",
            "{'loss': 0.3018, 'grad_norm': 0.05866776034235954, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.58}\n",
            "{'loss': 0.284, 'grad_norm': 0.0625155121088028, 'learning_rate': 7.623141076738271e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2965, 'grad_norm': 0.0561719611287117, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.59}\n",
            "{'loss': 0.3141, 'grad_norm': 0.05763905867934227, 'learning_rate': 7.201709859690081e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2902, 'grad_norm': 0.05096844583749771, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2854, 'grad_norm': 0.05447131395339966, 'learning_rate': 6.785605346968387e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2766, 'grad_norm': 0.05549703165888786, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2725, 'grad_norm': 0.05405657738447189, 'learning_rate': 6.375619617162985e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2921, 'grad_norm': 0.05359668284654617, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2616, 'grad_norm': 0.05406086519360542, 'learning_rate': 5.97253310141263e-06, 'epoch': 0.64}\n",
            "{'loss': 0.267, 'grad_norm': 0.05353052541613579, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2951, 'grad_norm': 0.05731518939137459, 'learning_rate': 5.5771130978099896e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2664, 'grad_norm': 0.05267135798931122, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2836, 'grad_norm': 0.05408530309796333, 'learning_rate': 5.190112310806126e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2935, 'grad_norm': 0.0592464953660965, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2704, 'grad_norm': 0.04936988279223442, 'learning_rate': 4.812267418394784e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2964, 'grad_norm': 0.0550573468208313, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2951, 'grad_norm': 0.05616539344191551, 'learning_rate': 4.444297669803981e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2997, 'grad_norm': 0.0548504963517189, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.7}\n",
            "{'loss': 0.3153, 'grad_norm': 0.0581885501742363, 'learning_rate': 4.086903516364179e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2893, 'grad_norm': 0.05900794640183449, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2847, 'grad_norm': 0.055955979973077774, 'learning_rate': 3.7407652781594094e-06, 'epoch': 0.72}\n",
            "{'loss': 0.3135, 'grad_norm': 0.06184452027082443, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2901, 'grad_norm': 0.055946290493011475, 'learning_rate': 3.4065418489993118e-06, 'epoch': 0.74}\n",
            "{'loss': 0.3055, 'grad_norm': 0.05778525769710541, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2925, 'grad_norm': 0.054535698145627975, 'learning_rate': 3.0848694421773075e-06, 'epoch': 0.75}\n",
            "{'loss': 0.3029, 'grad_norm': 0.05878932774066925, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2914, 'grad_norm': 0.05932941287755966, 'learning_rate': 2.776360379402445e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2968, 'grad_norm': 0.05428637936711311, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2854, 'grad_norm': 0.05767819285392761, 'learning_rate': 2.4816019252102274e-06, 'epoch': 0.78}\n",
            "{'loss': 0.3105, 'grad_norm': 0.05507738143205643, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2899, 'grad_norm': 0.05091524496674538, 'learning_rate': 2.201155169071184e-06, 'epoch': 0.79}\n",
            "{'loss': 0.3044, 'grad_norm': 0.055461373180150986, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.8}\n",
            "{'loss': 0.3036, 'grad_norm': 0.05954335629940033, 'learning_rate': 1.9355539573251737e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2732, 'grad_norm': 0.05614684522151947, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2853, 'grad_norm': 0.05125357210636139, 'learning_rate': 1.6853038769745466e-06, 'epoch': 0.82}\n",
            "{'loss': 0.3198, 'grad_norm': 0.05535026267170906, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2838, 'grad_norm': 0.055724821984767914, 'learning_rate': 1.4508812932705364e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2896, 'grad_norm': 0.05518971383571625, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2742, 'grad_norm': 0.054936911910772324, 'learning_rate': 1.2327324429249232e-06, 'epoch': 0.85}\n",
            "{'loss': 0.2923, 'grad_norm': 0.05332741513848305, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2871, 'grad_norm': 0.05432399734854698, 'learning_rate': 1.0312725846731174e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2737, 'grad_norm': 0.05040648579597473, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.87}\n",
            "{'loss': 0.3116, 'grad_norm': 0.05562888830900192, 'learning_rate': 8.468852088055291e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2865, 'grad_norm': 0.055196259170770645, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2922, 'grad_norm': 0.0612553134560585, 'learning_rate': 6.799213071720156e-07, 'epoch': 0.89}\n",
            "{'loss': 0.298, 'grad_norm': 0.056890081614255905, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.9}\n",
            "{'loss': 0.289, 'grad_norm': 0.05331532284617424, 'learning_rate': 5.306987050489442e-07, 'epoch': 0.91}\n",
            "{'loss': 0.252, 'grad_norm': 0.05762051045894623, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2821, 'grad_norm': 0.06444160640239716, 'learning_rate': 3.99501456140714e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2902, 'grad_norm': 0.056637346744537354, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2807, 'grad_norm': 0.05580741912126541, 'learning_rate': 2.865793018673857e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2805, 'grad_norm': 0.054460179060697556, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.94}\n",
            "{'loss': 0.268, 'grad_norm': 0.06150278076529503, 'learning_rate': 1.921471959676957e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2638, 'grad_norm': 0.05480406805872917, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2937, 'grad_norm': 0.05725155025720596, 'learning_rate': 1.1638489532239339e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2877, 'grad_norm': 0.05685364454984665, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.97}\n",
            "{'loss': 0.3022, 'grad_norm': 0.058740582317113876, 'learning_rate': 5.943661777680354e-08, 'epoch': 0.97}\n",
            "{'loss': 0.2708, 'grad_norm': 0.05029495060443878, 'learning_rate': 3.805301908254455e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2707, 'grad_norm': 0.051418911665678024, 'learning_rate': 2.1410767613965212e-08, 'epoch': 0.99}\n",
            "{'loss': 0.2776, 'grad_norm': 0.057810984551906586, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'loss': 0.3164, 'grad_norm': 0.0737880989909172, 'learning_rate': 2.379729200908676e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 2109.8549, 'train_samples_per_second': 4.355, 'train_steps_per_second': 0.068, 'train_loss': 0.28694125326971215, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/batch_64/models/model_iter_1 into base model; saving to ckpt/batch_64/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/batch_64/models/model_iter_1/checkpoint-120 into base model; saving to ckpt/batch_64/models/model_iter_1/checkpoint-120-merged\n",
            "Merging LoRA adapter from ckpt/batch_64/models/model_iter_1/checkpoint-144 into base model; saving to ckpt/batch_64/models/model_iter_1/checkpoint-144-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mvisionary-aardvark-13\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251117_093021-56cxmxbz/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/batch_64/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/batch_64\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9377 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/batch_64/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/batch_64/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/batch_64/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_64/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_64/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "aYbYLS1_Scvr",
        "outputId": "119e703a-6ace-4e99-ac13-a4cb00044fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aYbYLS1_Scvr",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/batch_64/models/model_iter_1-merged\n",
            "Output Directory: results/batch_64/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_100606\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/batch_64/rollout1/model_iter_1-merged_20251117_100606_inference.jsonl\n",
            "\n",
            "2025-11-17 10:06:12.186882: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:06:12.204702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763373972.226585   43164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763373972.233241   43164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763373972.250008   43164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763373972.250044   43164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763373972.250046   43164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763373972.250048   43164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:06:12.254943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:06:19 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:06:22 [utils.py:328] non-default args: {'tokenizer': 'ckpt/batch_64/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/batch_64/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:06:39 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:06:39 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:06:41 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:06:42 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:06:47.046682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763374007.067907   43436 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763374007.074234   43436 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763374007.089967   43436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374007.089999   43436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374007.090001   43436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374007.090003   43436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:06:54 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:56 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/batch_64/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/batch_64/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/batch_64/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:06:59.280111264 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:59 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m WARNING 11-17 10:06:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:59 [gpu_model_runner.py:2338] Starting to load model ckpt/batch_64/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:59 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:06:59 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.15it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.15it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:00 [default_loader.py:268] Loading weights took 0.90 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:01 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.139448 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:11 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/5328163a6e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:11 [backends.py:550] Dynamo bytecode transform time: 8.89 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:11 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:17 [backends.py:215] Compiling a graph for dynamic shape takes 5.81 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:18 [monitor.py:34] torch.compile takes 14.70 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:20 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:20 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:20 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.08it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:24 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:24 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43436)\u001b[0;0m INFO 11-17 10:07:24 [core.py:218] init engine (profile, create kv cache, warmup model) took 23.00 seconds\n",
            "INFO 11-17 10:07:26 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:07:26 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1782.50it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:19<00:00,  9.49it/s, est. speed input: 837.62 toks/s, output: 2492.29 toks/s]\n",
            "[rank0]:[W1117 10:09:46.226117519 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/batch_64/rollout1/model_iter_1-merged_20251117_100606_evaluation.jsonl\n",
            "\n",
            "Loading data from results/batch_64/rollout1/model_iter_1-merged_20251117_100606_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 788\n",
            "Overall Accuracy: 59.74%\n",
            "\n",
            "Results saved to results/batch_64/rollout1/model_iter_1-merged_20251117_100606_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/batch_64/rollout1/model_iter_1-merged_20251117_100606_inference.jsonl\n",
            "  - Evaluation results: results/batch_64/rollout1/model_iter_1-merged_20251117_100606_evaluation.jsonl\n",
            "  - Log file: results/batch_64/rollout1/model_iter_1-merged_20251117_100606.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_64/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_64/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "8lRG831YSeEr",
        "outputId": "848c1018-a1b1-4f38-a7b4-ba537747a3fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8lRG831YSeEr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/batch_64/models/model_iter_1-merged\n",
            "Output Directory: results/batch_64/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_100949\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/batch_64/rollout8/model_iter_1-merged_20251117_100949_inference.jsonl\n",
            "\n",
            "2025-11-17 10:09:54.161883: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:09:54.180482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763374194.202517   44464 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763374194.209091   44464 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763374194.226188   44464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374194.226224   44464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374194.226226   44464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374194.226228   44464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:09:54.231128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:10:01 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:10:03 [utils.py:328] non-default args: {'tokenizer': 'ckpt/batch_64/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/batch_64/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:10:20 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:10:20 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:10:22 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:10:22 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:10:27.341705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763374227.363676   44701 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763374227.370301   44701 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763374227.387065   44701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374227.387103   44701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374227.387105   44701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763374227.387107   44701 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:10:35 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:36 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:36 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/batch_64/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/batch_64/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/batch_64/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:10:39.176165442 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:39 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m WARNING 11-17 10:10:39 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:39 [gpu_model_runner.py:2338] Starting to load model ckpt/batch_64/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:39 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:39 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:40 [default_loader.py:268] Loading weights took 0.89 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:41 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.106074 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:49 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/5328163a6e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:49 [backends.py:550] Dynamo bytecode transform time: 7.99 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.203 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:54 [monitor.py:34] torch.compile takes 7.99 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:56 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:56 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:10:56 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.66it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:11:00 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:11:00 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=44701)\u001b[0;0m INFO 11-17 10:11:00 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.29 seconds\n",
            "INFO 11-17 10:11:02 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:11:02 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1130.07it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:41<00:00, 10.54it/s, est. speed input: 930.36 toks/s, output: 2746.42 toks/s]\n",
            "[rank0]:[W1117 10:27:45.386573172 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/batch_64/rollout8/model_iter_1-merged_20251117_100949_evaluation.jsonl\n",
            "\n",
            "Loading data from results/batch_64/rollout8/model_iter_1-merged_20251117_100949_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6484\n",
            "Overall Accuracy: 61.45%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5974          0.5974         \n",
            "2     0.6005          0.7058         \n",
            "3     0.6101          0.7680         \n",
            "4     0.6137          0.7991         \n",
            "5     0.6164          0.8249         \n",
            "6     0.6159          0.8400         \n",
            "7     0.6155          0.8484         \n",
            "8     0.6145          0.8552         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/batch_64/rollout8/model_iter_1-merged_20251117_100949_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/batch_64/rollout8/model_iter_1-merged_20251117_100949_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/batch_64/rollout8/model_iter_1-merged_20251117_100949_inference.jsonl\n",
            "  - Evaluation results: results/batch_64/rollout8/model_iter_1-merged_20251117_100949_evaluation.jsonl\n",
            "  - Log file: results/batch_64/rollout8/model_iter_1-merged_20251117_100949.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase batch size (256)"
      ],
      "metadata": {
        "id": "bNDay-KiIzGv"
      },
      "id": "bNDay-KiIzGv"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --total_batch_size 256 --run_name batch_256 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3UEKnTJMhJ",
        "outputId": "ab81c2fc-e311-493f-80da-1ed7145de99b"
      },
      "id": "MD3UEKnTJMhJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: batch_256\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/batch_256\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 256\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 64 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/batch_256/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 09:11:10.826084: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:11:10.843352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763197870.864276   43275 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763197870.870662   43275 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763197870.886706   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886730   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886732   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197870.886734   43275 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:11:10.891479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 09:11:18 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 09:11:20 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 09:11:35 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 09:11:35 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 09:11:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 09:11:38 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 09:11:42.846849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763197902.867840   43529 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763197902.874276   43529 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763197902.889891   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889914   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889916   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763197902.889918   43529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 09:11:50 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:51 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:51 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 09:11:54.275392244 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m WARNING 11-15 09:11:54 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:54 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:55 [default_loader.py:268] Loading weights took 1.01 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:11:56 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.247750 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:04 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:04 [backends.py:550] Dynamo bytecode transform time: 7.42 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:07 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.724 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:08 [monitor.py:34] torch.compile takes 7.42 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:09 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:10 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:10 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 24.46it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=43529)\u001b[0;0m INFO 11-15 09:12:14 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.27 seconds\n",
            "INFO 11-15 09:12:15 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 09:12:15 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 825.78it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:16<00:00, 62.29it/s, est. speed input: 5391.89 toks/s, output: 15429.70 toks/s]\n",
            "[rank0]:[W1115 09:16:35.054693913 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_256/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9309\n",
            "Overall Accuracy: 58.18%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5820          0.5820         \n",
            "2     0.5813          0.7185         \n",
            "3     0.5810          0.7770         \n",
            "4     0.5816          0.8195         \n",
            "5     0.5814          0.8425         \n",
            "6     0.5797          0.8550         \n",
            "7     0.5795          0.8650         \n",
            "8     0.5818          0.8770         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/batch_256/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/batch_256/iteration_0/evaluation.jsonl\n",
            "Filtered 9309 correct examples out of 16000 total examples\n",
            "Accuracy: 58.18%\n",
            "Saved to ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9309\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/batch_256/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 09:16:44.115618: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:16:44.133291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763198204.154940   44950 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763198204.161532   44950 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763198204.177783   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177809   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177811   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763198204.177812   44950 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:16:44.182611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['o_proj', 'gate_proj', 'v_proj', 'q_proj', 'k_proj', 'up_proj', 'down_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9309 examples from ckpt/batch_256/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9309/9309 [00:19<00:00, 484.83it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run kpqxntam\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_091731-kpqxntam\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run stellar-music-6\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/kpqxntam\n",
            "Saved preprocessed features to ckpt/batch_256/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9122, #eval 187\n",
            "100%|██████████| 36/36 [15:11<00:00, 25.32s/it]\n",
            "{'loss': 0.2855, 'grad_norm': 0.02936842478811741, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2798, 'grad_norm': 0.02932041883468628, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2748, 'grad_norm': 0.029304608702659607, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2834, 'grad_norm': 0.030547577887773514, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.11}\n",
            "{'loss': 0.288, 'grad_norm': 0.029294265434145927, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2855, 'grad_norm': 0.030200418084859848, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.17}\n",
            "{'loss': 0.281, 'grad_norm': 0.031050531193614006, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2804, 'grad_norm': 0.031001780182123184, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2912, 'grad_norm': 0.03204762563109398, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2878, 'grad_norm': 0.02918592467904091, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.28}\n",
            "{'loss': 0.277, 'grad_norm': 0.02910374291241169, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2892, 'grad_norm': 0.030668534338474274, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.34}\n",
            "{'loss': 0.277, 'grad_norm': 0.028750890865921974, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2963, 'grad_norm': 0.028941581025719643, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2773, 'grad_norm': 0.030207257717847824, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2967, 'grad_norm': 0.028317589312791824, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2874, 'grad_norm': 0.030486783012747765, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2751, 'grad_norm': 0.027700379490852356, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2843, 'grad_norm': 0.028920220211148262, 'learning_rate': 1e-05, 'epoch': 0.53}\n",
            "{'loss': 0.2957, 'grad_norm': 0.03000880964100361, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2955, 'grad_norm': 0.029308581724762917, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2735, 'grad_norm': 0.030717018991708755, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2806, 'grad_norm': 0.030530961230397224, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2813, 'grad_norm': 0.02824261598289013, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.67}\n",
            "{'loss': 0.3039, 'grad_norm': 0.029841937124729156, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2796, 'grad_norm': 0.029519828036427498, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.73}\n",
            "{'loss': 0.283, 'grad_norm': 0.030242038890719414, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2901, 'grad_norm': 0.028266804292798042, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2834, 'grad_norm': 0.027982031926512718, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2868, 'grad_norm': 0.028976362198591232, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2799, 'grad_norm': 0.02982487343251705, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.87}\n",
            "{'loss': 0.28, 'grad_norm': 0.0277622789144516, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2915, 'grad_norm': 0.03353345766663551, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2999, 'grad_norm': 0.03154942765831947, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2822, 'grad_norm': 0.029232030734419823, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.98}\n",
            "{'loss': 0.2872, 'grad_norm': 0.03809868171811104, 'learning_rate': 3.805301908254455e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 913.4291, 'train_samples_per_second': 9.987, 'train_steps_per_second': 0.039, 'train_loss': 0.2853355672624376, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1 into base model; saving to ckpt/batch_256/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1/checkpoint-30 into base model; saving to ckpt/batch_256/models/model_iter_1/checkpoint-30-merged\n",
            "Merging LoRA adapter from ckpt/batch_256/models/model_iter_1/checkpoint-36 into base model; saving to ckpt/batch_256/models/model_iter_1/checkpoint-36-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-music-6\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_091731-kpqxntam/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/batch_256/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/batch_256\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9309 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/batch_256/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/batch_256/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/batch_256/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_256/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_256/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "_1YFTDPsRgT9",
        "outputId": "12f0556f-d58f-413c-8aef-485a99bd2d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_1YFTDPsRgT9",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/batch_256/models/model_iter_1-merged\n",
            "Output Directory: results/batch_256/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_102749\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/batch_256/rollout1/model_iter_1-merged_20251117_102749_inference.jsonl\n",
            "\n",
            "2025-11-17 10:27:54.164418: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:27:54.183398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763375274.206015   49388 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763375274.212821   49388 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763375274.230155   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375274.230201   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375274.230204   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375274.230205   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:27:54.235448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:28:01 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:28:08 [utils.py:328] non-default args: {'tokenizer': 'ckpt/batch_256/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/batch_256/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:28:25 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:28:25 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:28:27 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:28:29 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:28:33.905394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763375313.926830   49663 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763375313.933432   49663 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763375313.949642   49663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375313.949674   49663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375313.949677   49663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375313.949680   49663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:28:41 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:43 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:43 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/batch_256/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/batch_256/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/batch_256/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:28:45.684253961 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:45 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m WARNING 11-17 10:28:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:45 [gpu_model_runner.py:2338] Starting to load model ckpt/batch_256/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:46 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:28:46 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.76s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.76s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:01 [default_loader.py:268] Loading weights took 14.78 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:01 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 14.996064 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:10 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/4b47aa8a4e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:10 [backends.py:550] Dynamo bytecode transform time: 7.94 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:10 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:16 [backends.py:215] Compiling a graph for dynamic shape takes 5.83 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:17 [monitor.py:34] torch.compile takes 13.77 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:19 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:19 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:19 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.42it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:23 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:23 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49663)\u001b[0;0m INFO 11-17 10:29:23 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.92 seconds\n",
            "INFO 11-17 10:29:25 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:29:25 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1761.38it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:17<00:00,  9.62it/s, est. speed input: 849.32 toks/s, output: 2488.36 toks/s]\n",
            "[rank0]:[W1117 10:31:43.337350667 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/batch_256/rollout1/model_iter_1-merged_20251117_102749_evaluation.jsonl\n",
            "\n",
            "Loading data from results/batch_256/rollout1/model_iter_1-merged_20251117_102749_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 799\n",
            "Overall Accuracy: 60.58%\n",
            "\n",
            "Results saved to results/batch_256/rollout1/model_iter_1-merged_20251117_102749_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/batch_256/rollout1/model_iter_1-merged_20251117_102749_inference.jsonl\n",
            "  - Evaluation results: results/batch_256/rollout1/model_iter_1-merged_20251117_102749_evaluation.jsonl\n",
            "  - Log file: results/batch_256/rollout1/model_iter_1-merged_20251117_102749.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/batch_256/models/model_iter_1-merged \\\n",
        "    --output_dir results/batch_256/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "PTLNC951SPvP",
        "outputId": "9a66e432-f9a1-4a90-839c-328d6a06b941",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PTLNC951SPvP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/batch_256/models/model_iter_1-merged\n",
            "Output Directory: results/batch_256/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_103146\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/batch_256/rollout8/model_iter_1-merged_20251117_103146_inference.jsonl\n",
            "\n",
            "2025-11-17 10:31:51.284543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:31:51.303147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763375511.325265   50734 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763375511.332101   50734 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763375511.349204   50734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375511.349242   50734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375511.349245   50734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375511.349246   50734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:31:51.353994: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:31:58 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:32:01 [utils.py:328] non-default args: {'tokenizer': 'ckpt/batch_256/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/batch_256/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:32:17 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:32:17 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:32:19 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:32:20 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:32:24.707373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763375544.729097   50970 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763375544.735525   50970 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763375544.751470   50970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375544.751504   50970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375544.751506   50970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763375544.751508   50970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:32:32 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:33 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:33 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/batch_256/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/batch_256/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/batch_256/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:32:36.566899770 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:36 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m WARNING 11-17 10:32:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:36 [gpu_model_runner.py:2338] Starting to load model ckpt/batch_256/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:36 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:37 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:38 [default_loader.py:268] Loading weights took 1.01 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:39 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.222807 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:47 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/4b47aa8a4e/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:47 [backends.py:550] Dynamo bytecode transform time: 8.01 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:51 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.181 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:52 [monitor.py:34] torch.compile takes 8.01 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:53 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:54 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:54 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.99it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:58 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:58 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=50970)\u001b[0;0m INFO 11-17 10:32:58 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.21 seconds\n",
            "INFO 11-17 10:32:59 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:32:59 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1125.37it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:38<00:00, 10.57it/s, est. speed input: 933.19 toks/s, output: 2746.73 toks/s]\n",
            "[rank0]:[W1117 10:49:39.791108155 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/batch_256/rollout8/model_iter_1-merged_20251117_103146_evaluation.jsonl\n",
            "\n",
            "Loading data from results/batch_256/rollout8/model_iter_1-merged_20251117_103146_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6340\n",
            "Overall Accuracy: 60.08%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.6103          0.6103         \n",
            "2     0.6149          0.7127         \n",
            "3     0.6050          0.7566         \n",
            "4     0.6077          0.7892         \n",
            "5     0.6032          0.8059         \n",
            "6     0.6008          0.8188         \n",
            "7     0.6020          0.8324         \n",
            "8     0.6008          0.8400         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/batch_256/rollout8/model_iter_1-merged_20251117_103146_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/batch_256/rollout8/model_iter_1-merged_20251117_103146_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/batch_256/rollout8/model_iter_1-merged_20251117_103146_inference.jsonl\n",
            "  - Evaluation results: results/batch_256/rollout8/model_iter_1-merged_20251117_103146_evaluation.jsonl\n",
            "  - Log file: results/batch_256/rollout8/model_iter_1-merged_20251117_103146.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase queries (3000)"
      ],
      "metadata": {
        "id": "ZQmWzY-OMBA9"
      },
      "id": "ZQmWzY-OMBA9"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --n_queries 3000 --run_name queries_3000 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s1qgYN-MD5W",
        "outputId": "1ce0e0fe-c3a3-4cec-d0cc-376a28f79852",
        "collapsed": true
      },
      "id": "9s1qgYN-MD5W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: queries_3000\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/queries_3000\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 3000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/queries_3000/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 08:37:09.974081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 08:37:09.990737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763195830.011773   34057 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763195830.018191   34057 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763195830.033905   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033930   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033932   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195830.033933   34057 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 08:37:10.038661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 08:37:17 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 08:37:19 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 08:37:34 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 08:37:34 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 08:37:36 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 08:37:37 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 08:37:41.736959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763195861.759942   34283 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763195861.766525   34283 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763195861.782378   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782405   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782407   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763195861.782410   34283 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 08:37:49 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:50 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:50 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 08:37:52.905517802 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:52 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m WARNING 11-15 08:37:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:52 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:53 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:53 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:54 [default_loader.py:268] Loading weights took 0.91 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:37:55 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.116693 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:03 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:03 [backends.py:550] Dynamo bytecode transform time: 7.50 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:06 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.773 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:07 [monitor.py:34] torch.compile takes 7.50 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:08 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 22.29it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=34283)\u001b[0;0m INFO 11-15 08:38:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.73 seconds\n",
            "INFO 11-15 08:38:14 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 08:38:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 3000/3000 [00:03<00:00, 911.74it/s]\n",
            "Processed prompts: 100%|██████████| 24000/24000 [06:55<00:00, 57.75it/s, est. speed input: 4961.75 toks/s, output: 14249.20 toks/s]\n",
            "[rank0]:[W1115 08:45:13.899744591 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_3000/iteration_0/inference.jsonl\n",
            "Loaded 24000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 24000\n",
            "Correct answers: 14191\n",
            "Overall Accuracy: 59.13%\n",
            "\n",
            "Detected multiple rollouts: 3000 unique questions with 24000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5817          0.5817         \n",
            "2     0.5872          0.7167         \n",
            "3     0.5903          0.7733         \n",
            "4     0.5917          0.8130         \n",
            "5     0.5896          0.8333         \n",
            "6     0.5919          0.8560         \n",
            "7     0.5914          0.8680         \n",
            "8     0.5913          0.8800         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/queries_3000/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_3000/iteration_0/evaluation.jsonl\n",
            "Filtered 14191 correct examples out of 24000 total examples\n",
            "Accuracy: 59.13%\n",
            "Saved to ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 14191\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/queries_3000/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 08:45:24.536046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 08:45:24.554338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763196324.576218   36398 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763196324.582845   36398 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763196324.599725   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599754   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599756   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763196324.599758   36398 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 08:45:24.604818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['up_proj', 'k_proj', 'q_proj', 'down_proj', 'gate_proj', 'v_proj', 'o_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 14191 examples from ckpt/queries_3000/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 14191/14191 [00:30<00:00, 465.50it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run shgyhjje\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_084630-shgyhjje\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run frosty-breeze-5\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/shgyhjje\n",
            "Saved preprocessed features to ckpt/queries_3000/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 13907, #eval 284\n",
            "{'loss': 0.2768, 'grad_norm': 0.04028002545237541, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2854, 'grad_norm': 0.04273822158575058, 'learning_rate': 1.9995846763238514e-05, 'epoch': 0.02}\n",
            "{'loss': 0.2759, 'grad_norm': 0.04066867008805275, 'learning_rate': 1.9983390502829168e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2784, 'grad_norm': 0.039802078157663345, 'learning_rate': 1.9962641565531694e-05, 'epoch': 0.04}\n",
            "{'loss': 0.292, 'grad_norm': 0.04141656309366226, 'learning_rate': 1.9933617186395917e-05, 'epoch': 0.05}\n",
            "{'loss': 0.2999, 'grad_norm': 0.04372560605406761, 'learning_rate': 1.9896341474445526e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2916, 'grad_norm': 0.04110953211784363, 'learning_rate': 1.985084539265195e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2879, 'grad_norm': 0.04345061257481575, 'learning_rate': 1.9797166732215078e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2751, 'grad_norm': 0.03979578614234924, 'learning_rate': 1.973535008117207e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2743, 'grad_norm': 0.041141096502542496, 'learning_rate': 1.9665446787360444e-05, 'epoch': 0.09}\n",
            "{'loss': 0.2959, 'grad_norm': 0.04029257968068123, 'learning_rate': 1.9587514915766124e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2928, 'grad_norm': 0.04131486639380455, 'learning_rate': 1.950161920029191e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2829, 'grad_norm': 0.04314209893345833, 'learning_rate': 1.940783098998643e-05, 'epoch': 0.12}\n",
            "{'loss': 0.304, 'grad_norm': 0.04469216614961624, 'learning_rate': 1.9306228189778255e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2861, 'grad_norm': 0.042133644223213196, 'learning_rate': 1.9196895195764363e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2803, 'grad_norm': 0.042241666465997696, 'learning_rate': 1.907992282510675e-05, 'epoch': 0.15}\n",
            "{'loss': 0.272, 'grad_norm': 0.04037926718592644, 'learning_rate': 1.8955408240595396e-05, 'epoch': 0.16}\n",
            "{'loss': 0.2963, 'grad_norm': 0.04457990452647209, 'learning_rate': 1.8823454869940243e-05, 'epoch': 0.17}\n",
            "{'loss': 0.275, 'grad_norm': 0.04307347163558006, 'learning_rate': 1.8684172319859258e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3016, 'grad_norm': 0.04214591905474663, 'learning_rate': 1.8537676285033886e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2907, 'grad_norm': 0.042574841529130936, 'learning_rate': 1.838408845200758e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2798, 'grad_norm': 0.03946307301521301, 'learning_rate': 1.8223536398107177e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2997, 'grad_norm': 0.04106869921088219, 'learning_rate': 1.8056153485471167e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2875, 'grad_norm': 0.04432719573378563, 'learning_rate': 1.788207875027274e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2969, 'grad_norm': 0.040162332355976105, 'learning_rate': 1.7701456787229805e-05, 'epoch': 0.23}\n",
            "{'loss': 0.2832, 'grad_norm': 0.03990985080599785, 'learning_rate': 1.751443762949772e-05, 'epoch': 0.24}\n",
            "{'loss': 0.302, 'grad_norm': 0.04155459627509117, 'learning_rate': 1.732117662404469e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2882, 'grad_norm': 0.04068106412887573, 'learning_rate': 1.712183430261319e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2864, 'grad_norm': 0.04048789665102959, 'learning_rate': 1.691657624837472e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2806, 'grad_norm': 0.039855845272541046, 'learning_rate': 1.6705572958388576e-05, 'epoch': 0.28}\n",
            "{'loss': 0.265, 'grad_norm': 0.04014814645051956, 'learning_rate': 1.6488999701978905e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2717, 'grad_norm': 0.04080072417855263, 'learning_rate': 1.6267036375147728e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2897, 'grad_norm': 0.040694400668144226, 'learning_rate': 1.6039867351144778e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2935, 'grad_norm': 0.04137460142374039, 'learning_rate': 1.5807681327318372e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2817, 'grad_norm': 0.043865349143743515, 'learning_rate': 1.557067116837444e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2876, 'grad_norm': 0.04406243562698364, 'learning_rate': 1.5329033746173975e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2865, 'grad_norm': 0.0383472703397274, 'learning_rate': 1.5082969776201948e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2792, 'grad_norm': 0.04001421481370926, 'learning_rate': 1.483268365084351e-05, 'epoch': 0.35}\n",
            "{'loss': 0.29, 'grad_norm': 0.044118039309978485, 'learning_rate': 1.4578383269606004e-05, 'epoch': 0.36}\n",
            "{'loss': 0.3094, 'grad_norm': 0.044070664793252945, 'learning_rate': 1.4320279866427798e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2651, 'grad_norm': 0.040482115000486374, 'learning_rate': 1.4058587834217356e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2848, 'grad_norm': 0.04045175388455391, 'learning_rate': 1.3793524546768358e-05, 'epoch': 0.39}\n",
            "{'loss': 0.291, 'grad_norm': 0.0408407598733902, 'learning_rate': 1.3525310178198707e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2752, 'grad_norm': 0.03954731673002243, 'learning_rate': 1.325416752006351e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2924, 'grad_norm': 0.03868754953145981, 'learning_rate': 1.2980321796293838e-05, 'epoch': 0.41}\n",
            "{'loss': 0.3134, 'grad_norm': 0.04140065237879753, 'learning_rate': 1.2704000476115079e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3099, 'grad_norm': 0.04064010828733444, 'learning_rate': 1.2425433085100224e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2833, 'grad_norm': 0.03956145420670509, 'learning_rate': 1.2144851014515055e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2653, 'grad_norm': 0.03889938443899155, 'learning_rate': 1.1862487329113606e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2826, 'grad_norm': 0.04027905687689781, 'learning_rate': 1.1578576573543541e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2894, 'grad_norm': 0.041782014071941376, 'learning_rate': 1.1293354577522264e-05, 'epoch': 0.47}\n",
            "{'loss': 0.3003, 'grad_norm': 0.043520957231521606, 'learning_rate': 1.1007058259945584e-05, 'epoch': 0.48}\n",
            "{'loss': 0.294, 'grad_norm': 0.041531480848789215, 'learning_rate': 1.0719925432091671e-05, 'epoch': 0.49}\n",
            "{'loss': 0.279, 'grad_norm': 0.03927183523774147, 'learning_rate': 1.043219460008374e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2791, 'grad_norm': 0.03828730806708336, 'learning_rate': 1.0144104766775574e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2788, 'grad_norm': 0.04105425998568535, 'learning_rate': 9.855895233224431e-06, 'epoch': 0.52}\n",
            "{'loss': 0.2829, 'grad_norm': 0.04085002839565277, 'learning_rate': 9.56780539991626e-06, 'epoch': 0.52}\n",
            "{'loss': 0.292, 'grad_norm': 0.0407135970890522, 'learning_rate': 9.28007456790833e-06, 'epoch': 0.53}\n",
            "{'loss': 0.277, 'grad_norm': 0.03943243250250816, 'learning_rate': 8.992941740054418e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2737, 'grad_norm': 0.037176404148340225, 'learning_rate': 8.706645422477739e-06, 'epoch': 0.55}\n",
            "{'loss': 0.2783, 'grad_norm': 0.03938686102628708, 'learning_rate': 8.42142342645646e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2848, 'grad_norm': 0.03843335434794426, 'learning_rate': 8.137512670886397e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2783, 'grad_norm': 0.037473294883966446, 'learning_rate': 7.855148985484946e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2866, 'grad_norm': 0.0399303063750267, 'learning_rate': 7.574566914899779e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2764, 'grad_norm': 0.03781687468290329, 'learning_rate': 7.295999523884921e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2873, 'grad_norm': 0.041501484811306, 'learning_rate': 7.019678203706164e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03989977389574051, 'learning_rate': 6.745832479936492e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2842, 'grad_norm': 0.04078992083668709, 'learning_rate': 6.474689821801295e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2848, 'grad_norm': 0.0419076532125473, 'learning_rate': 6.206475453231644e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2949, 'grad_norm': 0.04276753216981888, 'learning_rate': 5.941412165782645e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2801, 'grad_norm': 0.041444119065999985, 'learning_rate': 5.6797201335722064e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2879, 'grad_norm': 0.03962406516075134, 'learning_rate': 5.421616730394e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2901, 'grad_norm': 0.040491681545972824, 'learning_rate': 5.167316349156495e-06, 'epoch': 0.67}\n",
            "{'loss': 0.3025, 'grad_norm': 0.04124848544597626, 'learning_rate': 4.917030223798057e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2809, 'grad_norm': 0.03776826709508896, 'learning_rate': 4.670966253826027e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2802, 'grad_norm': 0.043126728385686874, 'learning_rate': 4.429328831625565e-06, 'epoch': 0.7}\n",
            "{'loss': 0.286, 'grad_norm': 0.039537299424409866, 'learning_rate': 4.192318672681631e-06, 'epoch': 0.71}\n",
            "100%|██████████| 109/109 [24:04<00:00, 13.25s/it]\n",
            "\n",
            "{'loss': 0.2684, 'grad_norm': 0.041475433856248856, 'learning_rate': 3.732963624852275e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2963, 'grad_norm': 0.04022267088294029, 'learning_rate': 3.511000298021098e-06, 'epoch': 0.74}\n",
            "{'loss': 0.3064, 'grad_norm': 0.04331429302692413, 'learning_rate': 3.2944270416114256e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2829, 'grad_norm': 0.03968895971775055, 'learning_rate': 3.0834237516252817e-06, 'epoch': 0.75}\n",
            "{'loss': 0.3044, 'grad_norm': 0.04018973559141159, 'learning_rate': 2.878165697386812e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2852, 'grad_norm': 0.039383452385663986, 'learning_rate': 2.678823375955314e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2927, 'grad_norm': 0.03943788632750511, 'learning_rate': 2.485562370502279e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2698, 'grad_norm': 0.03982218727469444, 'learning_rate': 2.2985432127701945e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2868, 'grad_norm': 0.04245101287961006, 'learning_rate': 2.1179212497272582e-06, 'epoch': 0.8}\n",
            "{'loss': 0.2908, 'grad_norm': 0.039420031011104584, 'learning_rate': 1.9438465145288377e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2891, 'grad_norm': 0.038497623056173325, 'learning_rate': 1.7764636018928249e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2844, 'grad_norm': 0.03872718662023544, 'learning_rate': 1.6159115479924259e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04169194772839546, 'learning_rate': 1.462323714966114e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2825, 'grad_norm': 0.04053223133087158, 'learning_rate': 1.3158276801407432e-06, 'epoch': 0.85}\n",
            "{'loss': 0.284, 'grad_norm': 0.04199393093585968, 'learning_rate': 1.1765451300597574e-06, 'epoch': 0.86}\n",
            "{'loss': 0.3002, 'grad_norm': 0.039502184838056564, 'learning_rate': 1.0445917594046073e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2709, 'grad_norm': 0.040075164288282394, 'learning_rate': 9.200771748932513e-07, 'epoch': 0.87}\n",
            "{'loss': 0.3057, 'grad_norm': 0.03965142369270325, 'learning_rate': 8.031048042356393e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2667, 'grad_norm': 0.037885427474975586, 'learning_rate': 6.937718102217461e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2887, 'grad_norm': 0.03893808275461197, 'learning_rate': 5.921690100135713e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2807, 'grad_norm': 0.0394974909722805, 'learning_rate': 4.983807997080925e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2944, 'grad_norm': 0.040340542793273926, 'learning_rate': 4.124850842338779e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03918294236063957, 'learning_rate': 3.345532126395579e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2945, 'grad_norm': 0.04081978276371956, 'learning_rate': 2.646499188279328e-07, 'epoch': 0.94}\n",
            "{'loss': 0.286, 'grad_norm': 0.039872072637081146, 'learning_rate': 2.028332677849254e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2814, 'grad_norm': 0.03757293149828911, 'learning_rate': 1.49154607348051e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2904, 'grad_norm': 0.040040452033281326, 'learning_rate': 1.0365852555447642e-07, 'epoch': 0.97}\n",
            "{'loss': 0.2885, 'grad_norm': 0.041506361216306686, 'learning_rate': 6.638281360408339e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2731, 'grad_norm': 0.04023900255560875, 'learning_rate': 3.735843446830867e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2763, 'grad_norm': 0.03877480328083038, 'learning_rate': 1.6609497170834154e-08, 'epoch': 0.99}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04942460358142853, 'learning_rate': 4.153236761488266e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 1446.6365, 'train_samples_per_second': 9.613, 'train_steps_per_second': 0.075, 'train_loss': 0.2863097934547914, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1 into base model; saving to ckpt/queries_3000/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1/checkpoint-109 into base model; saving to ckpt/queries_3000/models/model_iter_1/checkpoint-109-merged\n",
            "Merging LoRA adapter from ckpt/queries_3000/models/model_iter_1/checkpoint-90 into base model; saving to ckpt/queries_3000/models/model_iter_1/checkpoint-90-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mfrosty-breeze-5\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_084630-shgyhjje/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/queries_3000/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/queries_3000\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 14191 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/queries_3000/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/queries_3000/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/queries_3000/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_3000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_3000/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "ekoe-AGTRjw6",
        "outputId": "743f1f67-2f14-40de-b16e-09babb9e35ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ekoe-AGTRjw6",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/queries_3000/models/model_iter_1-merged\n",
            "Output Directory: results/queries_3000/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_104943\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_inference.jsonl\n",
            "\n",
            "2025-11-17 10:49:48.553948: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:49:48.572644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763376588.595462   55641 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763376588.602358   55641 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763376588.619616   55641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376588.619654   55641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376588.619656   55641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376588.619658   55641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:49:48.624629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:49:56 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:50:02 [utils.py:328] non-default args: {'tokenizer': 'ckpt/queries_3000/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/queries_3000/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:50:18 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:50:18 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:50:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:50:22 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:50:26.852765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763376626.874269   55908 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763376626.880860   55908 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763376626.897128   55908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376626.897169   55908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376626.897171   55908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376626.897173   55908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:50:34 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:36 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:36 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/queries_3000/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/queries_3000/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/queries_3000/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:50:38.693806077 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:38 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m WARNING 11-17 10:50:38 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:38 [gpu_model_runner.py:2338] Starting to load model ckpt/queries_3000/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:39 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:39 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.01s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.01s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:55 [default_loader.py:268] Loading weights took 16.04 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:50:56 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 16.250537 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:05 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ed0a43ba1f/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:05 [backends.py:550] Dynamo bytecode transform time: 8.63 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:05 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:11 [backends.py:215] Compiling a graph for dynamic shape takes 5.80 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:12 [monitor.py:34] torch.compile takes 14.43 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:14 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:14 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:14 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.35it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:18 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:18 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=55908)\u001b[0;0m INFO 11-17 10:51:18 [core.py:218] init engine (profile, create kv cache, warmup model) took 22.60 seconds\n",
            "INFO 11-17 10:51:20 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:51:20 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1789.31it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:17<00:00,  9.56it/s, est. speed input: 844.37 toks/s, output: 2492.35 toks/s]\n",
            "[rank0]:[W1117 10:53:39.077025145 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_evaluation.jsonl\n",
            "\n",
            "Loading data from results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 796\n",
            "Overall Accuracy: 60.35%\n",
            "\n",
            "Results saved to results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_inference.jsonl\n",
            "  - Evaluation results: results/queries_3000/rollout1/model_iter_1-merged_20251117_104943_evaluation.jsonl\n",
            "  - Log file: results/queries_3000/rollout1/model_iter_1-merged_20251117_104943.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_3000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_3000/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "CC1OrtejSNGK",
        "outputId": "27043eca-79a8-4ab4-d03e-1010d4e6defc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CC1OrtejSNGK",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/queries_3000/models/model_iter_1-merged\n",
            "Output Directory: results/queries_3000/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_105342\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_inference.jsonl\n",
            "\n",
            "2025-11-17 10:53:47.035518: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 10:53:47.054396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763376827.077044   56988 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763376827.083951   56988 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763376827.101216   56988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376827.101260   56988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376827.101262   56988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376827.101264   56988 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 10:53:47.106229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 10:53:54 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 10:53:56 [utils.py:328] non-default args: {'tokenizer': 'ckpt/queries_3000/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/queries_3000/models/model_iter_1-merged'}\n",
            "INFO 11-17 10:54:13 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 10:54:13 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 10:54:15 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 10:54:15 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 10:54:20.501831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763376860.525457   57230 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763376860.532661   57230 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763376860.550469   57230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376860.550503   57230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376860.550506   57230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763376860.550508   57230 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 10:54:28 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:29 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:29 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/queries_3000/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/queries_3000/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/queries_3000/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 10:54:32.406502855 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m WARNING 11-17 10:54:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:32 [gpu_model_runner.py:2338] Starting to load model ckpt/queries_3000/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:32 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:32 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:33 [default_loader.py:268] Loading weights took 0.89 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:34 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.110076 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:43 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ed0a43ba1f/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:43 [backends.py:550] Dynamo bytecode transform time: 8.05 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.218 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:47 [monitor.py:34] torch.compile takes 8.05 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:49 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:49 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:49 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.74it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:54 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:54 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=57230)\u001b[0;0m INFO 11-17 10:54:54 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.35 seconds\n",
            "INFO 11-17 10:54:55 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 10:54:55 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1113.51it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:38<00:00, 10.57it/s, est. speed input: 933.54 toks/s, output: 2745.29 toks/s]\n",
            "[rank0]:[W1117 11:11:35.328190410 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_evaluation.jsonl\n",
            "\n",
            "Loading data from results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6451\n",
            "Overall Accuracy: 61.14%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.6103          0.6103         \n",
            "2     0.6073          0.7096         \n",
            "3     0.6123          0.7574         \n",
            "4     0.6132          0.7908         \n",
            "5     0.6099          0.8127         \n",
            "6     0.6109          0.8271         \n",
            "7     0.6096          0.8362         \n",
            "8     0.6114          0.8506         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_inference.jsonl\n",
            "  - Evaluation results: results/queries_3000/rollout8/model_iter_1-merged_20251117_105342_evaluation.jsonl\n",
            "  - Log file: results/queries_3000/rollout8/model_iter_1-merged_20251117_105342.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease queries (1000)"
      ],
      "metadata": {
        "id": "lM1N-KByMSzQ"
      },
      "id": "lM1N-KByMSzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --n_queries 1000 --run_name queries_1000 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW9LAzaaMWaH",
        "outputId": "5229f6d0-34d3-48de-c06d-123a8218b9be",
        "collapsed": true
      },
      "id": "vW9LAzaaMWaH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: queries_1000\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/queries_1000\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 1000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/queries_1000/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 09:33:20.536588: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:33:20.554794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199200.575990   49383 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199200.582448   49383 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199200.599023   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599049   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599051   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199200.599052   49383 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:33:20.603849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 09:33:28 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 09:33:30 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 09:33:45 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 09:33:45 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 09:33:47 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 09:33:48 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 09:33:52.516880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199232.538171   49606 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199232.544641   49606 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199232.560380   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560406   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560408   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199232.560409   49606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 09:33:59 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:01 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:01 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 09:34:03.774311606 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:03 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m WARNING 11-15 09:34:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:03 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.18it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:05 [default_loader.py:268] Loading weights took 0.88 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:06 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.084327 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:13 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:13 [backends.py:550] Dynamo bytecode transform time: 7.51 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:17 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.740 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:18 [monitor.py:34] torch.compile takes 7.51 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:19 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 23.94it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=49606)\u001b[0;0m INFO 11-15 09:34:23 [core.py:218] init engine (profile, create kv cache, warmup model) took 17.48 seconds\n",
            "INFO 11-15 09:34:24 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 09:34:24 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1000/1000 [00:01<00:00, 971.33it/s]\n",
            "Processed prompts: 100%|██████████| 8000/8000 [02:10<00:00, 61.32it/s, est. speed input: 5303.64 toks/s, output: 15130.49 toks/s]\n",
            "[rank0]:[W1115 09:36:36.534600751 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_1000/iteration_0/inference.jsonl\n",
            "Loaded 8000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 8000\n",
            "Correct answers: 4697\n",
            "Overall Accuracy: 58.71%\n",
            "\n",
            "Detected multiple rollouts: 1000 unique questions with 8000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5900          0.5900         \n",
            "2     0.5945          0.7240         \n",
            "3     0.5963          0.7780         \n",
            "4     0.5952          0.8140         \n",
            "5     0.5902          0.8330         \n",
            "6     0.5950          0.8570         \n",
            "7     0.5907          0.8670         \n",
            "8     0.5871          0.8720         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/queries_1000/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/queries_1000/iteration_0/evaluation.jsonl\n",
            "Filtered 4697 correct examples out of 8000 total examples\n",
            "Accuracy: 58.71%\n",
            "Saved to ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 4697\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/queries_1000/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 09:36:45.081801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 09:36:45.099091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763199405.120200   50499 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763199405.126628   50499 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763199405.142873   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142897   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142899   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763199405.142901   50499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 09:36:45.147631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['k_proj', 'down_proj', 'o_proj', 'q_proj', 'up_proj', 'gate_proj', 'v_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 4697 examples from ckpt/queries_1000/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 4697/4697 [00:09<00:00, 482.68it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run rnldl98b\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_093722-rnldl98b\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run stellar-salad-7\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/rnldl98b\n",
            "Saved preprocessed features to ckpt/queries_1000/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 4603, #eval 94\n",
            "100%|██████████| 36/36 [07:37<00:00, 12.70s/it]\n",
            "{'loss': 0.2839, 'grad_norm': 0.04044223576784134, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2929, 'grad_norm': 0.03989022597670555, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2843, 'grad_norm': 0.03935060650110245, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2888, 'grad_norm': 0.04335852712392807, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2865, 'grad_norm': 0.04102490469813347, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2828, 'grad_norm': 0.039688486605882645, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.17}\n",
            "{'loss': 0.283, 'grad_norm': 0.04040113463997841, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2859, 'grad_norm': 0.040744468569755554, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2805, 'grad_norm': 0.04010471701622009, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2747, 'grad_norm': 0.03895456716418266, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.28}\n",
            "{'loss': 0.3227, 'grad_norm': 0.04341733828186989, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2929, 'grad_norm': 0.03923780471086502, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2929, 'grad_norm': 0.04196038097143173, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2993, 'grad_norm': 0.04025633633136749, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.39}\n",
            "{'loss': 0.3014, 'grad_norm': 0.04030328616499901, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2899, 'grad_norm': 0.04067549854516983, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2954, 'grad_norm': 0.03969379514455795, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2856, 'grad_norm': 0.040728725492954254, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2744, 'grad_norm': 0.04190267622470856, 'learning_rate': 1e-05, 'epoch': 0.53}\n",
            "{'loss': 0.3124, 'grad_norm': 0.04055052250623703, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2933, 'grad_norm': 0.03988247737288475, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.58}\n",
            "{'loss': 0.3022, 'grad_norm': 0.039171647280454636, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2908, 'grad_norm': 0.037890754640102386, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2784, 'grad_norm': 0.04115995392203331, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2788, 'grad_norm': 0.04423469677567482, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2935, 'grad_norm': 0.04607812315225601, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2928, 'grad_norm': 0.04000306874513626, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2909, 'grad_norm': 0.040131762623786926, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2704, 'grad_norm': 0.042681984603405, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2897, 'grad_norm': 0.04200751706957817, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2805, 'grad_norm': 0.04028240218758583, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2791, 'grad_norm': 0.03913095220923424, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2937, 'grad_norm': 0.041734568774700165, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2865, 'grad_norm': 0.043033815920352936, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2786, 'grad_norm': 0.041809841990470886, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.97}\n",
            "{'loss': 0.3009, 'grad_norm': 0.03993520140647888, 'learning_rate': 3.805301908254455e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 459.3239, 'train_samples_per_second': 10.021, 'train_steps_per_second': 0.078, 'train_loss': 0.2891683735781246, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1 into base model; saving to ckpt/queries_1000/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1/checkpoint-30 into base model; saving to ckpt/queries_1000/models/model_iter_1/checkpoint-30-merged\n",
            "Merging LoRA adapter from ckpt/queries_1000/models/model_iter_1/checkpoint-36 into base model; saving to ckpt/queries_1000/models/model_iter_1/checkpoint-36-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-salad-7\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_093722-rnldl98b/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/queries_1000/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/queries_1000\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 4697 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/queries_1000/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/queries_1000/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/queries_1000/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_1000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_1000/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "Gnb-oRv9RoBl",
        "outputId": "0dabf51b-97a9-4274-d9f9-29a6387dda97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gnb-oRv9RoBl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/queries_1000/models/model_iter_1-merged\n",
            "Output Directory: results/queries_1000/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_111139\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_inference.jsonl\n",
            "\n",
            "2025-11-17 11:11:44.166886: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:11:44.185830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763377904.208763   61902 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763377904.215691   61902 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763377904.233139   61902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377904.233181   61902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377904.233184   61902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377904.233185   61902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:11:44.238122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:11:51 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:11:57 [utils.py:328] non-default args: {'tokenizer': 'ckpt/queries_1000/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/queries_1000/models/model_iter_1-merged'}\n",
            "INFO 11-17 11:12:14 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 11:12:14 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 11:12:16 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 11:12:17 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 11:12:22.046649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763377942.068415   62165 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763377942.074951   62165 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763377942.091034   62165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377942.091070   62165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377942.091072   62165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763377942.091074   62165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 11:12:29 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:31 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:31 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/queries_1000/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/queries_1000/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/queries_1000/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 11:12:33.950877483 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:33 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m WARNING 11-17 11:12:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:33 [gpu_model_runner.py:2338] Starting to load model ckpt/queries_1000/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:34 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:12:34 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:49<00:00, 49.25s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:49<00:00, 49.25s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:23 [default_loader.py:268] Loading weights took 49.28 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:24 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 49.494598 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:33 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7e350c21f9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:33 [backends.py:550] Dynamo bytecode transform time: 7.92 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:33 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:39 [backends.py:215] Compiling a graph for dynamic shape takes 5.80 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:40 [monitor.py:34] torch.compile takes 13.73 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:41 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:42 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:42 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.01it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:46 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:46 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=62165)\u001b[0;0m INFO 11-17 11:13:46 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.97 seconds\n",
            "INFO 11-17 11:13:47 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 11:13:47 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1801.76it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:18<00:00,  9.53it/s, est. speed input: 841.30 toks/s, output: 2489.24 toks/s]\n",
            "[rank0]:[W1117 11:16:07.460330231 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_evaluation.jsonl\n",
            "\n",
            "Loading data from results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 795\n",
            "Overall Accuracy: 60.27%\n",
            "\n",
            "Results saved to results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_inference.jsonl\n",
            "  - Evaluation results: results/queries_1000/rollout1/model_iter_1-merged_20251117_111139_evaluation.jsonl\n",
            "  - Log file: results/queries_1000/rollout1/model_iter_1-merged_20251117_111139.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/queries_1000/models/model_iter_1-merged \\\n",
        "    --output_dir results/queries_1000/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "V30vRcT4SL_Y",
        "outputId": "f9b2899b-1efe-4643-88f6-df342e9e324b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V30vRcT4SL_Y",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/queries_1000/models/model_iter_1-merged\n",
            "Output Directory: results/queries_1000/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_111610\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_inference.jsonl\n",
            "\n",
            "2025-11-17 11:16:15.452220: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:16:15.470443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763378175.492789   63386 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763378175.499505   63386 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763378175.516741   63386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378175.516778   63386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378175.516780   63386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378175.516782   63386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:16:15.521822: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:16:23 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:16:25 [utils.py:328] non-default args: {'tokenizer': 'ckpt/queries_1000/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/queries_1000/models/model_iter_1-merged'}\n",
            "INFO 11-17 11:16:41 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 11:16:41 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 11:16:43 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 11:16:44 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 11:16:49.149456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763378209.171279   63628 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763378209.177735   63628 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763378209.193930   63628 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378209.193962   63628 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378209.193964   63628 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763378209.193965   63628 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 11:16:56 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:16:58 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:16:58 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/queries_1000/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/queries_1000/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/queries_1000/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 11:17:00.990399460 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:00 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m WARNING 11-17 11:17:00 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:01 [gpu_model_runner.py:2338] Starting to load model ckpt/queries_1000/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:01 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:01 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.14it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:02 [default_loader.py:268] Loading weights took 0.90 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:03 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.115204 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:11 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7e350c21f9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:11 [backends.py:550] Dynamo bytecode transform time: 8.00 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.185 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:16 [monitor.py:34] torch.compile takes 8.00 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:17 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:18 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:18 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.97it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:22 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:22 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=63628)\u001b[0;0m INFO 11-17 11:17:22 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.24 seconds\n",
            "INFO 11-17 11:17:23 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 11:17:23 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1066.14it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:35<00:00, 10.60it/s, est. speed input: 935.70 toks/s, output: 2749.27 toks/s]\n",
            "[rank0]:[W1117 11:34:01.556449838 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_evaluation.jsonl\n",
            "\n",
            "Loading data from results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6320\n",
            "Overall Accuracy: 59.89%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5974          0.5974         \n",
            "2     0.5997          0.7020         \n",
            "3     0.5982          0.7491         \n",
            "4     0.5982          0.7809         \n",
            "5     0.5971          0.8059         \n",
            "6     0.5997          0.8165         \n",
            "7     0.5992          0.8256         \n",
            "8     0.5989          0.8362         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_inference.jsonl\n",
            "  - Evaluation results: results/queries_1000/rollout8/model_iter_1-merged_20251117_111610_evaluation.jsonl\n",
            "  - Log file: results/queries_1000/rollout8/model_iter_1-merged_20251117_111610.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase lr (4e-5)"
      ],
      "metadata": {
        "id": "CMSprCe0MaAm"
      },
      "id": "CMSprCe0MaAm"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --learning_rate 4e-5 --run_name \"lr_4e-5\" --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5dD8PT2Nj4W",
        "outputId": "04279608-05ae-48b3-fcb7-90a7a951beae",
        "collapsed": true
      },
      "id": "e5dD8PT2Nj4W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: lr_4e-5\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/lr_4e-5\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 4e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/lr_4e-5/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 13:29:18.218469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:29:18.235655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213358.257456    1525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213358.264010    1525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213358.280685    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280718    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280720    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213358.280722    1525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:29:18.285680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 13:29:26 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 13:29:35 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 13:29:54 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 13:29:54 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 13:29:56 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 13:29:57 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 13:30:02.101225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213402.123911    1815 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213402.130545    1815 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213402.147818    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147849    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147851    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213402.147853    1815 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 13:30:09 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:10 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:11 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 13:30:13.761900733 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:13 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m WARNING 11-15 13:30:13 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:13 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:14 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:14 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.35s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.35s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:37 [default_loader.py:268] Loading weights took 23.37 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:38 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 23.713307 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:47 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:47 [backends.py:550] Dynamo bytecode transform time: 8.43 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:30:52 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:18 [backends.py:215] Compiling a graph for dynamic shape takes 30.76 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:20 [monitor.py:34] torch.compile takes 39.19 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:21 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:22 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:22 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.60it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1815)\u001b[0;0m INFO 11-15 13:31:26 [core.py:218] init engine (profile, create kv cache, warmup model) took 48.13 seconds\n",
            "INFO 11-15 13:31:27 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 13:31:27 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:01<00:00, 1120.48it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:30<00:00, 59.14it/s, est. speed input: 5119.56 toks/s, output: 14691.55 toks/s]\n",
            "[rank0]:[W1115 13:36:00.037337397 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_4e-5/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9441\n",
            "Overall Accuracy: 59.01%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.6030          0.6030         \n",
            "2     0.5970          0.7340         \n",
            "3     0.5972          0.7915         \n",
            "4     0.5972          0.8235         \n",
            "5     0.5945          0.8445         \n",
            "6     0.5936          0.8585         \n",
            "7     0.5929          0.8680         \n",
            "8     0.5901          0.8730         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/lr_4e-5/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_4e-5/iteration_0/evaluation.jsonl\n",
            "Filtered 9441 correct examples out of 16000 total examples\n",
            "Accuracy: 59.01%\n",
            "Saved to ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9441\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/lr_4e-5/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 13:36:14.975068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:36:14.993175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763213775.014907    4054 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763213775.021606    4054 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763213775.038285    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038313    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038315    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763213775.038318    4054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:36:15.043281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['gate_proj', 'up_proj', 'v_proj', 'o_proj', 'k_proj', 'q_proj', 'down_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9441 examples from ckpt/lr_4e-5/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9441/9441 [00:20<00:00, 464.46it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_133710-r4c7xt12\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run icy-glade-8\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/r4c7xt12\n",
            "Saved preprocessed features to ckpt/lr_4e-5/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9252, #eval 189\n",
            "100%|██████████| 73/73 [15:35<00:00, 12.82s/it]\n",
            "{'loss': 0.2947, 'grad_norm': 0.04046342521905899, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2783, 'grad_norm': 0.04024248197674751, 'learning_rate': 3.9981482302044604e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2917, 'grad_norm': 0.04054071009159088, 'learning_rate': 3.992596349869216e-05, 'epoch': 0.04}\n",
            "{'loss': 0.3012, 'grad_norm': 0.04290148988366127, 'learning_rate': 3.98335463979858e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2806, 'grad_norm': 0.03945600241422653, 'learning_rate': 3.9704402135121214e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2945, 'grad_norm': 0.04307609796524048, 'learning_rate': 3.953876985554364e-05, 'epoch': 0.08}\n",
            "{'loss': 0.3165, 'grad_norm': 0.043376438319683075, 'learning_rate': 3.933695627210555e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2904, 'grad_norm': 0.04156087338924408, 'learning_rate': 3.909933509710511e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2939, 'grad_norm': 0.04310508444905281, 'learning_rate': 3.8826346350256943e-05, 'epoch': 0.12}\n",
            "{'loss': 0.2981, 'grad_norm': 0.03999540954828262, 'learning_rate': 3.8518495543877e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2854, 'grad_norm': 0.040826570242643356, 'learning_rate': 3.817635274679006e-05, 'epoch': 0.15}\n",
            "{'loss': 0.304, 'grad_norm': 0.04155397787690163, 'learning_rate': 3.780055152869354e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2788, 'grad_norm': 0.03806138038635254, 'learning_rate': 3.739178778693222e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2876, 'grad_norm': 0.043116651475429535, 'learning_rate': 3.695081845785663e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2958, 'grad_norm': 0.03857306018471718, 'learning_rate': 3.6478460115151084e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3102, 'grad_norm': 0.04275401309132576, 'learning_rate': 3.59755874577273e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2956, 'grad_norm': 0.040434326976537704, 'learning_rate': 3.5443131689983285e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2889, 'grad_norm': 0.0412430614233017, 'learning_rate': 3.488207879742722e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2921, 'grad_norm': 0.038284193724393845, 'learning_rate': 3.429346772085923e-05, 'epoch': 0.26}\n",
            "{'loss': 0.291, 'grad_norm': 0.03850811347365379, 'learning_rate': 3.367838843249222e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2798, 'grad_norm': 0.03965120017528534, 'learning_rate': 3.303797991757425e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2911, 'grad_norm': 0.041029468178749084, 'learning_rate': 3.237342806525007e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2715, 'grad_norm': 0.0392947718501091, 'learning_rate': 3.168596347256737e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2745, 'grad_norm': 0.04070858284831047, 'learning_rate': 3.097685916569439e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2895, 'grad_norm': 0.041382696479558945, 'learning_rate': 3.024742824256848e-05, 'epoch': 0.35}\n",
            "{'loss': 0.2771, 'grad_norm': 0.038860443979501724, 'learning_rate': 2.9499021441341012e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2844, 'grad_norm': 0.03811751306056976, 'learning_rate': 2.8733024639121283e-05, 'epoch': 0.37}\n",
            "{'loss': 0.2861, 'grad_norm': 0.040622446686029434, 'learning_rate': 2.7950856285651124e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2768, 'grad_norm': 0.03956688940525055, 'learning_rate': 2.7153964776662517e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2932, 'grad_norm': 0.039295095950365067, 'learning_rate': 2.6343825771782125e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2963, 'grad_norm': 0.03799743205308914, 'learning_rate': 2.5521939461949384e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2545, 'grad_norm': 0.03534664213657379, 'learning_rate': 2.4689827791408198e-05, 'epoch': 0.44}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04099860042333603, 'learning_rate': 2.38490316394166e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2856, 'grad_norm': 0.03930551931262016, 'learning_rate': 2.3001107966893054e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2884, 'grad_norm': 0.038180407136678696, 'learning_rate': 2.2147626933283265e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2869, 'grad_norm': 0.03729045018553734, 'learning_rate': 2.1290168988986332e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2796, 'grad_norm': 0.036218106746673584, 'learning_rate': 2.0430321948724447e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2815, 'grad_norm': 0.03836500644683838, 'learning_rate': 1.956967805127556e-05, 'epoch': 0.53}\n",
            "{'loss': 0.2722, 'grad_norm': 0.03780762106180191, 'learning_rate': 1.8709831011013678e-05, 'epoch': 0.54}\n",
            "{'loss': 0.2857, 'grad_norm': 0.03769194707274437, 'learning_rate': 1.785237306671674e-05, 'epoch': 0.55}\n",
            "{'loss': 0.2945, 'grad_norm': 0.03590088710188866, 'learning_rate': 1.699889203310695e-05, 'epoch': 0.57}\n",
            "{'loss': 0.2839, 'grad_norm': 0.038144227117300034, 'learning_rate': 1.6150968360583404e-05, 'epoch': 0.58}\n",
            "{'loss': 0.2821, 'grad_norm': 0.038355764001607895, 'learning_rate': 1.531017220859181e-05, 'epoch': 0.59}\n",
            "{'loss': 0.2789, 'grad_norm': 0.03852032124996185, 'learning_rate': 1.4478060538050622e-05, 'epoch': 0.61}\n",
            "{'loss': 0.2825, 'grad_norm': 0.03709693253040314, 'learning_rate': 1.3656174228217883e-05, 'epoch': 0.62}\n",
            "{'loss': 0.2839, 'grad_norm': 0.03669588267803192, 'learning_rate': 1.284603522333749e-05, 'epoch': 0.64}\n",
            "{'loss': 0.2961, 'grad_norm': 0.03711657226085663, 'learning_rate': 1.2049143714348886e-05, 'epoch': 0.65}\n",
            "{'loss': 0.2926, 'grad_norm': 0.037974730134010315, 'learning_rate': 1.1266975360878723e-05, 'epoch': 0.66}\n",
            "{'loss': 0.2823, 'grad_norm': 0.038249656558036804, 'learning_rate': 1.0500978558659001e-05, 'epoch': 0.68}\n",
            "{'loss': 0.2799, 'grad_norm': 0.03641022369265556, 'learning_rate': 9.752571757431528e-06, 'epoch': 0.69}\n",
            "{'loss': 0.3002, 'grad_norm': 0.03839483857154846, 'learning_rate': 9.023140834305621e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03784503787755966, 'learning_rate': 8.314036527432631e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2712, 'grad_norm': 0.03670426085591316, 'learning_rate': 7.6265719347499376e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2694, 'grad_norm': 0.03493485972285271, 'learning_rate': 6.962020082425749e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2821, 'grad_norm': 0.03648994490504265, 'learning_rate': 6.321611567507795e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2905, 'grad_norm': 0.037305064499378204, 'learning_rate': 5.706532279140782e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2913, 'grad_norm': 0.037264470010995865, 'learning_rate': 5.1179212025727935e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2873, 'grad_norm': 0.035737309604883194, 'learning_rate': 4.556868310016715e-06, 'epoch': 0.8}\n",
            "{'loss': 0.2575, 'grad_norm': 0.0357542522251606, 'learning_rate': 4.024412542272706e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2907, 'grad_norm': 0.03496134281158447, 'learning_rate': 3.5215398848489167e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2881, 'grad_norm': 0.03901565819978714, 'learning_rate': 3.0491815421433825e-06, 'epoch': 0.84}\n",
            "{'loss': 0.3001, 'grad_norm': 0.03671937435865402, 'learning_rate': 2.60821221306778e-06, 'epoch': 0.86}\n",
            "{'loss': 0.3048, 'grad_norm': 0.03727235645055771, 'learning_rate': 2.199448471306467e-06, 'epoch': 0.87}\n",
            "{'loss': 0.286, 'grad_norm': 0.04000743106007576, 'learning_rate': 1.8236472532099413e-06, 'epoch': 0.89}\n",
            "{'loss': 0.2688, 'grad_norm': 0.03843948617577553, 'learning_rate': 1.481504456123004e-06, 'epoch': 0.9}\n",
            "{'loss': 0.2933, 'grad_norm': 0.03710481524467468, 'learning_rate': 1.1736536497430584e-06, 'epoch': 0.91}\n",
            "{'loss': 0.3056, 'grad_norm': 0.04032738134264946, 'learning_rate': 9.006649028948966e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2638, 'grad_norm': 0.0361216776072979, 'learning_rate': 6.630437278944501e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2854, 'grad_norm': 0.03626224771142006, 'learning_rate': 4.6123014445636605e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2813, 'grad_norm': 0.034978412091732025, 'learning_rate': 2.9559786487878716e-07, 'epoch': 0.97}\n",
            "{'loss': 0.2726, 'grad_norm': 0.036137793213129044, 'learning_rate': 1.6645360201420046e-07, 'epoch': 0.98}\n",
            "{'loss': 0.2886, 'grad_norm': 0.040458787232637405, 'learning_rate': 7.403650130784368e-08, 'epoch': 1.0}\n",
            "{'loss': 0.2596, 'grad_norm': 0.06696213781833649, 'learning_rate': 1.851769795540026e-08, 'epoch': 1.0}\n",
            "{'train_runtime': 947.7788, 'train_samples_per_second': 9.762, 'train_steps_per_second': 0.077, 'train_loss': 0.28611210029419154, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1 into base model; saving to ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/lr_4e-5/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/lr_4e-5/models/model_iter_1/checkpoint-73 into base model; saving to ckpt/lr_4e-5/models/model_iter_1/checkpoint-73-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33micy-glade-8\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_133710-r4c7xt12/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/lr_4e-5\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9441 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/lr_4e-5/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/lr_4e-5/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/lr_4e-5/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_4e-5/rollout1\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "O-bVo6-LRqgS",
        "outputId": "1c7d7822-ab3c-4521-cfed-334a449d8bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "O-bVo6-LRqgS",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_4e-5/rollout8\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "W0GQ1QYVSKWp",
        "outputId": "ac3134d5-69f7-4059-8639-9fd95aedca5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W0GQ1QYVSKWp",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: scripts/run_gsm8k_eval.sh ckpt/lr_4e-5/models/model_iter_1-merged: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease lr (1e-5)"
      ],
      "metadata": {
        "id": "Mtlkrf5nNycx"
      },
      "id": "Mtlkrf5nNycx"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --learning_rate 1e-5 --run_name \"lr_1e-5\" --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9-qsGO3N29Z",
        "outputId": "c58a02bb-d863-4eb8-80ad-9d44a63d20c1",
        "collapsed": true
      },
      "id": "O9-qsGO3N29Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: lr_1e-5\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/lr_1e-5\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 1e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 64\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/lr_1e-5/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-15 13:53:32.948355: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:53:32.966430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763214812.988366    8870 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763214812.994983    8870 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763214813.012181    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012212    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012214    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214813.012216    8870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:53:33.017155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-15 13:53:40 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-15 13:53:42 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-15 13:53:59 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-15 13:53:59 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-15 13:54:02 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-15 13:54:02 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-15 13:54:07.246521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763214847.267603    9128 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763214847.273928    9128 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763214847.289686    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289716    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289718    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763214847.289720    9128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-15 13:54:14 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:16 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:16 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1115 13:54:18.852469933 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:18 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m WARNING 11-15 13:54:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:18 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:19 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:19 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.09it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.09it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:20 [default_loader.py:268] Loading weights took 0.94 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:21 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.157260 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:29 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7f14632f00/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:29 [backends.py:550] Dynamo bytecode transform time: 7.63 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.115 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:33 [monitor.py:34] torch.compile takes 7.63 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:34 [gpu_worker.py:298] Available KV cache memory: 29.11 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:35 [kv_cache_utils.py:864] GPU KV cache size: 272,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:35 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 6.65x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.85it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [gpu_worker.py:391] Free memory on device (39.07/39.56 GiB) on startup. Desired GPU memory utilization is (0.8, 31.65 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=30587899699` to fit into requested memory, or `--kv-cache-memory=38558571520` to fully utilize gpu memory. Current kv cache memory in use is 31252696883 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=9128)\u001b[0;0m INFO 11-15 13:54:39 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.37 seconds\n",
            "INFO 11-15 13:54:40 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-15 13:54:40 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 798.38it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [04:32<00:00, 58.81it/s, est. speed input: 5090.41 toks/s, output: 14579.85 toks/s]\n",
            "[rank0]:[W1115 13:59:15.078110102 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_1e-5/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9412\n",
            "Overall Accuracy: 58.83%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5905          0.5905         \n",
            "2     0.5905          0.7275         \n",
            "3     0.5885          0.7845         \n",
            "4     0.5895          0.8145         \n",
            "5     0.5894          0.8390         \n",
            "6     0.5897          0.8540         \n",
            "7     0.5886          0.8675         \n",
            "8     0.5883          0.8780         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/lr_1e-5/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/lr_1e-5/iteration_0/evaluation.jsonl\n",
            "Filtered 9412 correct examples out of 16000 total examples\n",
            "Accuracy: 58.83%\n",
            "Saved to ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9412\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/lr_1e-5/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-15 13:59:25.623569: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 13:59:25.642370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763215165.665067   10674 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763215165.671836   10674 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763215165.688885   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688918   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688920   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763215165.688922   10674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 13:59:25.694096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['k_proj', 'up_proj', 'gate_proj', 'v_proj', 'q_proj', 'down_proj', 'o_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9412 examples from ckpt/lr_1e-5/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9412/9412 [00:19<00:00, 475.60it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run mqr18hn0\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251115_140016-mqr18hn0\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run dainty-blaze-9\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/mqr18hn0\n",
            "Saved preprocessed features to ckpt/lr_1e-5/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9223, #eval 189\n",
            "100%|██████████| 73/73 [15:11<00:00, 12.48s/it]\n",
            "{'loss': 0.2697, 'grad_norm': 0.04213186353445053, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2882, 'grad_norm': 0.040083013474941254, 'learning_rate': 9.995370575511151e-06, 'epoch': 0.03}\n",
            "{'loss': 0.2821, 'grad_norm': 0.03980862721800804, 'learning_rate': 9.98149087467304e-06, 'epoch': 0.04}\n",
            "{'loss': 0.2831, 'grad_norm': 0.04026975855231285, 'learning_rate': 9.95838659949645e-06, 'epoch': 0.06}\n",
            "{'loss': 0.2713, 'grad_norm': 0.037654511630535126, 'learning_rate': 9.926100533780304e-06, 'epoch': 0.07}\n",
            "{'loss': 0.2994, 'grad_norm': 0.04277901351451874, 'learning_rate': 9.88469246388591e-06, 'epoch': 0.08}\n",
            "{'loss': 0.2954, 'grad_norm': 0.04307286813855171, 'learning_rate': 9.834239068026388e-06, 'epoch': 0.1}\n",
            "{'loss': 0.2854, 'grad_norm': 0.04598747938871384, 'learning_rate': 9.774833774276278e-06, 'epoch': 0.11}\n",
            "{'loss': 0.2869, 'grad_norm': 0.04217357933521271, 'learning_rate': 9.706586587564236e-06, 'epoch': 0.12}\n",
            "{'loss': 0.2881, 'grad_norm': 0.03806336596608162, 'learning_rate': 9.62962388596925e-06, 'epoch': 0.14}\n",
            "{'loss': 0.2878, 'grad_norm': 0.0416540764272213, 'learning_rate': 9.544088186697515e-06, 'epoch': 0.15}\n",
            "{'loss': 0.2747, 'grad_norm': 0.041370321065187454, 'learning_rate': 9.450137882173385e-06, 'epoch': 0.17}\n",
            "{'loss': 0.283, 'grad_norm': 0.04098854213953018, 'learning_rate': 9.347946946733055e-06, 'epoch': 0.18}\n",
            "{'loss': 0.2878, 'grad_norm': 0.04159264639019966, 'learning_rate': 9.237704614464157e-06, 'epoch': 0.19}\n",
            "{'loss': 0.303, 'grad_norm': 0.04671835899353027, 'learning_rate': 9.119615028787771e-06, 'epoch': 0.21}\n",
            "{'loss': 0.291, 'grad_norm': 0.04431857541203499, 'learning_rate': 8.993896864431825e-06, 'epoch': 0.22}\n",
            "{'loss': 0.2819, 'grad_norm': 0.04016101732850075, 'learning_rate': 8.860782922495821e-06, 'epoch': 0.24}\n",
            "{'loss': 0.275, 'grad_norm': 0.04002957418560982, 'learning_rate': 8.720519699356804e-06, 'epoch': 0.25}\n",
            "{'loss': 0.2851, 'grad_norm': 0.039889223873615265, 'learning_rate': 8.573366930214807e-06, 'epoch': 0.26}\n",
            "{'loss': 0.2852, 'grad_norm': 0.04091254249215126, 'learning_rate': 8.419597108123054e-06, 'epoch': 0.28}\n",
            "{'loss': 0.2915, 'grad_norm': 0.04555299133062363, 'learning_rate': 8.259494979393563e-06, 'epoch': 0.29}\n",
            "{'loss': 0.2868, 'grad_norm': 0.04143838211894035, 'learning_rate': 8.093357016312518e-06, 'epoch': 0.31}\n",
            "{'loss': 0.2949, 'grad_norm': 0.04202131927013397, 'learning_rate': 7.921490868141843e-06, 'epoch': 0.32}\n",
            "{'loss': 0.277, 'grad_norm': 0.046570565551519394, 'learning_rate': 7.744214791423597e-06, 'epoch': 0.33}\n",
            "{'loss': 0.2672, 'grad_norm': 0.04326683655381203, 'learning_rate': 7.56185706064212e-06, 'epoch': 0.35}\n",
            "{'loss': 0.2997, 'grad_norm': 0.041319336742162704, 'learning_rate': 7.374755360335253e-06, 'epoch': 0.36}\n",
            "{'loss': 0.2728, 'grad_norm': 0.03881301358342171, 'learning_rate': 7.183256159780321e-06, 'epoch': 0.37}\n",
            "{'loss': 0.2779, 'grad_norm': 0.04066222533583641, 'learning_rate': 6.987714071412781e-06, 'epoch': 0.39}\n",
            "{'loss': 0.2748, 'grad_norm': 0.04284864291548729, 'learning_rate': 6.788491194165629e-06, 'epoch': 0.4}\n",
            "{'loss': 0.2824, 'grad_norm': 0.04069862514734268, 'learning_rate': 6.585956442945531e-06, 'epoch': 0.42}\n",
            "{'loss': 0.2651, 'grad_norm': 0.03951702266931534, 'learning_rate': 6.380484865487346e-06, 'epoch': 0.43}\n",
            "{'loss': 0.3054, 'grad_norm': 0.04261735454201698, 'learning_rate': 6.1724569478520495e-06, 'epoch': 0.44}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04011780396103859, 'learning_rate': 5.96225790985415e-06, 'epoch': 0.46}\n",
            "{'loss': 0.2888, 'grad_norm': 0.04307740181684494, 'learning_rate': 5.7502769917232635e-06, 'epoch': 0.47}\n",
            "{'loss': 0.2701, 'grad_norm': 0.04128422960639, 'learning_rate': 5.536906733320816e-06, 'epoch': 0.49}\n",
            "{'loss': 0.2945, 'grad_norm': 0.06433846056461334, 'learning_rate': 5.322542247246583e-06, 'epoch': 0.5}\n",
            "{'loss': 0.3021, 'grad_norm': 0.041216108947992325, 'learning_rate': 5.107580487181112e-06, 'epoch': 0.51}\n",
            "{'loss': 0.2861, 'grad_norm': 0.04107237234711647, 'learning_rate': 4.89241951281889e-06, 'epoch': 0.53}\n",
            "{'loss': 0.304, 'grad_norm': 0.04232661798596382, 'learning_rate': 4.6774577527534195e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2816, 'grad_norm': 0.03968316316604614, 'learning_rate': 4.463093266679185e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2982, 'grad_norm': 0.04115461930632591, 'learning_rate': 4.249723008276737e-06, 'epoch': 0.57}\n",
            "{'loss': 0.29, 'grad_norm': 0.044513799250125885, 'learning_rate': 4.037742090145851e-06, 'epoch': 0.58}\n",
            "{'loss': 0.2818, 'grad_norm': 0.04048357158899307, 'learning_rate': 3.827543052147952e-06, 'epoch': 0.6}\n",
            "{'loss': 0.286, 'grad_norm': 0.04381868615746498, 'learning_rate': 3.6195151345126556e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2672, 'grad_norm': 0.0423961877822876, 'learning_rate': 3.4140435570544708e-06, 'epoch': 0.62}\n",
            "{'loss': 0.2935, 'grad_norm': 0.040342457592487335, 'learning_rate': 3.2115088058343725e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2939, 'grad_norm': 0.0420951321721077, 'learning_rate': 3.0122859285872214e-06, 'epoch': 0.65}\n",
            "{'loss': 0.2958, 'grad_norm': 0.03989747539162636, 'learning_rate': 2.816743840219681e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2883, 'grad_norm': 0.04243665933609009, 'learning_rate': 2.6252446396647503e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2709, 'grad_norm': 0.042038850486278534, 'learning_rate': 2.438142939357882e-06, 'epoch': 0.69}\n",
            "{'loss': 0.2908, 'grad_norm': 0.041530542075634, 'learning_rate': 2.2557852085764053e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2851, 'grad_norm': 0.04001827538013458, 'learning_rate': 2.0785091318581577e-06, 'epoch': 0.72}\n",
            "{'loss': 0.2922, 'grad_norm': 0.04072616621851921, 'learning_rate': 1.9066429836874844e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2755, 'grad_norm': 0.0405224934220314, 'learning_rate': 1.7405050206064372e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2923, 'grad_norm': 0.04025265574455261, 'learning_rate': 1.5804028918769488e-06, 'epoch': 0.76}\n",
            "{'loss': 0.2813, 'grad_norm': 0.04130794107913971, 'learning_rate': 1.4266330697851955e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2802, 'grad_norm': 0.040605466812849045, 'learning_rate': 1.2794803006431984e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2975, 'grad_norm': 0.04052789509296417, 'learning_rate': 1.1392170775041788e-06, 'epoch': 0.8}\n",
            "{'loss': 0.3015, 'grad_norm': 0.041520487517118454, 'learning_rate': 1.0061031355681766e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2772, 'grad_norm': 0.03887774795293808, 'learning_rate': 8.803849712122292e-07, 'epoch': 0.83}\n",
            "{'loss': 0.2771, 'grad_norm': 0.039884015917778015, 'learning_rate': 7.622953855358456e-07, 'epoch': 0.85}\n",
            "{'loss': 0.2947, 'grad_norm': 0.042108204215765, 'learning_rate': 6.52053053266945e-07, 'epoch': 0.86}\n",
            "{'loss': 0.2706, 'grad_norm': 0.04087476804852486, 'learning_rate': 5.498621178266167e-07, 'epoch': 0.87}\n",
            "{'loss': 0.3018, 'grad_norm': 0.043477654457092285, 'learning_rate': 4.5591181330248534e-07, 'epoch': 0.89}\n",
            "{'loss': 0.287, 'grad_norm': 0.040444277226924896, 'learning_rate': 3.70376114030751e-07, 'epoch': 0.9}\n",
            "{'loss': 0.2801, 'grad_norm': 0.04282611608505249, 'learning_rate': 2.934134124357646e-07, 'epoch': 0.92}\n",
            "{'loss': 0.3238, 'grad_norm': 0.041937582194805145, 'learning_rate': 2.2516622572372416e-07, 'epoch': 0.93}\n",
            "{'loss': 0.292, 'grad_norm': 0.04169221222400665, 'learning_rate': 1.6576093197361253e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2998, 'grad_norm': 0.04154568165540695, 'learning_rate': 1.1530753611409151e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2914, 'grad_norm': 0.04479105770587921, 'learning_rate': 7.389946621969679e-08, 'epoch': 0.97}\n",
            "{'loss': 0.2918, 'grad_norm': 0.040006961673498154, 'learning_rate': 4.1613400503550114e-08, 'epoch': 0.99}\n",
            "{'loss': 0.3057, 'grad_norm': 0.04510121047496796, 'learning_rate': 1.850912532696092e-08, 'epoch': 1.0}\n",
            "{'loss': 0.3602, 'grad_norm': 0.18706075847148895, 'learning_rate': 4.629424488850065e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 913.3499, 'train_samples_per_second': 10.098, 'train_steps_per_second': 0.08, 'train_loss': 0.2881258701624936, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1 into base model; saving to ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/lr_1e-5/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/lr_1e-5/models/model_iter_1/checkpoint-73 into base model; saving to ckpt/lr_1e-5/models/model_iter_1/checkpoint-73-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mdainty-blaze-9\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251115_140016-mqr18hn0/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/lr_1e-5\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9412 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/lr_1e-5/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/lr_1e-5/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/lr_1e-5/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_1e-5/rollout1\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "ib03Ntu7RyQI",
        "outputId": "80bce89e-aad2-4165-c0fd-b2e3106db3d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ib03Ntu7RyQI",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash \"scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged\" \\\n",
        "    --output_dir \"results/lr_1e-5/rollout8\" \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "7cKISfoiSEsm",
        "outputId": "f67433ab-32b7-4a12-fdca-dbac9a13337b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7cKISfoiSEsm",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: scripts/run_gsm8k_eval.sh ckpt/lr_1e-5/models/model_iter_1-merged: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Increase LoRA rank (128)"
      ],
      "metadata": {
        "id": "_lldD-rLOM9F"
      },
      "id": "_lldD-rLOM9F"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --lora_r 128 --run_name rank_128 --batch_size_per_dev 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYmbAiCRORl0",
        "outputId": "5b09d23e-f37e-4f80-fa8e-4608c0582b46",
        "collapsed": true
      },
      "id": "rYmbAiCRORl0",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: rank_128\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/rank_128\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 128\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/rank_128/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-17 07:28:52.709091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 07:28:52.726733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763364532.749315    1541 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763364532.755916    1541 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763364532.772717    1541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364532.772758    1541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364532.772760    1541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364532.772762    1541 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 07:28:52.777766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 07:29:01 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 07:29:12 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-17 07:29:30 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 07:29:30 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 07:29:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 07:29:34 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 07:29:38.744071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763364578.765873    1841 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763364578.772372    1841 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763364578.788615    1841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364578.788647    1841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364578.788649    1841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763364578.788650    1841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 07:29:46 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:47 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 07:29:50.415557742 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:50 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m WARNING 11-17 07:29:50 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:50 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:50 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:29:51 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:30<00:00, 30.66s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:30<00:00, 30.66s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:30:21 [default_loader.py:268] Loading weights took 30.69 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:30:22 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 31.081247 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:30:31 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/44c42810b9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:30:31 [backends.py:550] Dynamo bytecode transform time: 8.45 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m [rank0]:W1117 07:30:33.032000 1841 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:30:38 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:05 [backends.py:215] Compiling a graph for dynamic shape takes 33.10 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:08 [monitor.py:34] torch.compile takes 41.55 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:10 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:10 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:10 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.22it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:15 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:15 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15646582988` to fit into requested memory, or `--kv-cache-memory=20175461376` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=1841)\u001b[0;0m INFO 11-17 07:31:15 [core.py:218] init engine (profile, create kv cache, warmup model) took 52.44 seconds\n",
            "INFO 11-17 07:31:16 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 07:31:16 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:01<00:00, 1033.69it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [13:13<00:00, 20.17it/s, est. speed input: 1745.85 toks/s, output: 5021.61 toks/s]\n",
            "[rank0]:[W1117 07:44:33.253780338 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_128/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9365\n",
            "Overall Accuracy: 58.53%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5955          0.5955         \n",
            "2     0.5905          0.7275         \n",
            "3     0.5912          0.7860         \n",
            "4     0.5844          0.8130         \n",
            "5     0.5838          0.8330         \n",
            "6     0.5858          0.8470         \n",
            "7     0.5864          0.8610         \n",
            "8     0.5853          0.8710         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/rank_128/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_128/iteration_0/evaluation.jsonl\n",
            "Filtered 9365 correct examples out of 16000 total examples\n",
            "Accuracy: 58.53%\n",
            "Saved to ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9365\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/rank_128/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-17 07:44:49.581007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 07:44:49.598918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763365489.621110    6245 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763365489.627775    6245 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763365489.644394    6245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763365489.644432    6245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763365489.644434    6245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763365489.644436    6245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 07:44:49.649366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['gate_proj', 'o_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj', 'up_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9365 examples from ckpt/rank_128/iteration_0/correct_examples.jsonl\n",
            "Cache metadata mismatch for key 'data_mtime': expected 1763365483.0, found 1763216512.0. Regenerating.\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9365/9365 [00:20<00:00, 460.53it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251117_074552-uxqaw9o5\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run revived-dragon-11\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/uxqaw9o5\n",
            "Saved preprocessed features to ckpt/rank_128/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9177, #eval 188\n",
            "100%|██████████| 72/72 [35:41<00:00, 29.74s/it]\n",
            "{'loss': 0.2679, 'grad_norm': 0.030542530119419098, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2881, 'grad_norm': 0.030264368280768394, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2751, 'grad_norm': 0.02734530344605446, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2892, 'grad_norm': 0.02883700281381607, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.06}\n",
            "{'loss': 0.269, 'grad_norm': 0.029183153063058853, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2968, 'grad_norm': 0.030664069578051567, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.3174, 'grad_norm': 0.02927273139357567, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2888, 'grad_norm': 0.028228679671883583, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.11}\n",
            "{'loss': 0.2758, 'grad_norm': 0.027431093156337738, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2981, 'grad_norm': 0.029321562498807907, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.14}\n",
            "{'loss': 0.2817, 'grad_norm': 0.028909290209412575, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2944, 'grad_norm': 0.02858049049973488, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.3074, 'grad_norm': 0.03146662935614586, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2901, 'grad_norm': 0.029500186443328857, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2766, 'grad_norm': 0.028358854353427887, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2723, 'grad_norm': 0.028166482225060463, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2726, 'grad_norm': 0.029874851927161217, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.24}\n",
            "{'loss': 0.296, 'grad_norm': 0.030813615769147873, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.25}\n",
            "{'loss': 0.3121, 'grad_norm': 0.02970985136926174, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2995, 'grad_norm': 0.030397562310099602, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2696, 'grad_norm': 0.029974620789289474, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2937, 'grad_norm': 0.02868441678583622, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2941, 'grad_norm': 0.031472865492105484, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2957, 'grad_norm': 0.030425293371081352, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.33}\n",
            "{'loss': 0.2781, 'grad_norm': 0.028975369408726692, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}\n",
            "{'loss': 0.281, 'grad_norm': 0.02818034216761589, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2909, 'grad_norm': 0.030309993773698807, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2843, 'grad_norm': 0.030359230935573578, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2918, 'grad_norm': 0.027588585391640663, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.4}\n",
            "{'loss': 0.2894, 'grad_norm': 0.027972187846899033, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2835, 'grad_norm': 0.03062865510582924, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2807, 'grad_norm': 0.028218822553753853, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2928, 'grad_norm': 0.029131153598427773, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2697, 'grad_norm': 0.030160579830408096, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.47}\n",
            "{'loss': 0.2816, 'grad_norm': 0.026893071830272675, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2792, 'grad_norm': 0.027473609894514084, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.5}\n",
            "{'loss': 0.2979, 'grad_norm': 0.03164174035191536, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2852, 'grad_norm': 0.02696535922586918, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.53}\n",
            "{'loss': 0.2869, 'grad_norm': 0.02794608287513256, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2771, 'grad_norm': 0.02640259452164173, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2815, 'grad_norm': 0.028048837557435036, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2888, 'grad_norm': 0.02919810265302658, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.59}\n",
            "{'loss': 0.2924, 'grad_norm': 0.028473803773522377, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2785, 'grad_norm': 0.027109436690807343, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2925, 'grad_norm': 0.028507549315690994, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2984, 'grad_norm': 0.026363052427768707, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2996, 'grad_norm': 0.02795454114675522, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2949, 'grad_norm': 0.028090236708521843, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2924, 'grad_norm': 0.028033647686243057, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}\n",
            "{'loss': 0.279, 'grad_norm': 0.02959713526070118, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2837, 'grad_norm': 0.02766365557909012, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2858, 'grad_norm': 0.030438730493187904, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.73}\n",
            "{'loss': 0.288, 'grad_norm': 0.027792206034064293, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2889, 'grad_norm': 0.027796944603323936, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2917, 'grad_norm': 0.031068529933691025, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.77}\n",
            "{'loss': 0.3054, 'grad_norm': 0.029376933351159096, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2863, 'grad_norm': 0.02858792059123516, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.79}\n",
            "{'loss': 0.2838, 'grad_norm': 0.03014492429792881, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2994, 'grad_norm': 0.027562228962779045, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2949, 'grad_norm': 0.030924567952752113, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2874, 'grad_norm': 0.027526576071977615, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.85}\n",
            "{'loss': 0.2959, 'grad_norm': 0.02810351364314556, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.86}\n",
            "{'loss': 0.2801, 'grad_norm': 0.030594829469919205, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2874, 'grad_norm': 0.027462586760520935, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2924, 'grad_norm': 0.03031613491475582, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2929, 'grad_norm': 0.031606148928403854, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2649, 'grad_norm': 0.0277793500572443, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.93}\n",
            "{'loss': 0.2877, 'grad_norm': 0.027521789073944092, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.95}\n",
            "{'loss': 0.278, 'grad_norm': 0.029140394181013107, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2932, 'grad_norm': 0.028025591745972633, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.98}\n",
            "{'loss': 0.3005, 'grad_norm': 0.029442936182022095, 'learning_rate': 3.805301908254455e-08, 'epoch': 0.99}\n",
            "{'loss': 0.2795, 'grad_norm': 0.033569756895303726, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 2154.0516, 'train_samples_per_second': 4.26, 'train_steps_per_second': 0.033, 'train_loss': 0.2877439479860995, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/rank_128/models/model_iter_1 into base model; saving to ckpt/rank_128/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/rank_128/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/rank_128/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/rank_128/models/model_iter_1/checkpoint-72 into base model; saving to ckpt/rank_128/models/model_iter_1/checkpoint-72-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mrevived-dragon-11\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251117_074552-uxqaw9o5/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/rank_128/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/rank_128\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9365 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/rank_128/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/rank_128/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/rank_128/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_128/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "FOtwG7IbR1ek",
        "outputId": "47682843-8db3-4eb3-b766-1883756587e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FOtwG7IbR1ek",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/rank_128/models/model_iter_1-merged\n",
            "Output Directory: results/rank_128/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_113406\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/rank_128/rollout1/model_iter_1-merged_20251117_113406_inference.jsonl\n",
            "\n",
            "2025-11-17 11:34:10.648773: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:34:10.667841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763379250.690664   68301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763379250.697503   68301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763379250.715070   68301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379250.715105   68301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379250.715107   68301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379250.715109   68301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:34:10.720187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:34:18 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:34:22 [utils.py:328] non-default args: {'tokenizer': 'ckpt/rank_128/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/rank_128/models/model_iter_1-merged'}\n",
            "INFO 11-17 11:34:39 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 11:34:39 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 11:34:41 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 11:34:42 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 11:34:47.305020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763379287.326414   68555 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763379287.332842   68555 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763379287.348620   68555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379287.348656   68555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379287.348659   68555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379287.348660   68555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 11:34:55 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:56 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/rank_128/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/rank_128/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/rank_128/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 11:34:59.149645619 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:59 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m WARNING 11-17 11:34:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:59 [gpu_model_runner.py:2338] Starting to load model ckpt/rank_128/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:59 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:34:59 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.16s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.16s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:11 [default_loader.py:268] Loading weights took 12.18 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:12 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 12.396043 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:21 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/c3a3b99c15/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:21 [backends.py:550] Dynamo bytecode transform time: 7.88 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:21 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:27 [backends.py:215] Compiling a graph for dynamic shape takes 5.76 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:28 [monitor.py:34] torch.compile takes 13.64 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:29 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:30 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:30 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.26it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:34 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:34 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=68555)\u001b[0;0m INFO 11-17 11:35:34 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.85 seconds\n",
            "INFO 11-17 11:35:35 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 11:35:35 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1740.66it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:19<00:00,  9.45it/s, est. speed input: 834.46 toks/s, output: 2487.62 toks/s]\n",
            "[rank0]:[W1117 11:37:56.607317005 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/rank_128/rollout1/model_iter_1-merged_20251117_113406_evaluation.jsonl\n",
            "\n",
            "Loading data from results/rank_128/rollout1/model_iter_1-merged_20251117_113406_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 786\n",
            "Overall Accuracy: 59.59%\n",
            "\n",
            "Results saved to results/rank_128/rollout1/model_iter_1-merged_20251117_113406_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/rank_128/rollout1/model_iter_1-merged_20251117_113406_inference.jsonl\n",
            "  - Evaluation results: results/rank_128/rollout1/model_iter_1-merged_20251117_113406_evaluation.jsonl\n",
            "  - Log file: results/rank_128/rollout1/model_iter_1-merged_20251117_113406.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_128/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_128/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "rjh0rzoZSD3l",
        "outputId": "4555f9fb-4d11-4cc8-db82-b75107dfd32b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rjh0rzoZSD3l",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/rank_128/models/model_iter_1-merged\n",
            "Output Directory: results/rank_128/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_113759\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/rank_128/rollout8/model_iter_1-merged_20251117_113759_inference.jsonl\n",
            "\n",
            "2025-11-17 11:38:04.510166: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:38:04.528546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763379484.550451   69625 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763379484.557172   69625 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763379484.573980   69625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379484.574015   69625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379484.574017   69625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379484.574019   69625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:38:04.578911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:38:12 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:38:14 [utils.py:328] non-default args: {'tokenizer': 'ckpt/rank_128/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/rank_128/models/model_iter_1-merged'}\n",
            "INFO 11-17 11:38:30 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 11:38:30 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 11:38:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 11:38:33 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 11:38:37.855771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763379517.877702   69865 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763379517.884937   69865 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763379517.902560   69865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379517.902599   69865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379517.902602   69865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763379517.902604   69865 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 11:38:45 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:47 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/rank_128/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/rank_128/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/rank_128/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 11:38:49.680517931 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:49 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m WARNING 11-17 11:38:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:49 [gpu_model_runner.py:2338] Starting to load model ckpt/rank_128/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:50 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:50 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:51 [default_loader.py:268] Loading weights took 0.86 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:38:52 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.071470 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:00 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/c3a3b99c15/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:00 [backends.py:550] Dynamo bytecode transform time: 7.92 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:03 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.180 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:05 [monitor.py:34] torch.compile takes 7.92 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:06 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:06 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:06 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.10it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:11 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:11 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=69865)\u001b[0;0m INFO 11-17 11:39:11 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.13 seconds\n",
            "INFO 11-17 11:39:12 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 11:39:12 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1101.49it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:44<00:00, 10.50it/s, est. speed input: 927.42 toks/s, output: 2744.42 toks/s]\n",
            "[rank0]:[W1117 11:55:58.881216477 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/rank_128/rollout8/model_iter_1-merged_20251117_113759_evaluation.jsonl\n",
            "\n",
            "Loading data from results/rank_128/rollout8/model_iter_1-merged_20251117_113759_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6322\n",
            "Overall Accuracy: 59.91%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5868          0.5868         \n",
            "2     0.5876          0.6967         \n",
            "3     0.5896          0.7453         \n",
            "4     0.5933          0.7817         \n",
            "5     0.5959          0.8014         \n",
            "6     0.5964          0.8188         \n",
            "7     0.5973          0.8287         \n",
            "8     0.5991          0.8415         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/rank_128/rollout8/model_iter_1-merged_20251117_113759_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/rank_128/rollout8/model_iter_1-merged_20251117_113759_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/rank_128/rollout8/model_iter_1-merged_20251117_113759_inference.jsonl\n",
            "  - Evaluation results: results/rank_128/rollout8/model_iter_1-merged_20251117_113759_evaluation.jsonl\n",
            "  - Log file: results/rank_128/rollout8/model_iter_1-merged_20251117_113759.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease LoRA rank (32)"
      ],
      "metadata": {
        "id": "8mCBekXZOhxf"
      },
      "id": "8mCBekXZOhxf"
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && \\\n",
        "export WANDB_API_KEY=\"cdb87cd68d7a18cd6d0a4e34d52b1ef4b41c20e3\" && \\\n",
        "export WANDB_PROJECT=\"COMP4901B-Homework3\" && \\\n",
        "export CUDA_VISIBLE_DEVICES=\"0\" && \\\n",
        "bash scripts/self_train_gsm8k.sh Qwen3-0.6B --lora_r 32 --run_name rank_32 --batch_size_per_dev 4"
      ],
      "metadata": {
        "id": "iflfrd2GOkdW",
        "collapsed": true,
        "outputId": "32d015f9-d38e-4b9a-c9cf-e95d7a5cc9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iflfrd2GOkdW",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Self-Improving Training Pipeline\n",
            "========================================\n",
            "Run Name: rank_32\n",
            "Initial Model: Qwen3-0.6B\n",
            "Pipeline Directory: ckpt/rank_32\n",
            "Number of Iterations: 1\n",
            "\n",
            "Inference Settings:\n",
            "  Dataset Split: train\n",
            "  Number of Queries: 2000\n",
            "  Max Tokens: 512\n",
            "  Temperature: 1.0\n",
            "  Top-p: 1\n",
            "  Top-k: -1\n",
            "  Number of Rollouts: 8\n",
            "  Tensor Parallel: 1\n",
            "  Mode: Zero-shot\n",
            "  Chat Template: Enabled\n",
            "  Thinking Mode: Disabled\n",
            "\n",
            "Training Settings:\n",
            "  Learning Rate: 2e-5\n",
            "  Total Batch Size: 128\n",
            "  Batch Size Per Device: 4\n",
            "  Gradient Accumulation Steps: 32 (auto-calculated)\n",
            "  Number of Epochs: 1\n",
            "  Save Steps: 30\n",
            "  LoRA Rank: 32\n",
            "  Model Max Length: 712 (auto-calculated: MAX_TOKENS + 200)\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "ITERATION 0 / 0\n",
            "========================================\n",
            "Model: Qwen3-0.6B\n",
            "\n",
            "[Step 1/4] Running inference...\n",
            "Output: ckpt/rank_32/iteration_0/inference.jsonl\n",
            "\n",
            "2025-11-17 08:22:23.224842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 08:22:23.242487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763367743.264379   15886 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763367743.270910   15886 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763367743.287229   15886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367743.287264   15886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367743.287266   15886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367743.287268   15886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 08:22:23.292143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 08:22:30 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 08:22:33 [utils.py:328] non-default args: {'tokenizer': 'Qwen3-0.6B', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen3-0.6B'}\n",
            "INFO 11-17 08:22:50 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 08:22:50 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 08:22:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 08:22:53 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 08:22:58.191738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763367778.215943   16150 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763367778.223089   16150 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763367778.242006   16150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367778.242038   16150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367778.242040   16150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763367778.242042   16150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 08:23:06 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:07 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:07 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen3-0.6B', speculative_config=None, tokenizer='Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 08:23:10.151698211 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:10 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m WARNING 11-17 08:23:10 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:10 [gpu_model_runner.py:2338] Starting to load model Qwen3-0.6B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:10 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:10 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:11 [default_loader.py:268] Loading weights took 0.86 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:12 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.070232 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:20 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/44c42810b9/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:20 [backends.py:550] Dynamo bytecode transform time: 7.84 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:24 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.138 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:25 [monitor.py:34] torch.compile takes 7.84 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:26 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:27 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:27 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.51it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:31 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:31 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=16150)\u001b[0;0m INFO 11-17 08:23:31 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.87 seconds\n",
            "INFO 11-17 08:23:32 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 08:23:32 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 2000/2000 [00:02<00:00, 863.59it/s]\n",
            "Processed prompts: 100%|██████████| 16000/16000 [13:07<00:00, 20.32it/s, est. speed input: 1758.74 toks/s, output: 5030.69 toks/s]\n",
            "[rank0]:[W1117 08:36:42.007938854 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "✓ Inference completed\n",
            "\n",
            "[Step 2/4] Running evaluation...\n",
            "Output: ckpt/rank_32/iteration_0/evaluation.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_32/iteration_0/inference.jsonl\n",
            "Loaded 16000 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 16000\n",
            "Correct answers: 9346\n",
            "Overall Accuracy: 58.41%\n",
            "\n",
            "Detected multiple rollouts: 2000 unique questions with 16000 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5910          0.5910         \n",
            "2     0.5885          0.7240         \n",
            "3     0.5873          0.7775         \n",
            "4     0.5820          0.8065         \n",
            "5     0.5817          0.8335         \n",
            "6     0.5844          0.8515         \n",
            "7     0.5847          0.8620         \n",
            "8     0.5841          0.8690         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to ckpt/rank_32/iteration_0/evaluation_metrics.json\n",
            "\n",
            "Results saved to ckpt/rank_32/iteration_0/evaluation.jsonl\n",
            "✓ Evaluation completed\n",
            "\n",
            "[Step 3/4] Filtering correct examples...\n",
            "Output: ckpt/rank_32/iteration_0/correct_examples.jsonl\n",
            "\n",
            "Loading data from ckpt/rank_32/iteration_0/evaluation.jsonl\n",
            "Filtered 9346 correct examples out of 16000 total examples\n",
            "Accuracy: 58.41%\n",
            "Saved to ckpt/rank_32/iteration_0/correct_examples.jsonl\n",
            "Number of correct examples: 9346\n",
            "✓ Filtering completed\n",
            "\n",
            "[Step 4/4] Training on correct examples...\n",
            "Training data: ckpt/rank_32/iteration_0/correct_examples.jsonl\n",
            "New model will be saved to: ckpt/rank_32/models/model_iter_1\n",
            "\n",
            "\n",
            "2025-11-17 08:36:52.855936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 08:36:52.874046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763368612.896807   19791 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763368612.903489   19791 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763368612.920297   19791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368612.920341   19791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368612.920343   19791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763368612.920345   19791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 08:36:52.925348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading model in torch.bfloat16 precision.\n",
            "Found linear module: model.layers.0.self_attn.q_proj\n",
            "Found linear module: model.layers.0.self_attn.k_proj\n",
            "Found linear module: model.layers.0.self_attn.v_proj\n",
            "Found linear module: model.layers.0.self_attn.o_proj\n",
            "Found linear module: model.layers.0.mlp.gate_proj\n",
            "Found linear module: model.layers.0.mlp.up_proj\n",
            "Found linear module: model.layers.0.mlp.down_proj\n",
            "Found linear module: model.layers.1.self_attn.q_proj\n",
            "Found linear module: model.layers.1.self_attn.k_proj\n",
            "Found linear module: model.layers.1.self_attn.v_proj\n",
            "Found linear module: model.layers.1.self_attn.o_proj\n",
            "Found linear module: model.layers.1.mlp.gate_proj\n",
            "Found linear module: model.layers.1.mlp.up_proj\n",
            "Found linear module: model.layers.1.mlp.down_proj\n",
            "Found linear module: model.layers.2.self_attn.q_proj\n",
            "Found linear module: model.layers.2.self_attn.k_proj\n",
            "Found linear module: model.layers.2.self_attn.v_proj\n",
            "Found linear module: model.layers.2.self_attn.o_proj\n",
            "Found linear module: model.layers.2.mlp.gate_proj\n",
            "Found linear module: model.layers.2.mlp.up_proj\n",
            "Found linear module: model.layers.2.mlp.down_proj\n",
            "Found linear module: model.layers.3.self_attn.q_proj\n",
            "Found linear module: model.layers.3.self_attn.k_proj\n",
            "Found linear module: model.layers.3.self_attn.v_proj\n",
            "Found linear module: model.layers.3.self_attn.o_proj\n",
            "Found linear module: model.layers.3.mlp.gate_proj\n",
            "Found linear module: model.layers.3.mlp.up_proj\n",
            "Found linear module: model.layers.3.mlp.down_proj\n",
            "Found linear module: model.layers.4.self_attn.q_proj\n",
            "Found linear module: model.layers.4.self_attn.k_proj\n",
            "Found linear module: model.layers.4.self_attn.v_proj\n",
            "Found linear module: model.layers.4.self_attn.o_proj\n",
            "Found linear module: model.layers.4.mlp.gate_proj\n",
            "Found linear module: model.layers.4.mlp.up_proj\n",
            "Found linear module: model.layers.4.mlp.down_proj\n",
            "Found linear module: model.layers.5.self_attn.q_proj\n",
            "Found linear module: model.layers.5.self_attn.k_proj\n",
            "Found linear module: model.layers.5.self_attn.v_proj\n",
            "Found linear module: model.layers.5.self_attn.o_proj\n",
            "Found linear module: model.layers.5.mlp.gate_proj\n",
            "Found linear module: model.layers.5.mlp.up_proj\n",
            "Found linear module: model.layers.5.mlp.down_proj\n",
            "Found linear module: model.layers.6.self_attn.q_proj\n",
            "Found linear module: model.layers.6.self_attn.k_proj\n",
            "Found linear module: model.layers.6.self_attn.v_proj\n",
            "Found linear module: model.layers.6.self_attn.o_proj\n",
            "Found linear module: model.layers.6.mlp.gate_proj\n",
            "Found linear module: model.layers.6.mlp.up_proj\n",
            "Found linear module: model.layers.6.mlp.down_proj\n",
            "Found linear module: model.layers.7.self_attn.q_proj\n",
            "Found linear module: model.layers.7.self_attn.k_proj\n",
            "Found linear module: model.layers.7.self_attn.v_proj\n",
            "Found linear module: model.layers.7.self_attn.o_proj\n",
            "Found linear module: model.layers.7.mlp.gate_proj\n",
            "Found linear module: model.layers.7.mlp.up_proj\n",
            "Found linear module: model.layers.7.mlp.down_proj\n",
            "Found linear module: model.layers.8.self_attn.q_proj\n",
            "Found linear module: model.layers.8.self_attn.k_proj\n",
            "Found linear module: model.layers.8.self_attn.v_proj\n",
            "Found linear module: model.layers.8.self_attn.o_proj\n",
            "Found linear module: model.layers.8.mlp.gate_proj\n",
            "Found linear module: model.layers.8.mlp.up_proj\n",
            "Found linear module: model.layers.8.mlp.down_proj\n",
            "Found linear module: model.layers.9.self_attn.q_proj\n",
            "Found linear module: model.layers.9.self_attn.k_proj\n",
            "Found linear module: model.layers.9.self_attn.v_proj\n",
            "Found linear module: model.layers.9.self_attn.o_proj\n",
            "Found linear module: model.layers.9.mlp.gate_proj\n",
            "Found linear module: model.layers.9.mlp.up_proj\n",
            "Found linear module: model.layers.9.mlp.down_proj\n",
            "Found linear module: model.layers.10.self_attn.q_proj\n",
            "Found linear module: model.layers.10.self_attn.k_proj\n",
            "Found linear module: model.layers.10.self_attn.v_proj\n",
            "Found linear module: model.layers.10.self_attn.o_proj\n",
            "Found linear module: model.layers.10.mlp.gate_proj\n",
            "Found linear module: model.layers.10.mlp.up_proj\n",
            "Found linear module: model.layers.10.mlp.down_proj\n",
            "Found linear module: model.layers.11.self_attn.q_proj\n",
            "Found linear module: model.layers.11.self_attn.k_proj\n",
            "Found linear module: model.layers.11.self_attn.v_proj\n",
            "Found linear module: model.layers.11.self_attn.o_proj\n",
            "Found linear module: model.layers.11.mlp.gate_proj\n",
            "Found linear module: model.layers.11.mlp.up_proj\n",
            "Found linear module: model.layers.11.mlp.down_proj\n",
            "Found linear module: model.layers.12.self_attn.q_proj\n",
            "Found linear module: model.layers.12.self_attn.k_proj\n",
            "Found linear module: model.layers.12.self_attn.v_proj\n",
            "Found linear module: model.layers.12.self_attn.o_proj\n",
            "Found linear module: model.layers.12.mlp.gate_proj\n",
            "Found linear module: model.layers.12.mlp.up_proj\n",
            "Found linear module: model.layers.12.mlp.down_proj\n",
            "Found linear module: model.layers.13.self_attn.q_proj\n",
            "Found linear module: model.layers.13.self_attn.k_proj\n",
            "Found linear module: model.layers.13.self_attn.v_proj\n",
            "Found linear module: model.layers.13.self_attn.o_proj\n",
            "Found linear module: model.layers.13.mlp.gate_proj\n",
            "Found linear module: model.layers.13.mlp.up_proj\n",
            "Found linear module: model.layers.13.mlp.down_proj\n",
            "Found linear module: model.layers.14.self_attn.q_proj\n",
            "Found linear module: model.layers.14.self_attn.k_proj\n",
            "Found linear module: model.layers.14.self_attn.v_proj\n",
            "Found linear module: model.layers.14.self_attn.o_proj\n",
            "Found linear module: model.layers.14.mlp.gate_proj\n",
            "Found linear module: model.layers.14.mlp.up_proj\n",
            "Found linear module: model.layers.14.mlp.down_proj\n",
            "Found linear module: model.layers.15.self_attn.q_proj\n",
            "Found linear module: model.layers.15.self_attn.k_proj\n",
            "Found linear module: model.layers.15.self_attn.v_proj\n",
            "Found linear module: model.layers.15.self_attn.o_proj\n",
            "Found linear module: model.layers.15.mlp.gate_proj\n",
            "Found linear module: model.layers.15.mlp.up_proj\n",
            "Found linear module: model.layers.15.mlp.down_proj\n",
            "Found linear module: model.layers.16.self_attn.q_proj\n",
            "Found linear module: model.layers.16.self_attn.k_proj\n",
            "Found linear module: model.layers.16.self_attn.v_proj\n",
            "Found linear module: model.layers.16.self_attn.o_proj\n",
            "Found linear module: model.layers.16.mlp.gate_proj\n",
            "Found linear module: model.layers.16.mlp.up_proj\n",
            "Found linear module: model.layers.16.mlp.down_proj\n",
            "Found linear module: model.layers.17.self_attn.q_proj\n",
            "Found linear module: model.layers.17.self_attn.k_proj\n",
            "Found linear module: model.layers.17.self_attn.v_proj\n",
            "Found linear module: model.layers.17.self_attn.o_proj\n",
            "Found linear module: model.layers.17.mlp.gate_proj\n",
            "Found linear module: model.layers.17.mlp.up_proj\n",
            "Found linear module: model.layers.17.mlp.down_proj\n",
            "Found linear module: model.layers.18.self_attn.q_proj\n",
            "Found linear module: model.layers.18.self_attn.k_proj\n",
            "Found linear module: model.layers.18.self_attn.v_proj\n",
            "Found linear module: model.layers.18.self_attn.o_proj\n",
            "Found linear module: model.layers.18.mlp.gate_proj\n",
            "Found linear module: model.layers.18.mlp.up_proj\n",
            "Found linear module: model.layers.18.mlp.down_proj\n",
            "Found linear module: model.layers.19.self_attn.q_proj\n",
            "Found linear module: model.layers.19.self_attn.k_proj\n",
            "Found linear module: model.layers.19.self_attn.v_proj\n",
            "Found linear module: model.layers.19.self_attn.o_proj\n",
            "Found linear module: model.layers.19.mlp.gate_proj\n",
            "Found linear module: model.layers.19.mlp.up_proj\n",
            "Found linear module: model.layers.19.mlp.down_proj\n",
            "Found linear module: model.layers.20.self_attn.q_proj\n",
            "Found linear module: model.layers.20.self_attn.k_proj\n",
            "Found linear module: model.layers.20.self_attn.v_proj\n",
            "Found linear module: model.layers.20.self_attn.o_proj\n",
            "Found linear module: model.layers.20.mlp.gate_proj\n",
            "Found linear module: model.layers.20.mlp.up_proj\n",
            "Found linear module: model.layers.20.mlp.down_proj\n",
            "Found linear module: model.layers.21.self_attn.q_proj\n",
            "Found linear module: model.layers.21.self_attn.k_proj\n",
            "Found linear module: model.layers.21.self_attn.v_proj\n",
            "Found linear module: model.layers.21.self_attn.o_proj\n",
            "Found linear module: model.layers.21.mlp.gate_proj\n",
            "Found linear module: model.layers.21.mlp.up_proj\n",
            "Found linear module: model.layers.21.mlp.down_proj\n",
            "Found linear module: model.layers.22.self_attn.q_proj\n",
            "Found linear module: model.layers.22.self_attn.k_proj\n",
            "Found linear module: model.layers.22.self_attn.v_proj\n",
            "Found linear module: model.layers.22.self_attn.o_proj\n",
            "Found linear module: model.layers.22.mlp.gate_proj\n",
            "Found linear module: model.layers.22.mlp.up_proj\n",
            "Found linear module: model.layers.22.mlp.down_proj\n",
            "Found linear module: model.layers.23.self_attn.q_proj\n",
            "Found linear module: model.layers.23.self_attn.k_proj\n",
            "Found linear module: model.layers.23.self_attn.v_proj\n",
            "Found linear module: model.layers.23.self_attn.o_proj\n",
            "Found linear module: model.layers.23.mlp.gate_proj\n",
            "Found linear module: model.layers.23.mlp.up_proj\n",
            "Found linear module: model.layers.23.mlp.down_proj\n",
            "Found linear module: model.layers.24.self_attn.q_proj\n",
            "Found linear module: model.layers.24.self_attn.k_proj\n",
            "Found linear module: model.layers.24.self_attn.v_proj\n",
            "Found linear module: model.layers.24.self_attn.o_proj\n",
            "Found linear module: model.layers.24.mlp.gate_proj\n",
            "Found linear module: model.layers.24.mlp.up_proj\n",
            "Found linear module: model.layers.24.mlp.down_proj\n",
            "Found linear module: model.layers.25.self_attn.q_proj\n",
            "Found linear module: model.layers.25.self_attn.k_proj\n",
            "Found linear module: model.layers.25.self_attn.v_proj\n",
            "Found linear module: model.layers.25.self_attn.o_proj\n",
            "Found linear module: model.layers.25.mlp.gate_proj\n",
            "Found linear module: model.layers.25.mlp.up_proj\n",
            "Found linear module: model.layers.25.mlp.down_proj\n",
            "Found linear module: model.layers.26.self_attn.q_proj\n",
            "Found linear module: model.layers.26.self_attn.k_proj\n",
            "Found linear module: model.layers.26.self_attn.v_proj\n",
            "Found linear module: model.layers.26.self_attn.o_proj\n",
            "Found linear module: model.layers.26.mlp.gate_proj\n",
            "Found linear module: model.layers.26.mlp.up_proj\n",
            "Found linear module: model.layers.26.mlp.down_proj\n",
            "Found linear module: model.layers.27.self_attn.q_proj\n",
            "Found linear module: model.layers.27.self_attn.k_proj\n",
            "Found linear module: model.layers.27.self_attn.v_proj\n",
            "Found linear module: model.layers.27.self_attn.o_proj\n",
            "Found linear module: model.layers.27.mlp.gate_proj\n",
            "Found linear module: model.layers.27.mlp.up_proj\n",
            "Found linear module: model.layers.27.mlp.down_proj\n",
            "Found linear module: lm_head\n",
            "Model type detected: qwen3\n",
            "Auto-selected LoRA target modules: ['up_proj', 'k_proj', 'gate_proj', 'o_proj', 'v_proj', 'down_proj', 'q_proj']\n",
            "Using GSM8K self-training mode - input already contains chat template\n",
            "Loaded 9346 examples from ckpt/rank_32/iteration_0/correct_examples.jsonl\n",
            "Tokenizing GSM8K examples: 100%|██████████| 9346/9346 [00:19<00:00, 471.81it/s]\n",
            "/content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/train_gsm8k_self_training_lora.py:665: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "wandb: Currently logged in as: lamyeungkong0108 (lamyeungkong0108-the-hong-kong-university-of-science-and) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: setting up run 1jlspx1p\n",
            "wandb: Tracking run with wandb version 0.22.3\n",
            "wandb: Run data is saved locally in /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3/wandb/run-20251117_083747-1jlspx1p\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run jolly-wave-12\n",
            "wandb: ⭐️ View project at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3\n",
            "wandb: 🚀 View run at https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/COMP4901B-Homework3/runs/1jlspx1p\n",
            "Saved preprocessed features to ckpt/rank_32/iteration_0/correct_examples_gsm8k_processed.pickle.\n",
            "#train 9159, #eval 187\n",
            "100%|██████████| 72/72 [34:49<00:00, 29.02s/it]\n",
            "{'loss': 0.2925, 'grad_norm': 0.058291103690862656, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
            "{'loss': 0.2816, 'grad_norm': 0.05614154785871506, 'learning_rate': 1.999048221581858e-05, 'epoch': 0.03}\n",
            "{'loss': 0.2676, 'grad_norm': 0.057649873197078705, 'learning_rate': 1.9961946980917457e-05, 'epoch': 0.04}\n",
            "{'loss': 0.2731, 'grad_norm': 0.06017749384045601, 'learning_rate': 1.9914448613738107e-05, 'epoch': 0.06}\n",
            "{'loss': 0.2855, 'grad_norm': 0.06223456189036369, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.07}\n",
            "{'loss': 0.2868, 'grad_norm': 0.06009100377559662, 'learning_rate': 1.9762960071199334e-05, 'epoch': 0.08}\n",
            "{'loss': 0.2874, 'grad_norm': 0.05765771493315697, 'learning_rate': 1.9659258262890683e-05, 'epoch': 0.1}\n",
            "{'loss': 0.2882, 'grad_norm': 0.06111738458275795, 'learning_rate': 1.953716950748227e-05, 'epoch': 0.11}\n",
            "{'loss': 0.269, 'grad_norm': 0.05617518350481987, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.13}\n",
            "{'loss': 0.2941, 'grad_norm': 0.06119087338447571, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.14}\n",
            "{'loss': 0.28, 'grad_norm': 0.06282384693622589, 'learning_rate': 1.9063077870366504e-05, 'epoch': 0.15}\n",
            "{'loss': 0.2784, 'grad_norm': 0.05526692792773247, 'learning_rate': 1.887010833178222e-05, 'epoch': 0.17}\n",
            "{'loss': 0.2957, 'grad_norm': 0.062102530151605606, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.18}\n",
            "{'loss': 0.2951, 'grad_norm': 0.05788533762097359, 'learning_rate': 1.843391445812886e-05, 'epoch': 0.2}\n",
            "{'loss': 0.2835, 'grad_norm': 0.05757012590765953, 'learning_rate': 1.819152044288992e-05, 'epoch': 0.21}\n",
            "{'loss': 0.2903, 'grad_norm': 0.06346441805362701, 'learning_rate': 1.7933533402912354e-05, 'epoch': 0.22}\n",
            "{'loss': 0.2813, 'grad_norm': 0.0538434199988842, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.24}\n",
            "{'loss': 0.275, 'grad_norm': 0.053196415305137634, 'learning_rate': 1.737277336810124e-05, 'epoch': 0.25}\n",
            "{'loss': 0.2984, 'grad_norm': 0.05648189038038254, 'learning_rate': 1.7071067811865477e-05, 'epoch': 0.27}\n",
            "{'loss': 0.2886, 'grad_norm': 0.056035954505205154, 'learning_rate': 1.6755902076156606e-05, 'epoch': 0.28}\n",
            "{'loss': 0.2675, 'grad_norm': 0.05718084052205086, 'learning_rate': 1.6427876096865394e-05, 'epoch': 0.29}\n",
            "{'loss': 0.2995, 'grad_norm': 0.05732005089521408, 'learning_rate': 1.608761429008721e-05, 'epoch': 0.31}\n",
            "{'loss': 0.2998, 'grad_norm': 0.05634203925728798, 'learning_rate': 1.573576436351046e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2784, 'grad_norm': 0.05669171363115311, 'learning_rate': 1.5372996083468242e-05, 'epoch': 0.34}\n",
            "{'loss': 0.2963, 'grad_norm': 0.060192547738552094, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.35}\n",
            "{'loss': 0.2779, 'grad_norm': 0.05581099912524223, 'learning_rate': 1.4617486132350343e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2918, 'grad_norm': 0.05724275857210159, 'learning_rate': 1.4226182617406996e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2879, 'grad_norm': 0.05749095603823662, 'learning_rate': 1.3826834323650899e-05, 'epoch': 0.39}\n",
            "{'loss': 0.2973, 'grad_norm': 0.061531759798526764, 'learning_rate': 1.342020143325669e-05, 'epoch': 0.41}\n",
            "{'loss': 0.291, 'grad_norm': 0.05661557614803314, 'learning_rate': 1.300705799504273e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2902, 'grad_norm': 0.05551521107554436, 'learning_rate': 1.2588190451025209e-05, 'epoch': 0.43}\n",
            "{'loss': 0.2773, 'grad_norm': 0.055433087050914764, 'learning_rate': 1.2164396139381029e-05, 'epoch': 0.45}\n",
            "{'loss': 0.3059, 'grad_norm': 0.0579434372484684, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.46}\n",
            "{'loss': 0.2973, 'grad_norm': 0.05704716593027115, 'learning_rate': 1.130526192220052e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2868, 'grad_norm': 0.05629638209939003, 'learning_rate': 1.0871557427476585e-05, 'epoch': 0.49}\n",
            "{'loss': 0.2788, 'grad_norm': 0.0548681914806366, 'learning_rate': 1.0436193873653362e-05, 'epoch': 0.5}\n",
            "{'loss': 0.3002, 'grad_norm': 0.05763590708374977, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
            "{'loss': 0.2808, 'grad_norm': 0.054563257843256, 'learning_rate': 9.563806126346643e-06, 'epoch': 0.53}\n",
            "{'loss': 0.3107, 'grad_norm': 0.05784735456109047, 'learning_rate': 9.128442572523418e-06, 'epoch': 0.54}\n",
            "{'loss': 0.2848, 'grad_norm': 0.05694811791181564, 'learning_rate': 8.694738077799487e-06, 'epoch': 0.56}\n",
            "{'loss': 0.2728, 'grad_norm': 0.056367918848991394, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.57}\n",
            "{'loss': 0.2698, 'grad_norm': 0.05763092264533043, 'learning_rate': 7.835603860618973e-06, 'epoch': 0.59}\n",
            "{'loss': 0.3015, 'grad_norm': 0.055821556597948074, 'learning_rate': 7.411809548974792e-06, 'epoch': 0.6}\n",
            "{'loss': 0.2848, 'grad_norm': 0.05870034173130989, 'learning_rate': 6.992942004957271e-06, 'epoch': 0.61}\n",
            "{'loss': 0.2877, 'grad_norm': 0.062080543488264084, 'learning_rate': 6.579798566743314e-06, 'epoch': 0.63}\n",
            "{'loss': 0.2912, 'grad_norm': 0.05723422393202782, 'learning_rate': 6.173165676349103e-06, 'epoch': 0.64}\n",
            "{'loss': 0.2728, 'grad_norm': 0.05693105608224869, 'learning_rate': 5.773817382593008e-06, 'epoch': 0.66}\n",
            "{'loss': 0.2856, 'grad_norm': 0.058297887444496155, 'learning_rate': 5.382513867649663e-06, 'epoch': 0.67}\n",
            "{'loss': 0.2714, 'grad_norm': 0.05233265459537506, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.68}\n",
            "{'loss': 0.2883, 'grad_norm': 0.05418494716286659, 'learning_rate': 4.627003916531761e-06, 'epoch': 0.7}\n",
            "{'loss': 0.2975, 'grad_norm': 0.0552893802523613, 'learning_rate': 4.264235636489542e-06, 'epoch': 0.71}\n",
            "{'loss': 0.2817, 'grad_norm': 0.05416756495833397, 'learning_rate': 3.912385709912794e-06, 'epoch': 0.73}\n",
            "{'loss': 0.2841, 'grad_norm': 0.06075769662857056, 'learning_rate': 3.5721239031346067e-06, 'epoch': 0.74}\n",
            "{'loss': 0.2942, 'grad_norm': 0.05581529811024666, 'learning_rate': 3.2440979238433977e-06, 'epoch': 0.75}\n",
            "{'loss': 0.2865, 'grad_norm': 0.05166428163647652, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.77}\n",
            "{'loss': 0.2735, 'grad_norm': 0.05521451309323311, 'learning_rate': 2.6272266318987606e-06, 'epoch': 0.78}\n",
            "{'loss': 0.2716, 'grad_norm': 0.0524153858423233, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.8}\n",
            "{'loss': 0.3028, 'grad_norm': 0.05391257628798485, 'learning_rate': 2.0664665970876496e-06, 'epoch': 0.81}\n",
            "{'loss': 0.2998, 'grad_norm': 0.05454582720994949, 'learning_rate': 1.808479557110081e-06, 'epoch': 0.82}\n",
            "{'loss': 0.2929, 'grad_norm': 0.05609680339694023, 'learning_rate': 1.566085541871145e-06, 'epoch': 0.84}\n",
            "{'loss': 0.2675, 'grad_norm': 0.05294415354728699, 'learning_rate': 1.339745962155613e-06, 'epoch': 0.85}\n",
            "{'loss': 0.2873, 'grad_norm': 0.05702134221792221, 'learning_rate': 1.129891668217783e-06, 'epoch': 0.87}\n",
            "{'loss': 0.2972, 'grad_norm': 0.054524537175893784, 'learning_rate': 9.369221296335007e-07, 'epoch': 0.88}\n",
            "{'loss': 0.2799, 'grad_norm': 0.06091780960559845, 'learning_rate': 7.612046748871327e-07, 'epoch': 0.89}\n",
            "{'loss': 0.2794, 'grad_norm': 0.058130037039518356, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.91}\n",
            "{'loss': 0.2863, 'grad_norm': 0.05694242939352989, 'learning_rate': 4.628304925177318e-07, 'epoch': 0.92}\n",
            "{'loss': 0.2832, 'grad_norm': 0.05851338803768158, 'learning_rate': 3.4074173710931804e-07, 'epoch': 0.94}\n",
            "{'loss': 0.2564, 'grad_norm': 0.05997682362794876, 'learning_rate': 2.370399288006664e-07, 'epoch': 0.95}\n",
            "{'loss': 0.2824, 'grad_norm': 0.05701938271522522, 'learning_rate': 1.519224698779198e-07, 'epoch': 0.96}\n",
            "{'loss': 0.2889, 'grad_norm': 0.057776764035224915, 'learning_rate': 8.555138626189619e-08, 'epoch': 0.98}\n",
            "{'loss': 0.2876, 'grad_norm': 0.05228644981980324, 'learning_rate': 3.805301908254455e-08, 'epoch': 0.99}\n",
            "{'loss': 0.2837, 'grad_norm': 0.07434571534395218, 'learning_rate': 9.517784181422018e-09, 'epoch': 1.0}\n",
            "{'train_runtime': 2091.5856, 'train_samples_per_second': 4.379, 'train_steps_per_second': 0.034, 'train_loss': 0.2858637500968244, 'epoch': 1.0}\n",
            "Merging saved LoRA adapters into standalone model weights.\n",
            "Merging LoRA adapter from ckpt/rank_32/models/model_iter_1 into base model; saving to ckpt/rank_32/models/model_iter_1-merged\n",
            "Merging LoRA adapter from ckpt/rank_32/models/model_iter_1/checkpoint-60 into base model; saving to ckpt/rank_32/models/model_iter_1/checkpoint-60-merged\n",
            "Merging LoRA adapter from ckpt/rank_32/models/model_iter_1/checkpoint-72 into base model; saving to ckpt/rank_32/models/model_iter_1/checkpoint-72-merged\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mjolly-wave-12\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251117_083747-1jlspx1p/logs\u001b[0m\n",
            "Using merged LoRA model: ckpt/rank_32/models/model_iter_1-merged\n",
            "✓ Training completed\n",
            "\n",
            "✓ Iteration 0 completed\n",
            "\n",
            "========================================\n",
            "Self-Training Pipeline Summary\n",
            "========================================\n",
            "Pipeline Directory: ckpt/rank_32\n",
            "Iterations Completed: 1 / 1\n",
            "\n",
            "Results by iteration:\n",
            "  Iteration 0: 9346 correct examples\n",
            "\n",
            "Logs:\n",
            "  - Main log: ckpt/rank_32/logs/pipeline.log\n",
            "  - Iteration logs: ckpt/rank_32/iteration_*/iteration.log\n",
            "\n",
            "✓ All iterations completed successfully!\n",
            "✓ Final model saved to: ckpt/rank_32/models/model_iter_1-merged\n",
            "\n",
            "========================================\n",
            "Pipeline finished!\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 1\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_32/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_32/rollout1 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 1"
      ],
      "metadata": {
        "id": "pCP5kCyaR_or",
        "outputId": "5ea46c0b-39b9-4a71-da73-4de073e85294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pCP5kCyaR_or",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/rank_32/models/model_iter_1-merged\n",
            "Output Directory: results/rank_32/rollout1\n",
            "Run Name: model_iter_1-merged_20251117_115602\n",
            "Dataset Split: test\n",
            "Number of Queries: All\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 1\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/rank_32/rollout1/model_iter_1-merged_20251117_115602_inference.jsonl\n",
            "\n",
            "2025-11-17 11:56:07.546182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:56:07.564798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763380567.586989   74568 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763380567.593751   74568 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763380567.610734   74568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380567.610773   74568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380567.610775   74568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380567.610777   74568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:56:07.615736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:56:15 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:56:18 [utils.py:328] non-default args: {'tokenizer': 'ckpt/rank_32/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/rank_32/models/model_iter_1-merged'}\n",
            "INFO 11-17 11:56:35 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 11:56:35 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 11:56:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 11:56:38 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 11:56:43.151283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763380603.173001   74821 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763380603.179445   74821 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763380603.195548   74821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380603.195577   74821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380603.195579   74821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380603.195581   74821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 11:56:50 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:52 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:52 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/rank_32/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/rank_32/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/rank_32/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 11:56:54.909261344 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:54 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m WARNING 11-17 11:56:54 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:54 [gpu_model_runner.py:2338] Starting to load model ckpt/rank_32/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:55 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:55 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.19it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.19it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:56 [default_loader.py:268] Loading weights took 0.87 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:56:57 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.079115 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:05 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/af9adfc30f/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:05 [backends.py:550] Dynamo bytecode transform time: 7.88 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:06 [backends.py:194] Cache the graph for dynamic shape for later use\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:11 [backends.py:215] Compiling a graph for dynamic shape takes 5.75 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:12 [monitor.py:34] torch.compile takes 13.64 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:14 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:14 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:14 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.07it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:19 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:19 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=74821)\u001b[0;0m INFO 11-17 11:57:19 [core.py:218] init engine (profile, create kv cache, warmup model) took 21.85 seconds\n",
            "INFO 11-17 11:57:20 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 11:57:20 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:00<00:00, 1682.86it/s]\n",
            "Processed prompts: 100%|██████████| 1319/1319 [02:17<00:00,  9.62it/s, est. speed input: 849.49 toks/s, output: 2495.01 toks/s]\n",
            "[rank0]:[W1117 11:59:38.590768569 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/rank_32/rollout1/model_iter_1-merged_20251117_115602_evaluation.jsonl\n",
            "\n",
            "Loading data from results/rank_32/rollout1/model_iter_1-merged_20251117_115602_inference.jsonl\n",
            "Loaded 1319 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 1319\n",
            "Correct answers: 811\n",
            "Overall Accuracy: 61.49%\n",
            "\n",
            "Results saved to results/rank_32/rollout1/model_iter_1-merged_20251117_115602_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/rank_32/rollout1/model_iter_1-merged_20251117_115602_inference.jsonl\n",
            "  - Evaluation results: results/rank_32/rollout1/model_iter_1-merged_20251117_115602_evaluation.jsonl\n",
            "  - Log file: results/rank_32/rollout1/model_iter_1-merged_20251117_115602.log\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of iteration 1 rollout 8\n",
        "!cd /content/drive/MyDrive/COMP4901B-Homework3/COMP4901B-LLMs/assignment3 && bash scripts/run_gsm8k_eval.sh ckpt/rank_32/models/model_iter_1-merged \\\n",
        "    --output_dir results/rank_32/rollout8 \\\n",
        "    --temperature 0.6 \\\n",
        "    --max_tokens 512 \\\n",
        "    --top_p 0.95 \\\n",
        "    --top_k 20 \\\n",
        "    --n_rollouts 8 \\\n",
        "    --n_queries 2000"
      ],
      "metadata": {
        "id": "qjiwnnfJSDM3",
        "outputId": "0a2993c3-6b56-4182-ee5d-50581b1608aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qjiwnnfJSDM3",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "GSM8K Evaluation Pipeline\n",
            "========================================\n",
            "Model: ckpt/rank_32/models/model_iter_1-merged\n",
            "Output Directory: results/rank_32/rollout8\n",
            "Run Name: model_iter_1-merged_20251117_115942\n",
            "Dataset Split: test\n",
            "Number of Queries: 2000\n",
            "Max Tokens: 512\n",
            "Temperature: 0.6\n",
            "Top-p: 0.95\n",
            "Top-k: 20\n",
            "Number of Rollouts: 8\n",
            "Tensor Parallel: 1\n",
            "Mode: Zero-shot\n",
            "Chat Template: Enabled\n",
            "Thinking Mode: Disabled\n",
            "========================================\n",
            "\n",
            "[1/2] Running inference...\n",
            "Output will be saved to: results/rank_32/rollout8/model_iter_1-merged_20251117_115942_inference.jsonl\n",
            "\n",
            "2025-11-17 11:59:46.658805: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 11:59:46.677468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763380786.700428   75834 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763380786.707273   75834 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763380786.724368   75834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380786.724404   75834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380786.724406   75834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380786.724408   75834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 11:59:46.729421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 11-17 11:59:54 [__init__.py:216] Automatically detected platform cuda.\n",
            "INFO 11-17 11:59:56 [utils.py:328] non-default args: {'tokenizer': 'ckpt/rank_32/models/model_iter_1-merged', 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'ckpt/rank_32/models/model_iter_1-merged'}\n",
            "INFO 11-17 12:00:12 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 11-17 12:00:12 [__init__.py:1815] Using max model len 40960\n",
            "INFO 11-17 12:00:14 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-17 12:00:15 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-17 12:00:20.301290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763380820.322681   76076 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763380820.329522   76076 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763380820.345559   76076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380820.345596   76076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380820.345599   76076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763380820.345601   76076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 11-17 12:00:28 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:29 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:29 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='ckpt/rank_32/models/model_iter_1-merged', speculative_config=None, tokenizer='ckpt/rank_32/models/model_iter_1-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/rank_32/models/model_iter_1-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[W1117 12:00:32.179223454 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m WARNING 11-17 12:00:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:32 [gpu_model_runner.py:2338] Starting to load model ckpt/rank_32/models/model_iter_1-merged...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:32 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:32 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:33 [default_loader.py:268] Loading weights took 0.98 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:34 [gpu_model_runner.py:2392] Model loading took 1.1201 GiB and 1.192305 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:43 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/af9adfc30f/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:43 [backends.py:550] Dynamo bytecode transform time: 7.98 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.188 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:47 [monitor.py:34] torch.compile takes 7.98 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:49 [gpu_worker.py:298] Available KV cache memory: 15.19 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:49 [kv_cache_utils.py:864] GPU KV cache size: 142,208 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:49 [kv_cache_utils.py:868] Maximum concurrency for 40,960 tokens per request: 3.47x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.95it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:53 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:53 [gpu_worker.py:391] Free memory on device (21.95/22.16 GiB) on startup. Desired GPU memory utilization is (0.8, 17.73 GiB). Actual usage is 1.12 GiB for weight, 1.4 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.47 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=15648680140` to fit into requested memory, or `--kv-cache-memory=20177558528` to fully utilize gpu memory. Current kv cache memory in use is 16311380172 bytes.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=76076)\u001b[0;0m INFO 11-17 12:00:53 [core.py:218] init engine (profile, create kv cache, warmup model) took 19.23 seconds\n",
            "INFO 11-17 12:00:55 [llm.py:295] Supported_tasks: ['generate']\n",
            "INFO 11-17 12:00:55 [__init__.py:36] No IOProcessor plugins requested by the model\n",
            "Adding requests: 100%|██████████| 1319/1319 [00:01<00:00, 1118.10it/s]\n",
            "Processed prompts: 100%|██████████| 10552/10552 [16:37<00:00, 10.58it/s, est. speed input: 934.11 toks/s, output: 2750.72 toks/s]\n",
            "[rank0]:[W1117 12:17:34.439894121 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\n",
            "Inference completed successfully!\n",
            "\n",
            "[2/2] Running evaluation...\n",
            "Evaluation results will be saved to: results/rank_32/rollout8/model_iter_1-merged_20251117_115942_evaluation.jsonl\n",
            "\n",
            "Loading data from results/rank_32/rollout8/model_iter_1-merged_20251117_115942_inference.jsonl\n",
            "Loaded 10552 examples\n",
            "\n",
            "Basic Evaluation Results:\n",
            "Total examples: 10552\n",
            "Correct answers: 6375\n",
            "Overall Accuracy: 60.42%\n",
            "\n",
            "Detected multiple rollouts: 1319 unique questions with 10552 total rollouts\n",
            "\n",
            "============================================================\n",
            "Multiple Rollout Metrics:\n",
            "============================================================\n",
            "k     avg@k           pass@k         \n",
            "------------------------------------------------------------\n",
            "1     0.5906          0.5906         \n",
            "2     0.5940          0.7036         \n",
            "3     0.5982          0.7506         \n",
            "4     0.6008          0.7824         \n",
            "5     0.5995          0.8006         \n",
            "6     0.6030          0.8196         \n",
            "7     0.6029          0.8370         \n",
            "8     0.6042          0.8446         \n",
            "============================================================\n",
            "\n",
            "Metrics saved to results/rank_32/rollout8/model_iter_1-merged_20251117_115942_evaluation_metrics.json\n",
            "\n",
            "Results saved to results/rank_32/rollout8/model_iter_1-merged_20251117_115942_evaluation.jsonl\n",
            "\n",
            "========================================\n",
            "Evaluation Pipeline Completed!\n",
            "========================================\n",
            "Results:\n",
            "  - Inference outputs: results/rank_32/rollout8/model_iter_1-merged_20251117_115942_inference.jsonl\n",
            "  - Evaluation results: results/rank_32/rollout8/model_iter_1-merged_20251117_115942_evaluation.jsonl\n",
            "  - Log file: results/rank_32/rollout8/model_iter_1-merged_20251117_115942.log\n",
            "========================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}